# Daily Updates on 3D-Related Papers

This repository automatically fetches new or updated arXiv papers in the [cs.CV] category every day, checks if they are relevant to "3D reconstruction" or "3D generation" via ChatGPT, and lists them below.

## How It Works
1. A GitHub Actions workflow runs daily at 09:00 UTC.  
2. It uses the script [fetch_cv_3d_papers.py](fetch_cv_3d_papers.py) to:  
   - Retrieve the latest arXiv papers in cs.CV.  
   - Use ChatGPT to filter out those related to 3D reconstruction/generation.  
   - Update this README.md with the new findings.  
   - Send an email via 163 Mail if any relevant papers are found.  

# Paper List
## Arxiv 2025-04-09

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2504.05400 GARF: Learning Generalizable 3D Reassembly for Real-World Fractures](https://arxiv.org/abs/2504.05400) <br> [{'name': 'Sihang Li, Zeyu Jiang, Grace Chen, Chenyang Xu, Siqi Tan, Xue Wang, Irving Fang, Kristof Zyskowski, Shannon P. McPherron, Radu Iovita, Chen Feng, Jing Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reassembly 三维重组<br>fracture 断裂<br>dataset 数据集 | Input: Various fractured 3D objects 各种破碎的三维物体<br>Step1: Fracture-aware feature learning 破碎感知特征学习<br>Step2: Flow matching for alignment 对齐的流匹配<br>Step3: One-step preassembly for robustness 一步预组装以提高鲁棒性<br>Output: Reassembled 3D models 重新组装的三维模型 |
9.5 | [[9.5] 2504.05649 POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud](https://arxiv.org/abs/2504.05649) <br> [{'name': 'Yining Shi, Kun Jiang, Xin Zhao, Kangan Qian, Chuchu Xie, Tuopu Wen, Mengmeng Yang, Diange Yang'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>FMCW LiDAR<br>autonomous driving | Input: Single-frame FMCW LiDAR point cloud<br>Step1: Generate virtual future point using ray casting<br>Step2: Create virtual two-frame point clouds<br>Step3: Encode with a sparse 4D encoder<br>Output: Predictive object detection results |
9.5 | [[9.5] 2504.05698 Point-based Instance Completion with Scene Constraints](https://arxiv.org/abs/2504.05698) <br> [{'name': 'Wesley Khademi, Li Fuxin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud<br>3D reconstruction<br>scene completion<br>autonomous systems | Input: Partial point clouds of objects 场景中物体的部分点云<br>Step1: Seed generation 生成种子点<br>Step2: Scene constraints integration 场景约束集成<br>Step3: Instance completion 模型完成<br>Output: Completed 3D objects 完成的三维对象 |
9.5 | [[9.5] 2504.05720 QEMesh: Employing A Quadric Error Metrics-Based Representation for Mesh Generation](https://arxiv.org/abs/2504.05720) <br> [{'name': 'Jiaqi Li, Ruowei Wang, Yu Liu, Qijun Zhao'}] | 3D Generation 三维生成 | v2<br>3D reconstruction<br>mesh generation<br>Quadric Error Metrics | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model evaluation 模型评估<br>Output: Enhanced 3D models 改进的三维模型 |
9.5 | [[9.5] 2504.05751 InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation](https://arxiv.org/abs/2504.05751) <br> [{'name': 'Jiangsan Zhao, Jakob Geipel, Krzysztof Kusnierek, Xuean Cui'}] | 3D Segmentation 3D分割 | v2<br>Neural Radiance Fields<br>3D segmentation<br>fine-tuning | Input: Multi-view RGB images and 2D segmentation masks 多视角RGB图像和2D分割掩膜<br>Step1: Train standard NeRF on RGB images 使用RGB图像训练标准NeRF<br>Step2: Fine-tune using 2D segmentation masks using the same NeRF architecture 使用相同的NeRF架构对2D分割掩膜进行微调<br>Output: Segmented 3D point clouds 输出：分割的3D点云 |
9.5 | [[9.5] 2504.06178 Flash Sculptor: Modular 3D Worlds from Objects](https://arxiv.org/abs/2504.06178) <br> [{'name': 'Yujia Hu, Songhua Liu, Xingyi Yang, Xinchao Wang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>scene generation<br>modular objects<br>image-to-3D | Input: Single image 单幅图像<br>Step1: Decouple tasks 任务分解<br>Step2: Estimate parameters 估计参数<br>Step3: Generate 3D scene 生成三维场景<br>Output: Compositional 3D scene 组合三维场景 |
9.5 | [[9.5] 2504.06210 HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation](https://arxiv.org/abs/2504.06210) <br> [{'name': 'Yiming Liang, Tianhan Xu, Yuta Kikuchi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>monocular videos | Input: Monocular video 单目视频<br>Step1: Motion decomposition 运动分解<br>Step2: Hierarchical representation design 层次表示设计<br>Step3: Gaussian deformation adjustment 高斯变形调整<br>Output: Enhanced dynamic 3D model  改进的动态三维模型 |
9.5 | [[9.5] 2504.06264 D^2USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes](https://arxiv.org/abs/2504.06264) <br> [{'name': 'Jisang Han, Honggyu An, Jaewoo Jung, Takuya Narihira, Junyoung Seo, Kazumi Fukuda, Chaehyun Kim, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dynamic scenes<br>4D pointmaps | Input: Multi-view images and dynamic scene data 多视角图像和动态场景数据<br>Step1: Regressing 4D pointmaps 回归4D点图<br>Step2: Establishing dense correspondences 进行密集对应<br>Step3: Model training with temporal awareness 模型训练与时间意识<br>Output: Enhanced 3D reconstruction 模型改进的三维重建 |
9.2 | [[9.2] 2504.06003 econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians](https://arxiv.org/abs/2504.06003) <br> [{'name': 'Can Zhang, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D semantic segmentation 3D语义分割<br>multi-view consistency 多视角一致性<br>open-vocabulary segmentation 开放词汇分割 | Input: Multi-view images 多视角图像<br>Step1: Data refinement using Confidence-region Guided Regularization (CRR) 使用信心区域引导正则化进行数据细化<br>Step2: Constructing a low-dimensional contextual space 创建低维上下文空间<br>Step3: Fusing backprojected multi-view features 融合反投影的多视角特征<br>Output: 3D semantic field representation 3D语义场表示 |
8.5 | [[8.5] 2504.05422 EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations](https://arxiv.org/abs/2504.05422) <br> [{'name': 'Yue Yao, Mohamed-Khalil Bouzidi, Daniel Goehring, Joerg Reichardt'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>traffic scene generation<br>autonomous vehicles<br>generative models | Input: Road layout and agent history 路布局和代理历史<br>Step1: Model designing using polynomial representations 模型设计，使用多项式表示<br>Step2: Training the diffusion-based generative model 训练基于扩散的生成模型<br>Step3: Evaluating traffic scene predictions 评估交通场景预测<br>Output: Diverse and plausible traffic scene continuations 生成多样和合理的交通场景延续 |
8.5 | [[8.5] 2504.05579 TAPNext: Tracking Any Point (TAP) as Next Token Prediction](https://arxiv.org/abs/2504.05579) <br> [{'name': 'Artem Zholus, Carl Doersch, Yi Yang, Skanda Koppula, Viorica Patraucean, Xu Owen He, Ignacio Rocco, Mehdi S. M. Sajjadi, Sarath Chandar, Ross Goroshin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point tracking<br>3D reconstruction<br>robotics | Input: Video frames 视频帧<br>Step 1: Point tracking point tracking 追踪<br>Step 2: Token decoding 令牌解码<br>Step 3: Model evaluation 模型评估<br>Output: Accurate point tracks 准确的点轨迹 |
8.5 | [[8.5] 2504.05786 How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM](https://arxiv.org/abs/2504.05786) <br> [{'name': 'Jirong Zha, Yuxuan Fan, Xiao Yang, Chen Gao, Xinlei Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D spatial understanding<br>Large Language Models<br>multimodal fusion<br>autonomous vehicles<br>robotics | Input: Integration of Large Language Models (LLMs) with 3D spatial understanding<br>Step1: Categorization into image-based, point cloud-based, and hybrid modality methods<br>Step2: Systematic review of existing research methods<br>Step3: Discussion on limitations and future directions<br>Output: Comprehensive framework for 3D-LLM integration |
8.5 | [[8.5] 2504.05882 Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques](https://arxiv.org/abs/2504.05882) <br> [{'name': 'Luca Barco, Giacomo Blanco, Gaetano Chiriaco, Alessia Intini, Luigi La Riccia, Vittorio Scolamiero, Piero Boccardo, Paolo Garza, Fabrizio Dominici'}] | 3D Semantic Segmentation 三维语义分割 | v2<br>3D segementation<br>LiDAR<br>urban modeling | Input: Aerial LiDAR data 空中激光雷达数据<br>Step1: Dataset collection 数据集收集<br>Step2: Performance benchmarking 性能基准测试<br>Step3: Semi-supervised learning application 半监督学习应用<br>Output: Improved 3D semantic segmentation results 改进的3D语义分割结果 |
8.5 | [[8.5] 2504.05908 PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario](https://arxiv.org/abs/2504.05908) <br> [{'name': 'Sriram Mandalika, Lalitha V, Athira Nambiar'}] | Autonomous Driving 自动驾驶 | v2<br>3D object detection<br>autonomous driving<br>uncertainty-aware modeling | Input: LiDAR-based 3D object detection and multi-view RGB references<br>Step1: Model Training with Bayesian Graph Neural Networks (BGNNs)<br>Step2: Uncertainty modeling for object interactions<br>Step3: Evaluation on DriveCoT dataset<br>Output: Enhanced decision-making under uncertainty |
8.0 | [[8.0] 2504.05458 Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images](https://arxiv.org/abs/2504.05458) <br> [{'name': 'In-Hwan Jin, Haesoo Choo, Seong-Hun Jeong, Heemoon Park, Junghwan Kim, Oh-joon Kwon, Kyeongbo Kong'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D space virtualization 3D空间虚拟化<br>dynamic scene video 动态场景视频 | Input: Single landscape image 单个景观图像<br>Step1: Generate multi-view images 生成多视角图像<br>Step2: Optimize 3D Gaussians 优化3D高斯<br>Step3: Estimate consistent 3D motion 估计一致的3D运动<br>Output: Dynamic scene video 动态场景视频 |
8.0 | [[8.0] 2504.05979 An Empirical Study of GPT-4o Image Generation Capabilities](https://arxiv.org/abs/2504.05979) <br> [{'name': 'Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, Wenhao Chai, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi'}] | Image Generation 图像生成 | v2<br>image generation<br>multimodal models<br>GPT-4o<br>image-to-3D generation | Input: Generative models and tasks 生成模型和任务<br>Step1: Evaluation against existing models 与现有模型的评估<br>Step2: Benchmarking across categories 在各类任务中的基准测试<br>Step3: Comparative analysis of strengths and limitations 优势和局限性的比较分析<br>Output: Comprehensive evaluation results 综合评估结果 |
7.5 | [[7.5] 2504.05402 Time-adaptive Video Frame Interpolation based on Residual Diffusion](https://arxiv.org/abs/2504.05402) <br> [{'name': 'Victor Fonte Chavez, Claudia Esteves, Jean-Bernard Hayet'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Video Frame Interpolation<br>Diffusion Models<br>Animation | Input: Animation frames 动画帧<br>Step1: Time handling during training 训练过程中的时间处理<br>Step2: Adapt diffusion scheme for VFI 适应扩散方案用于视频帧插值<br>Step3: Uncertainty estimation 不确定性估计<br>Output: Interpolated video frames 插值视频帧 |
6.5 | [[6.5] 2504.05456 Generative Adversarial Networks with Limited Data: A Survey and Benchmarking](https://arxiv.org/abs/2504.05456) <br> [{'name': 'Omar De Mitri, Ruyu Wang, Marco F. Huber'}] | Image Generation 图像生成 | v2<br>Generative Adversarial Networks<br>Limited Data<br>Image Synthesis<br>Generative Models | Input: Limited datasets 限量数据<br>Step 1: Literature review 文献综述<br>Step 2: Performance evaluation 性能评估<br>Step 3: Challenge identification 挑战识别<br>Output: Insights on GAN performance 生成对抗网络性能见解 |


## Arxiv 2025-04-08

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2504.03875 3D Scene Understanding Through Local Random Access Sequence Modeling](https://arxiv.org/abs/2504.03875) <br> [{'name': 'Wanhee Lee, Klemen Kotar, Rahul Mysore Venkatesh, Jared Watrous, Honglin Chen, Khai Loong Aw, Daniel L. K. Yamins'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene understanding<br>novel view synthesis<br>depth estimation | Input: Single images 单幅图像<br>Step1: Local patch quantization 局部图块量化<br>Step2: Randomly ordered sequence generation 随机顺序生成<br>Step3: 3D scene editing via optical flow 通过光流进行三维场景编辑<br>Output: Enhanced capabilities for 3D scene understanding 改进的三维场景理解能力 |
9.5 | [[9.5] 2504.03886 WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments](https://arxiv.org/abs/2504.03886) <br> [{'name': 'Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>3D reconstruction 三维重建<br>dynamic environments 动态环境<br>SLAM 同时定位与地图构建 | Input: Monocular video sequence 单目视频序列<br>Step1: Generate uncertainty map 生成不确定性地图<br>Step2: Dynamic object removal 动态物体移除<br>Step3: Dense bundle adjustment and Gaussian map optimization 密集束调整与高斯地图优化<br>Output: 3D Gaussian map and camera trajectory 3D高斯地图和相机轨迹 |
9.5 | [[9.5] 2504.04190 Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning](https://arxiv.org/abs/2504.04190) <br> [{'name': 'Yuyang Zhang, Baao Xie, Hu Zhu, Qi Wang, Huanting Guo, Xin Jin, Wenjun Zeng'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>interpretability<br>disentangled representation learning<br>single-view | Input: Single-view images 单视角图像<br>Step1: Data integration 数据集成<br>Step2: Hierarchical disentangled representation learning (DRL) 层次化解耦表征学习<br>Step3: 3D geometry and appearance disentanglement 3D几何和外观解耦<br>Output: Interpretable and high-quality 3D models 可解释的高质量3D模型 |
9.5 | [[9.5] 2504.04294 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS](https://arxiv.org/abs/2504.04294) <br> [{'name': 'Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Structure-from-Motion<br>camera pose optimization | Input: 3D Gaussian representations and camera poses 3D高斯表示和相机姿态<br>Step1: Joint optimization of 3D Gaussians and camera parameters 联合优化3D高斯和相机参数<br>Step2: Implement 3DGS-MCMC for robustness 对3DGS-MCMC实施以增强鲁棒性<br>Step3: Use an MLP for camera pose refinement 使用多层感知机（MLP）进行相机姿态优化<br>Output: High-quality novel views and accurate camera poses 输出：高质量的新视图和准确的相机姿态 |
9.5 | [[9.5] 2504.04448 Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models](https://arxiv.org/abs/2504.04448) <br> [{'name': 'Etienne Chassaing, Florent Forest, Olga Fink, Malcolm Mielle'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>finite element analysis<br>thermal modeling<br>voxel-based modeling | Input: Sparse RGB and thermal images RGB和热图像作为输入<br>Step1: Voxel representation 体素表示<br>Step2: Geometry and temperature optimization 几何和温度优化<br>Step3: Model transformation to tetrahedral meshes 将模型转换为四面体网格<br>Output: FEA-compatible 3D models 输出: 兼容FEA的3D模型 |
9.5 | [[9.5] 2504.04454 PRISM: Probabilistic Representation for Integrated Shape Modeling and Generation](https://arxiv.org/abs/2504.04454) <br> [{'name': 'Lei Cheng, Mahdi Saleh, Qing Cheng, Lu Sang, Hongli Xu, Daniel Cremers, Federico Tombari'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape generation<br>Statistical Shape Models (SSM)<br>Gaussian Mixture Models (GMM) | Input: Real-world objects 真实物体<br>Step1: Integration of Statistical Shape Models (SSM) and Gaussian Mixture Models (GMM) 整合统计形状模型和高斯混合模型<br>Step2: Application of categorical diffusion models 应用类别扩散模型<br>Step3: Shape generation and manipulation shapes 生成和操作形状<br>Output: High-fidelity, structurally coherent 3D shapes 高保真、结构一致的三维形状 |
9.5 | [[9.5] 2504.04597 Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians](https://arxiv.org/abs/2504.04597) <br> [{'name': 'Haebeom Jung, Namtae Kim, Jungwoo Kim, Jaesik Park'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR-camera calibration<br>3D Gaussian<br>autonomous driving | Input: LiDAR and camera data 激光雷达与相机数据<br>Step1: Freeze reliable LiDAR points as anchors 固定可靠的激光雷达点作为锚点<br>Step2: Jointly optimize sensor poses and Gaussian parameters 联合优化传感器姿态和高斯参数<br>Step3: Evaluate using photometric loss 和光度损失进行评估<br>Output: Improved calibration poses 改进的标定姿态 |
9.5 | [[9.5] 2504.04679 DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal](https://arxiv.org/abs/2504.04679) <br> [{'name': 'Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>occlusion removal<br>Neural Radiance Fields | Input: Incomplete images 不完整图像<br>Step1: Joint multi-view optimization of learnable camera parameters 学习相机参数的多视角联合优化<br>Step2: Application of occlusion annealing regularization 应用遮挡退火正则化<br>Step3: Use of stochastic structural similarity loss 使用随机结构相似性损失<br>Output: High-quality 3D scene reconstructions 高质量的三维场景重建 |
9.5 | [[9.5] 2504.04732 Inverse++: Vision-Centric 3D Semantic Occupancy Prediction Assisted with 3D Object Detection](https://arxiv.org/abs/2504.04732) <br> [{'name': 'Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Stewart Worrall'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic occupancy prediction 3D语义占用预测<br>autonomous vehicles 自动驾驶<br>3D object detection 3D物体检测 | Input: Surround-view images 360度视图图像<br>Step1: Introduce 3D object detection auxiliary branch 引入3D物体检测辅助分支<br>Step2: Enhance intermediate feature supervision 增强中间特征监督<br>Step3: Generate 3D semantic occupancy grid 生成3D语义占用网格<br>Output: Improved 3D perception capabilities 改进的3D感知能力 |
9.5 | [[9.5] 2504.05170 SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection](https://arxiv.org/abs/2504.05170) <br> [{'name': 'Bonan Ding, Jin Xie, Jing Nie, Jiale Cao'}] | 3D Object Detection 三维物体检测 | v2<br>3D object detection<br>feature fusion<br>autonomous systems | Input: Multi-modal data (LiDAR and camera images) 输入: 多模态数据（激光雷达和摄像机图像）<br>Step1: Feature extraction 特征提取<br>Step2: Scale-aligned feature fusion 按尺度对齐的特征融合<br>Step3: 3D-to-2D space alignment 3D到2D空间对齐<br>Step4: Cross-modal latent fusion 跨模态潜变量融合<br>Output: Accurate 3D object detection results 输出: 精确的3D物体检测结果 |
9.5 | [[9.5] 2504.05249 Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images](https://arxiv.org/abs/2504.05249) <br> [{'name': 'Wenzhao Tang, Weihang Li, Xiucheng Liang, Olaf Wysocki, Filip Biljecki, Christoph Holst, Boris Jutzi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D building reconstruction<br>LoD3<br>panoramic images<br>semantic segmentation | Input: Panoramic street-level images 全景街景图像<br>Step1: Image-to-object matching 图像与对象匹配<br>Step2: 3D model B-Rep surface simplification 3D模型边界表示表面简化<br>Step3: Ortho-rectification of images 图像正射校正<br>Step4: Facade segmentation facade segmentation<br>Output: Enhanced Level of Detail 3D building models 改进的细节层次(LoD) 3D建筑模型 |
9.2 | [[9.2] 2504.03868 Control Map Distribution using Map Query Bank for Online Map Generation](https://arxiv.org/abs/2504.03868) <br> [{'name': 'Ziming Liu, Leichen Wang, Ge Yang, Xinrun Li, Xingtao Hu, Hao Sun, Guangyu Gao'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>High-definition maps 高清地图<br>Online map generation 在线地图生成<br>Autonomous driving 自动驾驶<br>Transformers 变换器 | Input: Low-cost standard definition map data (SD map) 标准定义地图数据<br>Step1: Map query bank decomposition 地图查询银行分解<br>Step2: Initial distribution generation for scenarios 场景的初始分布生成<br>Step3: Map predictions optimization 地图预测优化<br>Output: Optimized HD maps 优化的高清地图 |
9.2 | [[9.2] 2504.05303 InteractVLM: 3D Interaction Reasoning from 2D Foundational Models](https://arxiv.org/abs/2504.05303) <br> [{'name': "Sai Kumar Dwivedi, Dimitrije Anti\\'c, Shashank Tripathi, Omid Taheri, Cordelia Schmid, Michael J. Black, Dimitrios Tzionas"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human-object interaction<br>Vision-Language Models | Input: In-the-wild images 在野外图像<br>Step1: Multi-view rendering 多视角渲染<br>Step2: 2D contact mask prediction 2D接触掩膜预测<br>Step3: 3D lifting of contact points 3D接触点提升<br>Output: 3D contact points 3D接触点 |
9.0 | [[9.0] 2504.04457 VSLAM-LAB: A Comprehensive Framework for Visual SLAM Methods and Datasets](https://arxiv.org/abs/2504.04457) <br> [{'name': 'Alejandro Fontan, Tobias Fischer, Javier Civera, Michael Milford'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>Visual SLAM<br>benchmarking<br>robotics | Input: VSLAM algorithms and datasets<br>Step1: Standardization of datasets and evaluation metrics<br>Step2: Automation of dataset downloading and preprocessing<br>Step3: Streamlined configuration and execution of experiments<br>Output: Efficient benchmarking of VSLAM systems |
9.0 | [[9.0] 2504.04753 CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images](https://arxiv.org/abs/2504.04753) <br> [{'name': 'Cheng Chen, Jiacheng Wei, Tianrun Chen, Chi Zhang, Xiaofeng Yang, Shangzhan Zhang, Bingchen Yang, Chuan-Sheng Foo, Guosheng Lin, Qixing Huang, Fayao Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>CAD models<br>image generation<br>multi-view images<br>geometric features | Input: Unconstrained real-world CAD images 非约束的现实世界CAD图像<br>Step1: Geometry encoding 几何编码<br>Step2: Latent diffusion modeling 潜在扩散建模<br>Step3: Code checking for validity 代码有效性检查<br>Output: Generated parametric CAD models 生成的参数化CAD模型 |
8.5 | [[8.5] 2504.04124 EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection](https://arxiv.org/abs/2504.04124) <br> [{'name': 'Muhammad Ahmed Ullah Khan, Abdul Hannan Khan, Andreas Dengel'}] | Autonomous Driving 自动驾驶 | v2<br>event-based detection<br>autonomous driving<br>object detection | Input: Event camera data 事件相机数据<br>Step1: Develop Event Progression Extractor module 开发事件进展提取模块<br>Step2: Implement Metaformer architecture 实现Metaformer架构<br>Step3: Evaluate on traffic object detection benchmarks 在交通物体检测基准上进行评估<br>Output: Efficient traffic object detection model 高效的交通物体检测模型 |
8.5 | [[8.5] 2504.04158 JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration](https://arxiv.org/abs/2504.04158) <br> [{'name': 'Yunlong Lin, Zixu Lin, Haoyu Chen, Panwang Pan, Chenxin Li, Sixiang Chen, Yeying Jin, Wenbo Li, Xinghao Ding'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous driving<br>image restoration<br>perception systems<br>vision-language models | Input: Real-world degraded images 真实世界退化图像<br>Step1: Model integration 模型集成<br>Step2: Two-stage framework development 二阶段框架开发<br>Step3: Evaluation on CleanBench dataset 在CleanBench数据集上评估<br>Output: Enhanced perception metrics 改进的感知指标 |
8.5 | [[8.5] 2504.04348 OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning](https://arxiv.org/abs/2504.04348) <br> [{'name': 'Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, Jose M. Alvarez'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D driving<br>vision-language models | Input: 3D driving tasks and vision-language dataset 3D驾驶任务和视觉语言数据集<br>Step 1: Data generation using counterfactual reasoning 基于反事实推理的数据生成<br>Step 2: Framework evaluation with Omni-L and Omni-Q Omni-L与Omni-Q的框架评估<br>Output: Improved decision-making models 改进的决策模型 |
8.5 | [[8.5] 2504.04540 The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?](https://arxiv.org/abs/2504.04540) <br> [{'name': 'Weichen Zhang, Ruiying Peng, Chen Gao, Jianjie Fang, Xin Zeng, Kaiyuan Li, Ziyou Wang, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D spatial reasoning<br>point clouds<br>Large Language Models | Input: Point clouds, visual and text inputs 3D点云、视觉和文本输入<br>Step1: Evaluating spatial reasoning能力评估空间推理<br>Step2: Developing a benchmark benchmark的开发<br>Step3: Analyzing model performance模型性能分析<br>Output: Insights into 3D LLMs对3D LLM的洞察 |
8.5 | [[8.5] 2504.04631 Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective](https://arxiv.org/abs/2504.04631) <br> [{'name': 'Lei Wan, Jianxin Zhao, Andreas Wiedholz, Manuel Bied, Mateus Martinez de Lucena, Abhishek Dinkar Jagtap, Andreas Festag, Ant\\^onio Augusto Fr\\"ohlich, Hannan Ejaz Keen, Alexey Vinel'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Collaborative Perception<br>Autonomous Vehicles<br>Computer Vision | Input: 106 peer-reviewed articles 106篇同行评审的文章<br>Step1: Literature selection 文献选择<br>Step2: Comparative analysis 比较分析<br>Step3: Identify research gaps 确定研究空白<br>Output: Systematic insights on CP 系统的CP洞察 |
8.5 | [[8.5] 2504.04701 DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation](https://arxiv.org/abs/2504.04701) <br> [{'name': 'Bo-Wen Yin, Jiao-Long Cao, Ming-Ming Cheng, Qibin Hou'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>RGBD segmentation<br>geometry prior<br>self-attention | Input: RGB and depth images RGB与深度图像<br>Step1: Feature extraction 特征提取<br>Step2: Geometry self-attention mechanism 几何自注意力机制<br>Step3: Model evaluation 模型评估<br>Output: Semantic segmentation results 语义分割结果 |
8.5 | [[8.5] 2504.04744 Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions](https://arxiv.org/abs/2504.04744) <br> [{'name': 'He Zhu, Quyu Kong, Kechun Xu, Xunlong Xia, Bing Deng, Jieping Ye, Rong Xiong, Yue Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object affordance<br>vision-language model<br>robotics | Input: Language instructions, visual observations, and interactions 语言指令、视觉观测和交互<br>Step1: Dataset collection 数据集收集<br>Step2: Multi-modal feature fusion 多模态特征融合<br>Step3: Model implementation and evaluation 模型实施与评估<br>Output: Grounded 3D object affordance 具备位置的3D对象效用 |
8.5 | [[8.5] 2504.04781 OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance](https://arxiv.org/abs/2504.04781) <br> [{'name': 'Chaoyi Wang, Baoqing Li, Xinhan Di'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D-aware supervision<br>occlusion recognition<br>multi-modal<br>large language models | Input: Multi-modal vision-language model and 3D expert reconstruction model 多模态视觉语言模型和3D专家重建模型<br>Step1: Pre-train the vision-language model 预训练视觉语言模型<br>Step2: Train the 3D expert reconstruction model 训练3D专家重建模型<br>Step3: Implement Chain-of-Thoughts learning 实施思维链学习<br>Output: Enhanced recognition of occluded objects 改进的遮挡物体识别 |
8.5 | [[8.5] 2504.04837 Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos](https://arxiv.org/abs/2504.04837) <br> [{'name': 'Zhi Zuo, Chenyi Zhuang, Zhiqiang Shen, Pan Gao, Jie Qin'}] | Point Cloud Processing 点云处理 | v2<br>point cloud videos<br>self-supervised learning<br>4D representation | Input: Point cloud videos 点云视频<br>Step1: Model motion representation in latent space 在潜在空间中建模运动表示<br>Step2: Introduce latent and geometry tokens 引入潜在和几何标记<br>Step3: Train self-disentangled MAE 训练自解耦MAE<br>Output: Discriminative 4D representations 差别化的4D表示 |
8.5 | [[8.5] 2504.04841 Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation](https://arxiv.org/abs/2504.04841) <br> [{'name': 'Sebastian Schmidt, Julius K\\"orner, Dominik Fuchsgruber, Stefano Gasperini, Federico Tombari, Stephan G\\"unnemann'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Panoptic segmentation 泛光分割<br>Anomaly detection 异常检测<br>Evidential learning 证据学习<br>Autonomous driving 自动驾驶 | Input: Pixel-wise binary mask assignments 像素级二进制掩模分配<br>Step1: Incorporate Beta prior 引入Beta先验<br>Step2: Compute model uncertainty 计算模型不确定性<br>Step3: Perform anomaly and panoptic segmentation 执行异常和全景分割<br>Output: State-of-the-art segmentation results 最先进的分割结果 |
8.5 | [[8.5] 2504.05075 PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition](https://arxiv.org/abs/2504.05075) <br> [{'name': 'Jie Wang, Tingfa Xu, Lihe Ding, Xinjie Zhang, Long Bai, Jianan Li'}] | Point Cloud Processing 点云处理 | v2<br>point cloud recognition<br>4D representation learning | Input: Point cloud video sequences 点云视频序列<br>Step1: Motion capture through Motion Imitator 捕获运动通过运动模仿器<br>Step2: One-step query operation from Single-Step Motion Encoder 单步查询操作通过单步运动编码器<br>Output: Efficient point cloud video recognition 高效的点云视频识别 |
8.5 | [[8.5] 2504.05148 Stereo-LiDAR Fusion by Semi-Global Matching With Discrete Disparity-Matching Cost and Semidensification](https://arxiv.org/abs/2504.05148) <br> [{'name': 'Yasuhiro Yao, Ryoichi Ishikawa, Takeshi Oishi'}] | Depth Estimation 深度估计 | v2<br>Depth Estimation 深度估计<br>Sensor Fusion 传感器融合<br>Autonomous Systems 自主系统 | Input: Stereo camera images and LiDAR data 立体相机图像和LiDAR数据<br>Step1: Apply Semi-Global Matching (SGM) to estimate disparity 使用Semi-Global Matching (SGM)估计视差<br>Step2: Implement Discrete Disparity-matching Cost (DDC) for disparity evaluation 实现离散视差匹配成本 (DDC) 用于视差评估<br>Step3: Perform semidensification to enhance disparity maps 进行半密集化以增强视差图<br>Step4: Execute stereo-LiDAR consistency check for validation 执行立体-激光雷达一致性检查以进行验证<br>Output: Accurate depth maps with improved performance 输出：准确的深度图并提高性能 |
8.5 | [[8.5] 2504.05152 PanoDreamer: Consistent Text to 360-Degree Scene Generation](https://arxiv.org/abs/2504.05152) <br> [{'name': 'Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs'}] | 3D Generation 三维生成 | v2<br>3D generation<br>text to 3D<br>geometric consistency | Input: Text description and/or reference image 文本描述和/或参考图像<br>Step1: Generate initial panoramic scene 生成初始全景场景<br>Step2: Lift panorama into 3D 提升全景至三维<br>Step3: Generate images from different viewpoints 根据不同视点生成图像<br>Step4: Compose images into a global point cloud 将图像合成全局点云<br>Step5: Use 3D Gaussian Splatting for final scene rendering 使用3D高斯点云进行最终场景渲染 |
8.5 | [[8.5] 2504.05201 3D Universal Lesion Detection and Tagging in CT with Self-Training](https://arxiv.org/abs/2504.05201) <br> [{'name': 'Jared Frazier, Tejas Sudharshan Mathai, Jianfei Liu, Angshuman Paul, Ronald M. Summers'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D lesion detection<br>self-training<br>computed tomography | Input: CT images 计算机断层扫描图像<br>Step1: Train VFNet model for 2D detection 训练VFNet模型进行二维检测<br>Step2: Expand 2D detection to 3D 将二维检测扩展到三维<br>Step3: Self-training with 3D proposals 进行自我训练以使用3D预测<br>Output: Tagged 3D lesions 标记的三维病变 |
7.5 | [[7.5] 2504.04099 TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection](https://arxiv.org/abs/2504.04099) <br> [{'name': 'Chunzhao Xie, Tongxuan Liu, Lei Jiang, Yuting Zeng, jinrong Guo, Yunheng Shen, Weizhe Huang, Jing Li, Xiaohua Xu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>hallucination mitigation<br>Temporal Attention | Input: Large Vision-Language Models (LVLMs)<br>Step 1: Investigate attention decay correlation with hallucinations<br>Step 2: Propose Temporal Attention Real-time Accumulative Connection (TARAC)<br>Step 3: Integrate TARAC into existing LVLM architectures<br>Output: Enhanced attention mechanisms mitigating hallucinations |
7.5 | [[7.5] 2504.04676 Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering](https://arxiv.org/abs/2504.04676) <br> [{'name': 'Bo Li, Jing Yun'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>Multi-view clustering 多视角聚类<br>Consistency 一致性<br>Complementarity 互补性 | Input: Multi-view data 多视角数据<br>Step1: Separate shared and private information 分离共享和私有信息<br>Step2: Learn consistencies通过对比学习最大化互信息<br>Step3: Apply dual consistency constraints 使用双一致性约束<br>Output: Improved clustering performance 改进的聚类性能 |
7.5 | [[7.5] 2504.04911 IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR](https://arxiv.org/abs/2504.04911) <br> [{'name': 'Ziyun Liang, Xiaoqing Guo, Wentian Xu, Yasin Ibrahim, Natalie Voets, Pieter M Pretorius, J. Alison Noble, Konstantinos Kamnitsas'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anomaly detection<br>MRI segmentation | Input: 3D Brain MRI scans 3D 脑部 MRI 扫描<br>Step1: Spatial masking of images 空间掩蔽图像<br>Step2: Iterative mask refinement 迭代掩蔽精化<br>Step3: Anomaly reconstruction 异常重建<br>Output: Segmented anomalies 细分异常 |
7.0 | [[7.0] 2504.04740 Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data](https://arxiv.org/abs/2504.04740) <br> [{'name': 'Samarth Mishra, Kate Saenko, Venkatesh Saligrama'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>compositional reasoning<br>vision-language models<br>multimodal learning | Input: Multimodal large language models (MLLMs) 多模态大型语言模型<br>Step1: Data augmentation data generation 数据增强数据生成<br>Step2: Preference tuning on synthetic data 通过合成数据进行偏好调整<br>Step3: Model evaluation on compositional benchmarks 模型在合成基准上的评估<br>Output: Improved compositional reasoning capabilities 改进的组合推理能力 |


## Arxiv 2025-04-07

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2504.03052 Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks](https://arxiv.org/abs/2504.03052) <br> [{'name': 'Hyun-Ho Choi, Kangsoo Kim, Ki-Ho Lee, Kisong Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D pose estimation<br>cooperative inference<br>mobile edge computing<br>real-time processing | Input: Images captured by multiple cameras 多摄像头捕获的图像<br>Step1: 2D pose estimation from images 从图像中估计二维姿态<br>Step2: Offloading filtered images to edge server 将筛选后的图像转发到边缘服务器<br>Step3: 3D joint coordinate calculation on edge server 在边缘服务器上计算三维关节坐标<br>Output: Real-time 3D pose estimation 实时三维姿态估计 |
9.5 | [[9.5] 2504.03059 Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization](https://arxiv.org/abs/2504.03059) <br> [{'name': 'Haishan Wang, Mohammad Hassan Vali, Arno Solin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>memory compression<br>3D reconstruction | Input: 3D Gaussian Splatting models 3D高斯点云模型<br>Step1: Build attribute codebooks 构建属性码本<br>Step2: Apply noise-substituted vector quantization 应用噪声替代的向量量化<br>Step3: Optimize memory usage 优化内存使用<br>Output: Compressed 3D representations 压缩的三维表示 |
9.5 | [[9.5] 2504.03164 NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2504.03164) <br> [{'name': 'Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>autonomous driving<br>spatial reasoning<br>3D scene graph | Input: NuScenes dataset with multi-modal sensor data 启用: NuScenes 数据集与多模态传感器数据<br>Step1: 3D scene graph generation pipeline 3D场景图生成管道<br>Step2: QA generation pipeline 问答生成管道<br>Step3: Evaluation of VLMs on spatial understanding and reasoning VLM在空间理解和推理上的评估<br>Output: Benchmark for VLMs in autonomous driving autonomous driving中的VLM基准 |
9.5 | [[9.5] 2504.03177 Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image](https://arxiv.org/abs/2504.03177) <br> [{'name': 'Yuki Kawana, Tatsuya Harada'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>articulated objects<br>RGBD images | Input: Single RGBD image 单个RGBD图像<br>Step1: Part detection 部件检测<br>Step2: Kinematics-aware part fusion 运动学感知部件融合<br>Step3: Anisotropic scale normalization 各向异性尺度归一化<br>Step4: Cross-refinement for output space cross-refinement 在输出空间进行交叉细化<br>Output: Reconstructed articulated shapes 重建的关节形状 |
9.5 | [[9.5] 2504.03198 Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video](https://arxiv.org/abs/2504.03198) <br> [{'name': 'Jiaxin Guo, Wenzhen Dong, Tianyu Huang, Hao Ding, Ziyi Wang, Haomin Kuang, Qi Dou, Yun-Hui Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>monocular video<br>surgical robotics | Input: Monocular surgical videos 单目外科视频<br>Step1: Data integration 数据集成<br>Step2: Algorithm implementation 算法实现<br>Step3: Uncertainty measurement 不确定性测量<br>Step4: Pointmap and depth prediction 点图和深度预测<br>Output: 3D models and camera parameters 3D模型和相机参数 |
9.5 | [[9.5] 2504.03258 TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking](https://arxiv.org/abs/2504.03258) <br> [{'name': 'Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang'}] | 3D Multi-Object Tracking 3D多目标跟踪 | v2<br>3D tracking<br>query denoising<br>autonomous driving | Input: Ground truth detections from previous frame<br>Step1: Generate denoising queries with noise<br>Step2: Propagate denoising queries to current frame<br>Step3: Predict corresponding ground truths<br>Output: Enhanced tracking results |
9.5 | [[9.5] 2504.03438 ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving](https://arxiv.org/abs/2504.03438) <br> [{'name': 'Sheng Yang, Tong Zhan, Shichen Qiao, Jicheng Gong, Qing Yang, Yanfeng Lu, Jian Wang'}] | 3D Object Detection 3D物体检测 | v2<br>3D object perception<br>autonomous driving<br>4D radar | Input: 4D radar and camera data 4D 雷达和相机数据<br>Step1: Fusion of sensor data 传感器数据融合<br>Step2: Feature extraction 特征提取<br>Step3: 3D object detection algorithm 3D物体检测算法<br>Output: Improved object perception 精确的物体感知 |
9.5 | [[9.5] 2504.03563 PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector](https://arxiv.org/abs/2504.03563) <br> [{'name': 'Kaidong Li, Tianxiao Zhang, Kuan-Chuan Peng, Guanghui Wang'}] | 3D Object Detection and LiDAR融合 3D对象检测 | v2<br>3D detection 3D检测<br>LiDAR<br>autonomous driving 自动驾驶 | Input: Camera images and LiDAR point clouds 摄像机图像和激光雷达点云<br>Step1: Data preprocessing 数据预处理<br>Step2: Feature extraction 特征提取 using foundation model encoders<br>Step3: Soft prompt integration 软提示集成 for feature fusion<br>Step4: 3D detection model training 3D检测模型训练<br>Output: Enhanced 3D object detection results 改进的3D物体检测结果 |
9.5 | [[9.5] 2504.03602 Robust Human Registration with Body Part Segmentation on Noisy Point Clouds](https://arxiv.org/abs/2504.03602) <br> [{'name': 'Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, Francis Engelmann'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human meshes<br>body-part segmentation<br>pose estimation<br>noisy point clouds<br>mesh fitting | Input: Noisy point clouds 噪声点云<br>Step1: Body part segmentation body-part segmentation<br>Step2: SMPL-X fitting SMPL-X拟合<br>Step3: Pose and orientation initialization 姿态和方向初始化<br>Step4: Refinement of point cloud alignment 点云对齐细化<br>Output: Accurate human mesh 人体网格 |
9.0 | [[9.0] 2504.03536 HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration](https://arxiv.org/abs/2504.03536) <br> [{'name': 'Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human avatars<br>autonomous driving | Input: Single human image 单幅人像图像<br>Step1: 3D Gaussian Splatting for initial geometry 初步几何构建<br>Step2: Multi-view generation through integration 多视角图像生成<br>Step3: HumanFixer for restoration and refinement 修复与改进流程<br>Output: High-quality, animatable human avatars 输出：高质量可动画人形模型 |
8.5 | [[8.5] 2504.02884 Enhancing Traffic Sign Recognition On The Performance Based On Yolov8](https://arxiv.org/abs/2504.02884) <br> [{'name': 'Baba Ibrahim (Hubei University of Automotive Technology,Hubei University of Automotive Technology), Zhou Kui (Hubei University of Automotive Technology,Hubei University of Automotive Technology)'}] | Autonomous Driving 自动驾驶 | v2<br>Traffic Sign Recognition<br>Yolov8<br>Autonomous Driving | Input: Traffic sign images 交通标志图像<br>Step1: Data augmentation 数据增强<br>Step2: Model training using YOLOv8 使用YOLOv8模型训练<br>Step3: Model evaluation on various datasets 在不同数据集上评估模型<br>Output: Enhanced detection models 改进的检测模型 |
8.5 | [[8.5] 2504.02920 LiDAR-based Object Detection with Real-time Voice Specifications](https://arxiv.org/abs/2504.02920) <br> [{'name': 'Anurag Kulkarni'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>LiDAR<br>object detection<br>autonomous driving<br>real-time voice synthesis | Input: LiDAR and RGB data LiDAR和RGB数据<br>Step1: Data integration 数据集成<br>Step2: Object detection algorithm development 物体检测算法开发<br>Step3: Real-time voice synthesis implementation 实时语音合成实现<br>Output: Real-time feedback and 3D visualizations 实时反馈和3D可视化 |
8.5 | [[8.5] 2504.03047 Attention-Aware Multi-View Pedestrian Tracking](https://arxiv.org/abs/2504.03047) <br> [{'name': 'Reef Alturki, Adrian Hilton, Jean-Yves Guillemaut'}] | Multi-view Stereo 多视角立体 | v2<br>multi-view tracking<br>attention mechanisms<br>pedestrian detection | Input: Multi-view images 多视角图像<br>Step 1: Early-fusion for detection 早期融合进行检测<br>Step 2: Cross-attention mechanism for association 使用交叉注意机制进行关联<br>Step 3: Robust feature propagation 可靠特征传播<br>Output: Enhanced pedestrian tracking performance 改进的人行道跟踪性能 |
8.5 | [[8.5] 2504.03089 SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections](https://arxiv.org/abs/2504.03089) <br> [{'name': 'Prashant Kumar, Dheeraj Vattikonda, Kshitij Madhav Bhat, Kunal Dargan, Prem Kalra'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>LiDAR-based SLAM<br>autonomous driving<br>adversarial attacks<br>point injections | Input: LiDAR scans from autonomous vehicles<br>Step1: Develop a novel autoencoder with segmentation-based attention<br>Step2: Integrate contrastive learning for precise LiDAR reconstructions<br>Step3: Implement point injections to test adversarial attacks<br>Output: Efficacy of point injections on SLAM navigation |
8.5 | [[8.5] 2504.03171 Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion](https://arxiv.org/abs/2504.03171) <br> [{'name': 'Zeyang Zheng, Arman Hosseini, Dong Chen, Omid Shoghli, Arsalan Heydarian'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人技术 | v2<br>obstacle detection<br>e-scooter<br>deep learning<br>sensor fusion | Input: RGB camera and depth camera RGB相机和深度相机<br>Step1: Sensor integration 传感器集成<br>Step2: Obstacle detection using YOLO 使用YOLO进行障碍物检测<br>Step3: Depth data analysis 深度数据分析<br>Output: Real-time obstacle detection results 实时障碍物检测结果 |
8.5 | [[8.5] 2504.03193 Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation](https://arxiv.org/abs/2504.03193) <br> [{'name': 'Xin Zhang, Robby T. Tan'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Domain Generalized Semantic Segmentation<br>Vision Foundation Models<br>Vision-Language Models<br>autonomous driving<br>computational efficiency | Input: Domain data and models 领域数据与模型<br>Step1: Feature extraction 特征提取<br>Step2: Model adaptation 模型适应<br>Step3: Domain generalization evaluation 域泛化评估<br>Output: Enhanced segmentation performance 改进的分割性能 |
8.5 | [[8.5] 2504.03306 Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection](https://arxiv.org/abs/2504.03306) <br> [{'name': 'Mathis Kruse, Bodo Rosenhahn'}] | Multi-view Stereo 多视角立体 | v2<br>Multi-view anomaly detection 多视角异常检测<br>Normalizing flows 正规化流<br>Industrial applications 工业应用 | Input: Multi-view images 多视角图像<br>Step1: Data fusion 融合数据<br>Step2: Cross-view message passing 跨视图信息传递<br>Step3: Anomaly detection 进行异常检测<br>Output: Detected anomalies 检测到的异常 |
8.5 | [[8.5] 2504.03468 D-Garment: Physics-Conditioned Latent Diffusion for Dynamic Garment Deformations](https://arxiv.org/abs/2504.03468) <br> [{'name': 'Antoine Dumoulin, Adnane Boukhayma, Laurence Boissieux, Bharath Bhushan Damodaran, Pierre Hellier, Stefanie Wuhrer'}] | 3D Generation 三维生成 | v2<br>3D Garment Deformation<br>Latent Diffusion Model<br>Dynamic Modeling<br>Vision Sensors | Input: 3D garment template 3D服装模板<br>Step1: Condition on body shape and motion 以身体形状和运动为条件<br>Step2: Use latent diffusion model 使用潜在扩散模型<br>Step3: Optimize to fit observations 最优化以适应观测<br>Output: Dynamically deformed garment output 动态变形服装输出 |
8.5 | [[8.5] 2504.03637 An Algebraic Geometry Approach to Viewing Graph Solvability](https://arxiv.org/abs/2504.03637) <br> [{'name': "Federica Arrigoni, Kathl\\'en Kohn, Andrea Fusiello, Tomas Pajdla"}] | Multi-view Geometry 多视图几何 | v2<br>Viewing Graph<br>Structure-from-Motion<br>Algebraic Geometry | Input: Viewing graph associated with cameras 视图图与相机关联<br>Step1: Develop novel algebraic framework for solvability problems 提出新的代数框架用于求解问题<br>Step2: Analyze conditions for camera determinability 分析相机可确定性的条件<br>Step3: Implement computational methods for graph partitioning and solvability testing 实现图划分和求解测试的计算方法<br>Output: Improved understanding of structure-from-motion graphs and their solvability 改进对运动结构图及其可解性的理解 |
8.0 | [[8.0] 2504.03249 Robot Localization Using a Learned Keypoint Detector and Descriptor with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2504.03249) <br> [{'name': 'Piet Br\\"ommel, Dominik Br\\"amer, Oliver Urbann, Diana Kleingarn'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>robot localization<br>feature extraction | Input: Images of industrial floor 工业地面的图像<br>Step1: Keypoint extraction 关键点提取<br>Step2: Deep learning for features 深度学习获取特征<br>Step3: Position estimation 位置估计<br>Output: Accurate robot localization 准确的机器人定位 |
7.5 | [[7.5] 2504.02876 Multimodal Reference Visual Grounding](https://arxiv.org/abs/2504.02876) <br> [{'name': 'Yangxiao Lu, Ruosen Li, Liqiang Jing, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi, Yu Xiang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal reference visual grounding<br>large vision-language models<br>few-shot object detection | Input: Query image and reference images 输入: 查询图像和参考图像<br>Step1: Dataset creation for MRVG 创建MRVG数据集<br>Step2: Novel method for visual grounding using LLMs 开发基于LLMs的视觉定位新方法<br>Step3: Evaluation of the model's visual grounding performance 模型可视化定位性能的评估<br>Output: Bounding boxes or segmentation masks 输出: 目标对象的边界框或分割掩码 |
7.5 | [[7.5] 2504.03140 Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models](https://arxiv.org/abs/2504.03140) <br> [{'name': 'Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, Harry Yang'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>diffusion models<br>caching strategy | Input: Video sequences 视频序列<br>Step1: Analyze attention distributions 分析注意力分布<br>Step2: Develop adaptive caching strategy 开发自适应缓存策略<br>Step3: Validate through experiments 实验验证<br>Output: Efficient video generation 高效视频生成 |
7.5 | [[7.5] 2504.03154 TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference](https://arxiv.org/abs/2504.03154) <br> [{'name': 'Junshan Hu, Jialiang Mao, Zhikang Liu, Zhongpu Xia, Peng Jia, Xianpeng Lang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Token adaptation | Input: Images as input images 作为输入图像<br>Step1: Stochastic training of vision tokens 随机训练视觉令牌<br>Step2: Dynamic adjustment of token counts 动态调整令牌数量<br>Step3: Experiments on vision-language benchmarks 在视觉-语言基准上的实验<br>Output: Performance evaluation and comparison with fixed-token models 输出：与固定令牌模型的性能评估和比较 |
7.5 | [[7.5] 2504.03440 Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models](https://arxiv.org/abs/2504.03440) <br> [{'name': 'Mirko Borszukovszki, Ivo Pascal de Jong, Matias Valdenegro-Toro'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Language Models<br>Uncertainty Estimation<br>Corrupted Images<br>Large Language Models | Input: Corrupted image data 受损图像数据<br>Step1: Model testing 模型测试<br>Step2: Uncertainty estimation 不确定性估计<br>Step3: Results analysis 结果分析<br>Output: Confidence scores 置信度分数 |
6.0 | [[6.0] 2504.03490 BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution](https://arxiv.org/abs/2504.03490) <br> [{'name': 'Zihao He, Shengchuan Zhang, Runze Hu, Yunhang Shen, Yan Zhang'}] | Image Generation 图像生成 | v2<br>super-resolution<br>diffusion models | Input: Low-resolution images (LR) 低分辨率图像<br>Step1: Bayesian model generates uncertainty masks 贝叶斯模型生成不确定性掩码<br>Step2: Modulation of noise during diffusion process 在扩散过程中对噪声进行调制<br>Step3: Training with enhanced focus on high-uncertainty areas 在高不确定性区域进行增强关注的训练<br>Output: Super-resolved images 高分辨率图像 |
5.0 | [[5.0] 2504.03254 SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding](https://arxiv.org/abs/2504.03254) <br> [{'name': 'Yimin Wei, Aoran Xiao, Yexian Ren, Yuting Zhu, Hongruixuan Chen, Junshi Xia, Naoto Yokoya'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Synthetic Aperture Radar (SAR)<br>Vision-Language Models (VLMs)<br>Image Captioning<br>Visual Question Answering (VQA) | Input: SAR images and corresponding text annotations SAR 图像与对应文本注释<br>Step1: Dataset creation 数据集创建<br>Step2: Model training and evaluation 模型训练与评估<br>Output: Enhanced understanding of SAR images 改进的SAR图像理解 |


## Arxiv 2025-04-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2504.02261 WonderTurbo: Generating Interactive 3D World in 0.72 Seconds](https://arxiv.org/abs/2504.02261) <br> [{'name': 'Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, Wenjun Mei'}] | 3D Generation 三维生成 | v2<br>3D generation<br>real-time rendering<br>interactive 3D | Input: User-provided single image 用户提供的单张图像<br>Step1: Implement StepSplat for geometric updates 实现StepSplat进行几何更新<br>Step2: Use QuickDepth for depth consistency 使用QuickDepth确保深度一致性<br>Step3: Apply FastPaint for appearance inpainting 应用FastPaint进行外观修复<br>Output: Interactive 3D scenes with high-quality output 输出: 高质量的交互式3D场景 |
9.5 | [[9.5] 2504.02270 MinkOcc: Towards real-time label-efficient semantic occupancy prediction](https://arxiv.org/abs/2504.02270) <br> [{'name': 'Samuel Sze, Daniele De Martini, Lars Kunze'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic occupancy prediction<br>autonomous driving | Input: Multi-view images and LiDAR data 多视角图像和激光雷达数据<br>Step1: Warm-start with small dataset of 3D annotations 用小型3D注释数据集进行热启动<br>Step2: Continued training with LiDAR sweeps and images 使用激光雷达扫描和图像进行后续训练<br>Step3: Real-time inference through sparse convolution networks 通过稀疏卷积网络实现实时推断<br>Output: 3D semantic occupancy prediction 3D语义占用预测 |
9.5 | [[9.5] 2504.02316 ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation](https://arxiv.org/abs/2504.02316) <br> [{'name': 'Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han'}] | 3D Generation 三维生成 | text-to-3D generation<br>multi-view consistency<br>view biases<br>visual quality<br>geometry consistency | Input: Text descriptions 文本描述<br>Step1: View Disentanglement Module (VDM) 视图解耦模块<br>Step2: Similarity-based partial order loss 相似性基础的部分顺序损失<br>Output: Geometrically consistent 3D generation 几何一致的3D生成 |
9.5 | [[9.5] 2504.02337 LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images](https://arxiv.org/abs/2504.02337) <br> [{'name': 'Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D room-level scene generation<br>NeRF<br>GAN | Input: In-the-wild images 从野外图像输入<br>Step1: Define local-pose-alignment (LPA) framework 定义局部姿态对齐框架<br>Step2: Implement LPA-GAN for scene generation 实现LPA-GAN进行场景生成<br>Step3: Co-optimize pose predictor and scene generation co-optimizing姿态预测器和场景生成<br>Output: Generated 3D indoor scenes 生成的3D室内场景 |
9.5 | [[9.5] 2504.02356 All-day Depth Completion via Thermal-LiDAR Fusion](https://arxiv.org/abs/2504.02356) <br> [{'name': 'Janghyun Kim, Minseong Kweon, Jinsun Park, Ukcheol Shin'}] | Depth Estimation 深度估计 | v2<br>Depth Completion 深度补全<br>Thermal-LiDAR Fusion 热激光雷达融合<br>Autonomous Driving 自动驾驶 | Input: Sparse LiDAR and RGB images 垂直激光雷达和RGB图像<br>Step1: Benchmark existing algorithms 基准现有算法<br>Step2: Propose COntrastive learning and Pseudo-Supervision framework 提出C对比学习和伪监督框架<br>Step3: Enhance depth boundary clarity 改进深度边界的清晰度<br>Output: Enhanced depth completion performance 改进的深度完成性能 |
9.5 | [[9.5] 2504.02437 MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM](https://arxiv.org/abs/2504.02437) <br> [{'name': 'Renwu Li, Wenjing Ke, Dong Li, Lu Tian, Emad Barsoum'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>3D Gaussian mapping<br>Simultaneous Localization and Mapping (SLAM)<br>RGB inputs<br>Visual odometry | Input: RGB images 仅输入RGB图像<br>Step1: Dynamic 3D Gaussian insertion 动态三维高斯插入<br>Step2: Gaussian densification module 高斯密集模块<br>Step3: Online visual odometry 视觉里程计<br>Output: Accurate 3D mapping 准确的三维映射 |
9.5 | [[9.5] 2504.02464 CornerPoint3D: Look at the Nearest Corner Instead of the Center](https://arxiv.org/abs/2504.02464) <br> [{'name': 'Ruixiao Zhang, Runwei Guan, Xiangyu Chen, Adam Prugel-Bennett, Xiaohao Cai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object detection<br>LiDAR point clouds<br>autonomous driving | Input: LiDAR point clouds from 3D sensors<br>Step1: Analyze object surfaces and centers<br>Step2: Develop EdgeHead for surface detection<br>Step3: Implement CornerPoint3D for corner prediction<br>Output: Enhanced 3D object detection performance |
9.5 | [[9.5] 2504.02762 MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection](https://arxiv.org/abs/2504.02762) <br> [{'name': 'Ahmet Burak Yildirim, Mustafa Utku Aydogdu, Duygu Ceylan, Aysegul Dundar'}] | Image and Video Generation 图像生成 | v2<br>3D shapes<br>text-guided texture generation<br>multi-view consistency | Input: Pretrained text-to-image diffusion models 预训练的文本到图像扩散模型<br>Step1: Implement multi-diffusion consistency mechanism 实现多扩散一致性机制<br>Step2: Fuse noise predictions from multiple views 融合来自多个视角的噪声预测<br>Step3: Generate coherent textures for 3D shapes 生成一致的3D形状纹理<br>Output: Fast and consistent textured 3D models 速度快且一致的纹理3D模型 |
9.5 | [[9.5] 2504.02763 CanonNet: Canonical Ordering and Curvature Learning for Point Cloud Analysis](https://arxiv.org/abs/2504.02763) <br> [{'name': 'Benjy Friedmann, Michael Werman'}] | Point Cloud Processing 点云处理 | v2<br>point cloud processing<br>geometry<br>curvature estimation<br>neural networks | Input: Raw point clouds 原始点云<br>Step1: Preprocessing pipeline for canonical point ordering 预处理管道用于规范点排序<br>Step2: Geometric learning framework for curvature estimation 几何学习框架用于曲率估计<br>Output: Enhanced point cloud features 改进的点云特征 |
9.5 | [[9.5] 2504.02764 Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model](https://arxiv.org/abs/2504.02764) <br> [{'name': 'Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D scene generation<br>video diffusion model<br>momentum | Input: Single image 单幅图像<br>Step1: Construct noisy samples from original features 原始特征构建噪声样本<br>Step2: Introduce pixel-level momentum to generate video 引入像素级动量生成视频<br>Step3: Iteratively recover a 3D scene 迭代恢复3D场景<br>Output: High-fidelity 3D scene 高保真3D场景 |
9.5 | [[9.5] 2504.02817 Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization](https://arxiv.org/abs/2504.02817) <br> [{'name': 'Kangle Deng, Hsueh-Ti Derek Liu, Yiheng Zhu, Xiaoxia Sun, Chong Shang, Kiran Bhat, Deva Ramanan, Jun-Yan Zhu, Maneesh Agrawala, Tinghui Zhou'}] | 3D Generation 三维生成 | v2<br>3D generation 3D生成<br>autoregressive models 自回归模型<br>adaptive tokenization 自适应标记化 | Input: 3D shapes 3D形状<br>Step1: Adaptive tokenization 动态标记化<br>Step2: Octree construction 八叉树构建<br>Step3: Autoregressive shape generation 自回归形状生成<br>Output: High-quality 3D content 高质量3D内容 |
9.0 | [[9.0] 2504.02480 Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging](https://arxiv.org/abs/2504.02480) <br> [{'name': 'Kyungmin Choi, JaKeoung Koo, Stephen McLaughlin, Abderrahim Halimi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>single-photon Lidar<br>Bayesian modeling<br>dual-peak imaging | Input: Single-photon Lidar data 单光子激光雷达数据<br>Step1: Histogram data processing 直方图数据处理<br>Step2: Dual peak feature extraction 双峰特征提取<br>Step3: Bayesian modeling and neural network unrolling 贝叶斯建模与神经网络展开<br>Output: 3D reconstruction results 3D重建结果 |
8.5 | [[8.5] 2504.02158 UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting](https://arxiv.org/abs/2504.02158) <br> [{'name': 'Jaehoon Choi, Dongki Jung, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>digital twins 数字孪生<br>UAV 无人机<br>3D Gaussian Splatting 3D高斯点云 | Input: UAV images UAV 图像<br>Step1: Foreground component synthesis 前景组件合成<br>Step2: Gaussian splatting integration 结合高斯点云<br>Step3: Data augmentation 数据增强<br>Output: Digital twin generation 数字孪生生成 |
8.5 | [[8.5] 2504.02264 MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception](https://arxiv.org/abs/2504.02264) <br> [{'name': 'Wenzhuo Liu, Wenshuo Wang, Yicheng Qiao, Qiannan Guo, Jiayin Zhu, Pengfei Li, Zilong Chen, Huiming Yang, Zhiwei Li, Lening Wang, Tiao Tan, Huaping Liu'}] | Autonomous Driving 自动驾驶 | v2<br>multimodal learning<br>driver assistance systems<br>multi-task learning | Input: Multimodal data (driving context, driver behavior) 驱动上下文、多模态数据（驾驶上下文，驾驶员行为）<br>Step 1: Multi-axis region attention to extract features 从多轴区域关注提取特征<br>Step 2: Dual-branch multimodal embedding to adjust parameters 双支路多模态嵌入调整参数<br>Step 3: Evaluate on AIDE dataset 在AIDE数据集上评估<br>Output: Improved recognition performance 提升的识别性能 |
8.5 | [[8.5] 2504.02454 Taylor Series-Inspired Local Structure Fitting Network for Few-shot Point Cloud Semantic Segmentation](https://arxiv.org/abs/2504.02454) <br> [{'name': 'Changshuo Wang, Shuting He, Xiang Fang, Meiqing Wu, Siew-Kei Lam, Prayag Tiwari'}] | Point Cloud Processing 点云处理 | v2<br>few-shot learning<br>point cloud segmentation<br>3D reconstruction | Input: Point clouds and limited labeled data 点云和有限标注数据<br>Step1: Polynomial fitting for local structure representation 局部结构表示的多项式拟合<br>Step2: Development of TaylorConv for local structure fitting 开发TaylorConv以进行局部结构拟合<br>Step3: Constructing variants of TaylorSeg (TaylorSeg-NN, TaylorSeg-PN) 构建TaylorSeg的变体（TaylorSeg-NN，TaylorSeg-PN）<br>Output: Enhanced segmentation of unseen categories 改进的未见类别分割 |
8.5 | [[8.5] 2504.02517 MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields](https://arxiv.org/abs/2504.02517) <br> [{'name': 'Yash Kulthe, Andrew Gilbert, John Collomosse'}] | Neural Rendering 神经渲染 | v2<br>3D watermarking<br>Neural Radiance Fields<br>intellectual property<br>3D content | Input: NeRF model with watermarking grid 采用带水印网格的NeRF模型<br>Step1: Extend TensoRF with watermark grid 扩展TensoRF以包含水印网格<br>Step2: Implement FiLM-based conditional modulation 实现基于FiLM的条件调制<br>Step3: Train the model with watermark embedding 训练模型以嵌入水印<br>Output: NeRF model with multiple watermarks 输出：带有多个水印的NeRF模型 |
8.5 | [[8.5] 2504.02617 PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation](https://arxiv.org/abs/2504.02617) <br> [{'name': 'Lihua Liu, Jiehong Lin, Zhenxin Liu, Kui Jia'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>pose estimation<br>3D models<br>correspondence learning | Input: RGB images and CAD models RGB图像和CAD模型<br>Step1: Feature matching for coarse correspondences 特征匹配以获得粗略对应<br>Step2: Global transformation estimation for smooth correspondences 全局变换估计以平滑对应<br>Step3: Local refinement for fine correspondences 局部细化以优化对应<br>Output: 6D object pose estimation 6D物体姿态估计 |
8.5 | [[8.5] 2504.02782 GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation](https://arxiv.org/abs/2504.02782) <br> [{'name': 'Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, Li Yuan'}] | Image Generation 图像生成 | v2<br>image generation<br>benchmark<br>GPT-4o | Input: GPT-4o model outputs<br>Step1: Benchmark creation for evaluation<br>Step2: Qualitative and quantitative analysis of generated images<br>Step3: Comparative study with other models<br>Output: Insights on generative performance and limitations |
8.5 | [[8.5] 2504.02812 BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation](https://arxiv.org/abs/2504.02812) <br> [{'name': 'Van Nguyen Nguyen, Stephen Tyree, Andrew Guo, Mederic Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann Labbe, Martin Sundermeyer, Tomas Hodan'}] | 6D Object Pose Estimation 6D物体位姿估计 | v2<br>6D pose estimation<br>object detection<br>model-based<br>model-free | Input: 6D object pose estimation task 6D物体位姿估计任务<br>Step1: Develop evaluation methodology 开发评估方法<br>Step2: Introduce new datasets 引入新数据集<br>Step3: Implement model-based and model-free approaches 实现基于模型和无模型的方法<br>Output: Results of the BOP Challenge 2024 2024 BOP挑战的结果 |
7.5 | [[7.5] 2504.02799 Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence](https://arxiv.org/abs/2504.02799) <br> [{'name': 'Anita Rau, Mark Endo, Josiah Aklilu, Jaewoo Heo, Khaled Saab, Alberto Paderno, Jeffrey Jopling, F. Christopher Holsinger, Serena Yeung-Levy'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>surgical AI | Input: Large Vision-Language Models 视觉语言模型<br>Step1: Comprehensive analysis of VLMs 对VLM的综合分析<br>Step2: Performance evaluation on surgical tasks 对外科任务的性能评估<br>Step3: Insights on adaptability 适应性洞察<br>Output: Insights for surgical AI 外科人工智能的洞察 |
7.5 | [[7.5] 2504.02821 Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/abs/2504.02821) <br> [{'name': 'Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Sparse Autoencoders<br>Vision-Language Models<br>Interpretability | Input: Sparse Autoencoders (SAEs) 稀疏自编码器<br>Step1: Framework introduction 框架介绍<br>Step2: Monosemanticity evaluation 单义性评估<br>Step3: Application to VLMs 应用到视觉语言模型<br>Output: Enhanced interpretability of VLMs 改进的视觉语言模型可解释性 |


## Arxiv 2025-04-03

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2504.01023 Omnidirectional Depth-Aided Occupancy Prediction based on Cylindrical Voxel for Autonomous Driving](https://arxiv.org/abs/2504.01023) <br> [{'name': 'Chaofan Wu, Jiaheng Li, Jinghao Cao, Ming Li, Yongkang Feng, Jiayu Wu Shuwen Xu, Zihang Gao, Sidan Du, Yang Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D perception<br>occupancy prediction<br>autonomous driving<br>cylindrical voxel | Input: Omnidirectional depth data 全向深度数据<br>Step1: Build cylindrical voxel representation 构建圆柱体体素表示<br>Step2: Implement Sketch-Coloring framework 实现素描上色框架<br>Step3: Evaluate occupancy prediction performance 评估占用预测性能<br>Output: Enhanced 3D occupancy prediction 改进的3D占用预测 |
9.5 | [[9.5] 2504.01503 Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment](https://arxiv.org/abs/2504.01503) <br> [{'name': 'Ziteng Cui, Xuangeng Chu, Tatsuya Harada'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting 3D高斯点云<br>novel view synthesis 新视图合成<br>lighting adaptation 光照适应 | Input: Multi-view images 多视角图像<br>Step1: Image processing with per-view color matrix mapping 使用每视图的颜色矩阵映射进行图像处理<br>Step2: Curve adjustment to adapt to lighting conditions 曲线调整以适应光照条件<br>Step3: Joint optimization with 3DGS parameters 与3DGS参数共同优化<br>Output: Enhanced novel views 改进的新视图 |
9.5 | [[9.5] 2504.01512 High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model](https://arxiv.org/abs/2504.01512) <br> [{'name': 'Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao'}] | 3D Generation 三维生成 | v2<br>3D generation 三维生成<br>Gaussian splatting 高斯点云 | Input: Single-view images 单视图图像<br>Step1: Feature extraction 特征提取<br>Step2: Gaussian generation 高斯生成<br>Step3: 3D reconstruction 3D重建<br>Output: High-fidelity 3D objects 高保真3D物体 |
9.5 | [[9.5] 2504.01559 RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable 3D Gaussian Avatars](https://arxiv.org/abs/2504.01559) <br> [{'name': 'Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Dynamic Clothing Modeling<br>Animatable Avatars | Input: Multi-view videos 多视角视频<br>Step1: Motion trend modeling 动态趋势建模<br>Step2: Skeletal feature encoding 骨骼特征编码<br>Step3: Clothing deformation capture 服装变形捕捉<br>Output: High-fidelity animatable avatars 高保真动画化虚拟人像 |
9.5 | [[9.5] 2504.01619 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting](https://arxiv.org/abs/2504.01619) <br> [{'name': 'Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>bonsai generation<br>Gaussian splatting | Input: Text descriptions and conditions 输入: 文本描述和条件<br>Step1: Design trainable 3D space colonization algorithm 第一步: 设计可训练的三维空间殖民算法<br>Step2: Generate bonsai structures using structure-aware 3D Gaussian splatting 第二步: 使用结构感知的三维高斯点云生成盆栽结构<br>Step3: Evaluate model with 2D-3D consistency checks 第三步: 使用2D-3D一致性检查评估模型<br>Output: Complex 3D bonsai models 输出: 复杂的三维盆栽模型 |
9.5 | [[9.5] 2504.01641 Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment](https://arxiv.org/abs/2504.01641) <br> [{'name': 'Zhixin Cheng, Jiacheng Deng, Xinjun Li, Baoqun Yin, Tianzhu Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image registration<br>point cloud | Input: Image and point cloud data 图像和点云数据<br>Step1: Image-to-point cloud registration 基于图像至点云的配准<br>Step2: Uncertainty-aware matching 关注不确定性匹配<br>Step3: Domain alignment 域对齐<br>Output: Accurate transformations for 3D reconstruction 适用于三维重建的准确变换 |
9.5 | [[9.5] 2504.01647 FlowR: Flowing from Sparse to Dense 3D Reconstructions](https://arxiv.org/abs/2504.01647) <br> [{'name': 'Tobias Fischer, Samuel Rota Bul\\`o, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman M\\"uller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>novel view synthesis<br>multi-view<br>flow matching<br>Gaussian splatting | Input: A set of 2D images of a 3D scene 场景的二维图像集<br>Step1: Data collection and preprocessing 数据收集与预处理<br>Step2: 3D reconstruction using 3D Gaussian splatting 采用3D高斯喷溅进行三维重建<br>Step3: Flow matching to connect sparse and dense renderings 使用流匹配连接稀疏和密集渲染<br>Output: Improved novel view synthesis and 3D reconstruction 改进的视图合成和三维重建 |
9.5 | [[9.5] 2504.01732 FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking](https://arxiv.org/abs/2504.01732) <br> [{'name': 'Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene reconstruction<br>fisheye image dataset | Input: Fisheye images 鱼眼图像<br>Step1: Dataset collection 数据集收集<br>Step2: Point cloud generation 点云生成<br>Step3: Model evaluation 模型评估<br>Output: Benchmarking results 基准测试结果 |
9.5 | [[9.5] 2504.01844 BOGausS: Better Optimized Gaussian Splatting](https://arxiv.org/abs/2504.01844) <br> [{'name': "St\\'ephane Pateux, Matthieu Gendrin, Luce Morin, Th\\'eo Ladune, Xiaoran Jiang"}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>novel view synthesis<br>optimization<br>high-fidelity rendering | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Analyze training process 分析训练过程<br>Step2: Propose optimization methodology 提出优化方法<br>Step3: Model evaluation and comparison 模型评估与比较<br>Output: Optimized Gaussian models 优化的高斯模型 |
9.5 | [[9.5] 2504.01872 CoMatcher: Multi-View Collaborative Feature Matching](https://arxiv.org/abs/2504.01872) <br> [{'name': 'Jintao Zhang, Zimin Xia, Mingyue Dong, Shuhan Shen, Linwei Yue, Xianwei Zheng'}] | Multi-view Stereo 多视角立体 | v2<br>3D reconstruction<br>multi-view matching<br>deep learning<br>feature matching | Input: Image set of a scene 场景的图像集<br>Step1: Group images based on co-visibility 根据可见性分组图像<br>Step2: Collaborative matching using CoMatcher 使用CoMatcher进行协同匹配<br>Step3: Establish correspondence for 3D reconstruction 建立对应关系以进行3D重建<br>Output: Reliable multi-view matches 可靠的多视角匹配 |
9.5 | [[9.5] 2504.01901 Ross3D: Reconstructive Visual Instruction Tuning with 3D-Awareness](https://arxiv.org/abs/2504.01901) <br> [{'name': 'Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Zhaoxiang Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>visual instruction tuning | Input: Multi-view images 多视角图像<br>Step1: Cross-view reconstruction 交叉视图重建<br>Step2: Global-view reconstruction 全局视图重建<br>Step3: 3D representation learning 3D 表示学习<br>Output: Enhanced understanding of 3D scenes 改进的三维场景理解 |
9.5 | [[9.5] 2504.01956 VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step](https://arxiv.org/abs/2504.01956) <br> [{'name': 'Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene generation<br>video diffusion models<br>sparse views | Input: Sparse views and corresponding camera poses 输入: 稀疏视图和对应的相机姿态<br>Step1: Coarse scene generation using a sparse-view 3DGS model 第一步: 使用稀疏视图3DGS模型生成粗略场景<br>Step2: Rapid distillation through a leap flow strategy 第二步: 通过跃流策略快速蒸馏<br>Step3: Denoising with a dynamic policy network 第三步: 使用动态策略网络去噪<br>Output: 3D scenes generated from video input 输出: 从视频输入生成的3D场景 |
9.5 | [[9.5] 2504.01957 GaussianLSS -- Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting](https://arxiv.org/abs/2504.01957) <br> [{'name': 'Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D perception<br>Depth estimation<br>Autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Implement uncertainty modeling 实现不确定性建模<br>Step2: Transform depth distribution into 3D Gaussians 将深度分布转化为3D高斯分布<br>Step3: Rasterize for BEV feature construction 为BEV特征构建进行光栅化<br>Output: Uncertainty-aware BEV features 不确定性感知的BEV特征 |
9.5 | [[9.5] 2504.01960 Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2504.01960) <br> [{'name': 'Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>view synthesis<br>Gaussian Splatting<br>multi-view<br>diffusion models | Input: Multi-view images 多视角图像<br>Step1: Generate pseudo-observations using a diffusion model 通过扩散模型生成伪观察<br>Step2: Apply 3D Gaussian Splatting for optimization 使用三维高斯点云进行优化<br>Step3: Integrate appearance embeddings and depth priors 集成外观嵌入和深度先验<br>Output: Enhanced 3D reconstruction and novel views 输出：改进的三维重建和新视图 |
9.2 | [[9.2] 2504.01476 Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction](https://arxiv.org/abs/2504.01476) <br> [{'name': 'Junlong Ren, Hao Wang'}] | Cross-modal 3D Retrieval 跨模态3D检索 | v2<br>3D retrieval<br>multi-view images<br>point clouds<br>text modalities | Input: Multi-view images and point clouds 多视角图像和点云<br>Step1: Joint representation of 3D shapes 3D形状的联合表示<br>Step2: Tri-modal reconstruction 三模态重建<br>Step3: Fine-grained 2D-3D fusion 细粒度2D-3D融合<br>Output: Multimodal embeddings with enhanced alignment 输出：增强对齐的多模态嵌入 |
9.0 | [[9.0] 2504.01596 DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image](https://arxiv.org/abs/2504.01596) <br> [{'name': 'Jijun Xiang, Xuan Zhu, Xianqi Wang, Yu Wang, Hong Zhang, Fei Guo, Xin Yang'}] | Depth Estimation 深度估计 | v2<br>depth enhancement<br>dToF<br>3D reconstruction<br>depth completion | Input: Raw dToF signals and RGB images 原始dToF信号与RGB图像<br>Step1: Simulate real-world dToF data using synthetic datasets 使用合成数据集模拟真实世界的dToF数据<br>Step2: Develop a depth completion network integrating monocular depth estimation (MDE) 开发整合单目深度估计的深度补全网络<br>Step3: Perform training with noise-robust strategy 使用抗噪声的训练策略进行训练<br>Output: High-precision dense depth maps 高精度密集深度图 |
9.0 | [[9.0] 2504.01941 End-to-End Driving with Online Trajectory Evaluation via BEV World Model](https://arxiv.org/abs/2504.01941) <br> [{'name': 'Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, Zhaoxiang Zhang'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>trajectory evaluation<br>world model | Input: Sensor data 传感器数据<br>Step1: Trajectory prediction 轨迹预测<br>Step2: Future state prediction 未来状态预测<br>Step3: Trajectory evaluation 轨迹评估<br>Output: Optimized trajectories 优化的轨迹 |
8.5 | [[8.5] 2504.01040 Cal or No Cal? -- Real-Time Miscalibration Detection of LiDAR and Camera Sensors](https://arxiv.org/abs/2504.01040) <br> [{'name': 'Ilir Tahiraj, Jeremialie Swadiryus, Felix Fent, Markus Lienkamp'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>miscalibration detection<br>sensor fusion<br>autonomous driving<br>3D sensing | Input: LiDAR and camera data 数据集成: LiDAR和摄像头数据<br>Step1: Feature extraction 特征提取<br>Step2: Miscalibration state classification 失调状态分类<br>Step3: Performance analysis 性能分析<br>Output: Detection results 检测结果 |
8.5 | [[8.5] 2504.01298 Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation](https://arxiv.org/abs/2504.01298) <br> [{'name': 'Shiyong Liu, Zhihao Li, Xiao Tang, Jianzhuang Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D hand pose estimation<br>direction-aware hybrid features<br>joint optimization<br>motion capture | Input: RGB images from hand motion capture<br>Step1: Fusion of implicit image features and explicit 2D joint coordinates<br>Step2: Joint optimization of 2D and 3D coordinates<br>Step3: Motion capture confidence calculation based on contrastive learning<br>Output: Improved accuracy in 3D hand pose and shape estimation |
8.5 | [[8.5] 2504.01428 MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation](https://arxiv.org/abs/2504.01428) <br> [{'name': 'Zhuangzhuang Chen, Hualiang Wang, Chubin Ou, Xiaomeng Li'}] | 3D Image Translation 三维图像翻译 | v2<br>3D image translation<br>multi-view alignment<br>optical coherence tomography<br>OCTA | Input: 3D Optical Coherence Tomography (OCT) images 3D光学相干断层扫描图像<br>Step1: Pre-train VQ-VAE models for OCT & OCTA data 对OCT和OCTA数据进行VQ-VAE模型预训练<br>Step2: Multi-view tri-alignment to learn mapping from OCT to OCTA using three views 三视角联合对齐学习从OCT到OCTA的映射<br>Output: Translated 3D OCTA images 翻译后的3D OCTA图像 |
8.5 | [[8.5] 2504.01449 Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement](https://arxiv.org/abs/2504.01449) <br> [{'name': 'Zaipeng Duan, Xuzhong Hu, Pei An, Jie Ma'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Segmentation 点云分割<br>Multi-modal Integration 多模态集成 | Input: LiDAR and image data (virtual points) 激光雷达与图像数据（虚拟点）<br>Step1: Integration of virtual points from images 通过图像整合虚拟点<br>Step2: Adaptive filtering to select valuable pseudo points 采用自适应过滤选择有价值的伪点<br>Step3: Noise-robust feature extraction 噪声稳健特征提取<br>Output: Enhanced semantic segmentation results 改进的语义分割结果 |
8.5 | [[8.5] 2504.01466 Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes](https://arxiv.org/abs/2504.01466) <br> [{'name': 'Kaiwei Zhang, Dandan Zhu, Xiongkuo Min, Guangtao Zhai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>mesh saliency<br>3D reconstruction<br>texture integration | Input: Mesh models 网格模型<br>Step1: Dataset creation 数据集创建<br>Step2: Model development 模型开发<br>Step3: Validation experiments 验证实验<br>Output: Saliency predictions for meshes 网格的显著性预测 |
8.5 | [[8.5] 2504.01620 A Conic Transformation Approach for Solving the Perspective-Three-Point Problem](https://arxiv.org/abs/2504.01620) <br> [{'name': 'Haidong Wu, Snehal Bhayani, Janne Heikkil\\"a'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Perspective-Three-Point problem<br>conic transformation<br>camera pose estimation | Input: 3D points and their 2D projections 3D点及其2D投影<br>Step1: Coordinate transformation 坐标变换<br>Step2: Solving for intersection points 求交点<br>Step3: Extracting camera pose 信息提取相机位置<br>Output: Camera pose and optimized parameters 输出：相机位置和优化参数 |
8.5 | [[8.5] 2504.01648 ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation](https://arxiv.org/abs/2504.01648) <br> [{'name': 'Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D point cloud segmentation 3D点云分割<br>class-incremental learning 类增量学习<br>ProtoGuard | Input: 3D point clouds 3D点云<br>Step1: Base-class training with prototypes 基类训练与原型<br>Step2: Novel-class training with pseudo-labels 新类训练与伪标签<br>Step3: Evaluation of segmentations 分割的评估<br>Output: Enhanced segmentation accuracy 改进的分割精度 |
8.5 | [[8.5] 2504.01659 Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks](https://arxiv.org/abs/2504.01659) <br> [{'name': 'Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding'}] | 3D Point Cloud Processing 点云处理 | v2<br>3D point cloud segmentation<br>unsupervised domain adaptation<br>adversarial robustness | Input: 3D point cloud data 3D点云数据<br>Step1: Adversarial point cloud generation 攻击点云生成<br>Step2: Dataset formulation 数据集构建<br>Step3: Framework development 框架开发<br>Output: Robust segmentation model 稳健的分割模型 |
8.5 | [[8.5] 2504.01668 Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2504.01668) <br> [{'name': 'Junjie Chen, Yuecong Xu, Haosheng Li, Kemi Ding'}] | 3D Semantic Segmentation 三维语义分割 | v2<br>3D semantic segmentation<br>unsupervised domain adaptation<br>autonomous driving | Input: 3D point cloud data 3D点云数据<br>Step1: Robustness evaluation评估鲁棒性<br>Step2: Invertible attention alignment构建可逆注意力对齐模块<br>Step3: Contrastive memory bank construction构建对比记忆库<br>Output: Enhanced segmentation performance改进的分割性能 |
8.5 | [[8.5] 2504.01764 Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2504.01764) <br> [{'name': 'Mingrui Ye, Lianping Yang, Hegui Zhu, Zenghao Zheng, Xin Wang, Yantao Lo'}] | 3D Human Pose Estimation 3D人类姿态估计 | v2<br>3D human pose estimation<br>Transformer<br>GCN | Input: RGB images and videos from a single viewpoint 使用单一视角的RGB图像和视频<br>Step1: Masking 2D pose features 对2D姿态特征进行掩蔽<br>Step2: Learning representations using Transformer-GCN model 使用Transformer-GCN模型学习表示<br>Step3: Adaptive fusion of features 特征的自适应融合<br>Output: Enhanced 3D human pose estimations 改进的3D人类姿态估计 |
8.0 | [[8.0] 2504.01589 Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models](https://arxiv.org/abs/2504.01589) <br> [{'name': 'Zhaochen Wang, Yujun Cai, Zi Huang, Bryan Hooi, Yiwei Wang, Ming-Hsuan Yang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>ASCII Art ASCII艺术 | Step1: Evaluate five state-of-the-art VLMs on ASCII art tasks 测试五个最先进的视觉语言模型 |
7.5 | [[7.5] 2504.01308 Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2504.01308) <br> [{'name': 'Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Gaussian noise<br>adversarial attacks | Input: VLMs and noisy visual inputs (e.g., images with Gaussian noise)<br>Step1: Conduct vulnerability analysis of VLMs absent noise augmentation<br>Step2: Develop Robust-VLGuard dataset with noise-augmented fine-tuning<br>Step3: Evaluate the performance of enhanced VLMs against adversarial perturbations<br>Output: A robust VLM framework able to handle Gaussian noise and improve functionality |


## Arxiv 2025-04-02

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.22986 FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2503.22986) <br> [{'name': 'Yunsong Wang, Tianxin Huang, Hanlin Chen, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian splatting<br>indoor scene reconstruction<br>multi-view images | Input: Multi-view images 多视角图像<br>Step1: Low-cost Cross-View Aggregation framework 低成本跨视角聚合框架<br>Step2: Pixel-wise triplet fusion method 像素级三重融合方法<br>Step3: Weighted floater removal strategy 加权漂浮物去除策略<br>Step4: Depth-regularized per-scene fine-tuning depth-正则化的逐场景微调<br>Output: Enhanced 3D scene reconstruction 改进的三维场景重建 |
9.5 | [[9.5] 2503.23022 MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs](https://arxiv.org/abs/2503.23022) <br> [{'name': 'Xianglong He, Junyi Chen, Di Huang, Zexiang Liu, Xiaoshui Huang, Wanli Ouyang, Chun Yuan, Yangguang Li'}] | Mesh Reconstruction 网格重建 | v2<br>3D reconstruction<br>mesh generation<br>deep learning | Input: Raw mesh data 原始网格数据<br>Step1: Encode meshes into continuous tokens 编码网格为连续标记<br>Step2: Use flow-based model to generate meshes 使用基于流的模型生成网格<br>Step3: Output the final mesh based on face control 根据面数控制输出最终网格 |
9.5 | [[9.5] 2503.23024 Empowering Large Language Models with 3D Situation Awareness](https://arxiv.org/abs/2503.23024) <br> [{'name': 'Zhihao Yuan, Yibo Peng, Jinke Ren, Yinghong Liao, Yatong Han, Chun-Mei Feng, Hengshuang Zhao, Guanbin Li, Shuguang Cui, Zhen Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D scene understanding<br>Vision-Language Models<br>situational awareness | Input: RGB-D videos RGB-D 视频<br>Step1: Data collection 数据收集<br>Step2: Caption generation 标题生成<br>Step3: Situation grounding 位置基础<br>Output: Situation-aware dataset 情境感知数据集 |
9.5 | [[9.5] 2503.23044 CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction](https://arxiv.org/abs/2503.23044) <br> [{'name': 'Yuanyuan Gao, Hao Li, Jiaqi Chen, Zhengyu Zou, Zhihang Zhong, Dingwen Zhang, Xiao Sun, Junwei Han'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>large-scale reconstruction 大规模重建<br>geometric accuracy 几何准确性<br>3D scene modeling 三维场景建模<br>autonomous driving 自动驾驶 | Input: Multi-view images 多视角图像<br>Step1: Develop parallelized hybrid hierarchical 3D representation 构建并行化的混合层次三维表示<br>Step2: Implement batch-level multi-task rendering 采用批量级别的多任务渲染<br>Step3: Conduct experiments on large-scale datasets 在大规模数据集上进行实验<br>Output: Enhanced large-scale 3D scene models 改进的大规模三维场景模型 |
9.5 | [[9.5] 2503.23162 NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations](https://arxiv.org/abs/2503.23162) <br> [{'name': 'Zhenyu Tang, Chaoran Feng, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liu, Xiaoxiao Long, Wenping Wang, Li Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>neural fields<br>3D reconstruction<br>compression methods<br>multilayer perceptron | Input: Original 3D Gaussian Splatting (3DGS) data  原始三维高斯点云(3DGS)数据<br>Step1: Compute Gaussian importance scores 计算高斯重要性分数<br>Step2: Prune less important Gaussians 修剪不太重要的高斯<br>Step3: Cluster Gaussians based on attributes 根据属性聚类高斯<br>Step4: Fit separate MLPs for each cluster 为每个聚类拟合不同的多层感知器 (MLPs)<br>Step5: Fine-tune NeuralGS representation and apply frequency loss 对NeuralGS表示进行微调并应用频率损失<br>Output: Compact 3D representation with reduced storage requirements 输出：具有减小存储要求的紧凑3D表示 |
9.5 | [[9.5] 2503.23282 AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos](https://arxiv.org/abs/2503.23282) <br> [{'name': 'Felix Wimbauer, Weirong Chen, Dominik Muhle, Christian Rupprecht, Daniel Cremers'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>camera poses<br>intrinsics<br>3D reconstruction<br>SfM<br>dynamic videos | Input: Casual video inputs 休闲视频输入<br>Step1: Preprocess video with depth and flow networks 通过深度和流网络预处理视频<br>Step2: Apply transformer model to estimate camera poses and intrinsics 应用变换器模型估计相机姿态和内参<br>Step3: Implement trajectory refinement to reduce drift 实施轨迹优化以减少漂移<br>Output: Accurate camera poses, intrinsics, and 4D pointclouds 输出: 精确的相机姿态、内参和4D点云 |
9.5 | [[9.5] 2503.23297 ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning](https://arxiv.org/abs/2503.23297) <br> [{'name': 'Zhenyang Liu, Yikai Wang, Sixiao Zheng, Tongying Pan, Longfei Liang, Yanwei Fu, Xiangyang Xue'}] | 3D Visual Grounding 三维视觉定位 | v2<br>3D visual grounding<br>open-vocabulary<br>neural rendering | Input: Implicit language descriptions 语言描述<br>Step1: Adaptive grouping based on physical scale 基于物理尺度的自适应分组<br>Step2: 3D Gaussian feature splatting 3D高斯特征喷涂<br>Step3: Object localization 物体定位<br>Output: Accurate 3D grounding and reasoning 精确的3D定位与推理 |
9.5 | [[9.5] 2503.23337 Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction](https://arxiv.org/abs/2503.23337) <br> [{'name': 'Jingui Ma, Yang Hu, Luyang Tang, Jiayu Yang, Yongqi Zhai, Ronggang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Compression<br>Novel View Synthesis<br>Real-time Rendering | Input: 3D Gaussian representation 3D高斯表示<br>Step1: Introduce prediction technique 引入预测技术<br>Step2: Implement spatial condition-based prediction 实施基于空间条件的预测<br>Step3: Develop instance-aware hyper prior model 开发基于实例感知的超先验模型<br>Output: Compressed 3D Gaussian models 压缩的3D高斯模型 |
9.5 | [[9.5] 2503.23463 OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model](https://arxiv.org/abs/2503.23463) <br> [{'name': 'Xingcheng Zhou, Xuyuan Han, Feng Yang, Yunpu Ma, Alois C. Knoll'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>Vision-Language Model 视觉-语言模型<br>Autonomous Driving 自动驾驶<br>Trajectory Generation 轨迹生成 | Input: Multimodal inputs (3D environmental perception, vehicle state, driver commands)  输入：多模态输入（3D环境感知、车辆状态、驾驶员命令）<br>Step1: Hierarchical vision-language alignment 模块  步骤1：分层视觉-语言对齐模块<br>Step2: Autoregressive interaction modeling  步骤2：自回归交互建模<br>Step3: Trajectory generation  步骤3：轨迹生成<br>Output: Reliable driving trajectories  输出：可靠的驾驶轨迹 |
9.5 | [[9.5] 2503.23502 Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model](https://arxiv.org/abs/2503.23502) <br> [{'name': 'Jannik Endres, Oliver Hahn, Charles Corbi\\`ere, Simone Schaub-Meyer, Stefan Roth, Alexandre Alahi'}] | Stereo Vision 立体视觉 | v2<br>omnidirectional stereo matching<br>depth estimation<br>robotics | Input: Equirectangular images captured by two vertically stacked omnidirectional cameras 拍摄的两个垂直堆叠的全景相机的等距图像<br>Step1: Integrate the pre-trained monocular depth foundation model into the stereo matching architecture 将预训练的单眼深度基础模型集成到立体匹配架构中<br>Step2: Apply a two-stage training strategy to adapt features to omnidirectional stereo matching 采用两阶段训练策略将特征适应于全景立体匹配<br>Step3: Fine-tune the model using scale-invariant loss against actual depth data 使用无尺度损失对实际深度数据微调模型<br>Output: Enhanced disparity estimation and improved depth accuracy 改进的视差估计和深度准确性 |
9.5 | [[9.5] 2503.23664 LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds](https://arxiv.org/abs/2503.23664) <br> [{'name': 'Masahiko Tsuji, Hitoshi Niigaki, Ryuichi Tanida'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Visual Localization 视觉定位<br>3D Reconstruction 三维重建<br>LiDAR<br>Camera Pose Estimation 相机姿态估计 | Input: Query image and 3D LiDAR point clouds 查询图像和3D LiDAR点云<br>Step1: Extract keypoints from the reference image 从参考图像中提取关键点<br>Step2: Generate a 3D reference map with keypoints using LiDAR 生成包含关键点的3D参考地图，使用LiDAR<br>Step3: Assign 3D LiDAR points directly to 2D keypoints 直接将3D LiDAR点分配给2D关键点<br>Output: Enhanced camera pose estimation through a dense 3D reference map 输出：通过密集的3D参考地图增强相机姿态估计 |
9.5 | [[9.5] 2503.23670 Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation](https://arxiv.org/abs/2503.23670) <br> [{'name': 'Takeshi Noda, Chao Chen, Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu, Zhizhong Han'}] | Surface Reconstruction 表面重建 | v2<br>3D reconstruction<br>signed distance functions<br>sparse point clouds | Input: Sparse point clouds 稀疏点云<br>Step1: Learn bijective surface parameterization (BSP) 学习双射表面参数化<br>Step2: Construct dynamic deformation network 动态变形网络构建<br>Step3: Optimize grid deformation to refine surfaces 优化网格变形以精炼表面<br>Output: Signed distance functions (SDF) representation of the surface 表面的符号距离函数表示 |
9.5 | [[9.5] 2503.23684 Detail-aware multi-view stereo network for depth estimation](https://arxiv.org/abs/2503.23684) <br> [{'name': 'Haitao Tian, Junyang Li, Chenxing Wang, Helong Jiang'}] | Multi-view Stereo 多视角立体 | v2<br>Multi-view stereo<br>Depth estimation<br>3D reconstruction<br>Geometric depth | Input: Multi-view images 多视角图像<br>Step1: Geometric depth embedding 数据几何深度嵌入<br>Step2: Image synthesis loss enhancement 图像合成损失增强<br>Step3: Adaptive depth interval adjustment 自适应深度区间调整<br>Output: Accurate depth maps 精确的深度图 |
9.5 | [[9.5] 2503.23747 Consistency-aware Self-Training for Iterative-based Stereo Matching](https://arxiv.org/abs/2503.23747) <br> [{'name': 'Jingyi Zhou, Peng Ye, Haoyu Zhang, Jiakang Yuan, Rao Qiang, Liu YangChenXu, Wu Cailin, Feng Xu, Tao Chen'}] | Multi-view Stereo 多视角立体 | v2<br>stereo matching<br>depth estimation<br>3D vision | Input: Pairs of rectified images 视差分割的处理方式<br>Step1: Introduce consistency-aware self-training framework 引入一致性自我训练框架<br>Step2: Implement consistency-aware soft filtering module 实现一致性软过滤模块<br>Step3: Adjust weights of pseudo-labels with soft-weighted loss 使用软加权损失调整伪标签权重<br>Output: Enhanced stereo matching performance 提升的立体匹配性能 |
9.5 | [[9.5] 2503.23881 ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image](https://arxiv.org/abs/2503.23881) <br> [{'name': 'Tianyi Gong, Boyan Li, Yifei Zhong, Fangxin Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>single-view reconstruction<br>Gaussian Splatting<br>panoramic image generation | Input: Single-view image 单视图图像<br>Step1: Generate panoramic image 生成全景图像<br>Step2: Depth estimation 深度估计<br>Step3: 3D Gaussian Splatting model training 训练3D高斯点云模型<br>Step4: Refinement with video diffusion 通过视频扩散进行优化<br>Output: Consistent immersive 3D scene 一致的沉浸式3D场景 |
9.5 | [[9.5] 2503.23965 Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving](https://arxiv.org/abs/2503.23965) <br> [{'name': 'Miao Fan, Xuxu Kong, Shengtong Xu, Haoyi Xiong, Xiangzeng Liu'}] | Autonomous Driving 自动驾驶 | v2<br>traffic light recognition<br>autonomous driving<br>neural networks<br>real-time processing | Input: Multi-frame video data 多帧视频数据<br>Step1: Temporal data integration 时间数据集成<br>Step2: Neural network architecture design 神经网络架构设计<br>Step3: Real-time processing capabilities evaluation 实时处理能力评估<br>Output: Robust traffic light recognition results 稳健的交通灯识别结果 |
9.5 | [[9.5] 2503.23993 DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model](https://arxiv.org/abs/2503.23993) <br> [{'name': 'Ming Yuan, Sichao Wang, Chuang Zhang, Lei He, Qing Xu, Jianqiang Wang'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>autonomous driving<br>diffusion model<br>3D reconstruction | Input: Sparse depth maps and RGB images 稀疏深度图和 RGB 图像<br>Step1: Feature extraction 特征提取<br>Step2: Conditional diffusion process 条件扩散过程<br>Step3: Multi-step iterative refinement 多步迭代优化<br>Output: Dense depth map 生成密集深度图 |
9.5 | [[9.5] 2503.24210 DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting](https://arxiv.org/abs/2503.24210) <br> [{'name': 'Seungjun Lee, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>motion deblurring<br>event streams | Input: Blurry multi-view images and event streams 模糊的多视角图像和事件流<br>Step1: Optimize deblurring 3DGS 通过联合利用实际捕获的事件流和预训练的扩散模型约束去模糊3DGS<br>Step2: Introduce EDI constraints 引入事件双积分约束<br>Step3: Leverage diffusion prior 为了进一步改善细节，利用扩散先验<br>Output: Enhanced 3D representations 改进的3D表示 |
9.5 | [[9.5] 2503.24229 Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance Segmentation from 3D Synthetic Scenes](https://arxiv.org/abs/2503.24229) <br> [{'name': 'Daichi Otsuka, Shinichi Mae, Ryosuke Yamada, Hirokatsu Kataoka'}] | 3D Point Cloud Processing 点云处理 | v2<br>3D point cloud segmentation<br>synthetic data<br>generative models | Input: 3D point cloud data 3D点云数据<br>Step1: Data generation 数据生成<br>Step2: Model training 模型训练<br>Step3: Model evaluation 模型评估<br>Output: Improved instance segmentation results 改进的实例分割结果 |
9.5 | [[9.5] 2503.24374 ERUPT: Efficient Rendering with Unposed Patch Transformer](https://arxiv.org/abs/2503.24374) <br> [{'name': 'Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>novel view synthesis<br>scene reconstruction<br>unposed imagery<br>3D reconstruction<br>computer vision | Input: Small collections of RGB images 小规模RGB图像集<br>Step1: Patch-based querying of unposed imagery 基于补丁的无姿势图像查询<br>Step2: Latent camera pose learning 学习潜在相机姿态<br>Step3: Efficient model rendering and training 模型的高效渲染和训练<br>Output: High-quality rendered images 高质量渲染图像 |
9.5 | [[9.5] 2503.24382 Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views](https://arxiv.org/abs/2503.24382) <br> [{'name': 'Chong Bao, Xiyu Zhang, Zehao Yu, Jiale Shi, Guofeng Zhang, Songyou Peng, Zhaopeng Cui'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neural rendering<br>view synthesis<br>layered Gaussian | Input: Extremely sparse views (3-4) 极稀疏视图<br>Step1: Use dense stereo reconstruction to recover coarse geometry 使用稠密立体重建恢复粗糙几何<br>Step2: Apply layered Gaussian representation for scene modeling 应用分层高斯表示进行场景建模<br>Step3: Integrate reconstruction and generation iteratively 迭代整合重建与生成<br>Output: High-quality 3D reconstruction and unbounded view synthesis 输出: 高质量三维重建和无界视图合成 |
9.2 | [[9.2] 2503.23882 GLane3D : Detecting Lanes with Graph of 3D Keypoints](https://arxiv.org/abs/2503.23882) <br> [{'name': 'Halil \\.Ibrahim \\"Ozt\\"urk, Muhammet Esat Kalfao\\u{g}lu, Ozsel Kilinc'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D lane detection 3D车道检测<br>autonomous driving 自动驾驶 | Input: Multi-view images 多视角图像<br>Step1: Keypoint detection 关键点检测<br>Step2: Sequential connection prediction 顺序连接预测<br>Step3: Lane extraction 车道提取<br>Output: Complete 3D lanes 完整的三维车道 |
9.2 | [[9.2] 2503.24366 StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting](https://arxiv.org/abs/2503.24366) <br> [{'name': 'Shakiba Kheradmand, Delio Vicini, George Kopanas, Dmitry Lagun, Kwang Moo Yi, Mark Matthews, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian splatting<br>stochastic rasterization<br>neural rendering | Input: 3D Gaussian splatting 3D高斯点云<br>Step1: Implement stochastic rasterization 实现随机光栅化<br>Step2: Use Monte Carlo estimator 使用蒙特卡罗估计器<br>Step3: Render using OpenGL shaders 使用OpenGL着色器渲染<br>Output: Fast and high-quality rendering 快速高质量渲染 |
9.2 | [[9.2] 2503.24391 Easi3R: Estimating Disentangled Motion from DUSt3R Without Training](https://arxiv.org/abs/2503.24391) <br> [{'name': 'Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D reconstruction<br>dynamic segmentation<br>camera pose estimation | Input: Dynamic image collections 动态图像集<br>Step1: Attention adaptation during inference 推理期间注意力适应<br>Step2: Dynamic object segmentation 动态目标分割<br>Step3: Camera pose estimation 相机位姿估计<br>Output: 4D dense point map reconstruction 4D稠密点图重建 |
9.0 | [[9.0] 2503.23587 PhysPose: Refining 6D Object Poses with Physical Constraints](https://arxiv.org/abs/2503.23587) <br> [{'name': "Martin Malenick\\'y, Martin C\\'ifka, M\\'ed\\'eric Fourmy, Louis Montaut, Justin Carpentier, Josef Sivic, Vladimir Petrik"}] | Robotic Perception 机器人感知 | v2<br>6D object pose estimation<br>physical constraints<br>robotics<br>scene reconstruction<br>autonomous driving | Input: Images and geometric scene description (输入: 图像和几何场景描述)<br>Step 1: Estimate initial 6D object poses (步骤 1: 估计初始的 6D 物体姿态)<br>Step 2: Post-process to enforce physical consistency (步骤 2: 后处理以强制物理一致性)<br>Step 3: Evaluate and refine pose estimates (步骤 3: 评估和改进姿态估计)<br>Output: Accurate and physically plausible object poses (输出: 准确且物理上合理的物体姿态) |
9.0 | [[9.0] 2503.23963 A Benchmark for Vision-Centric HD Mapping by V2I Systems](https://arxiv.org/abs/2503.23963) <br> [{'name': 'Miao Fan, Shanshan Yu, Shengtong Xu, Kun Jiang, Haoyi Xiong, Xiangzeng Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Vehicle-to-Infrastructure (V2I)<br>HD mapping<br>autonomous driving<br>neural framework<br>vectorized maps | Input: Collaborative camera frames from vehicles and infrastructures 车辆与基础设施的协作摄像头帧<br>Step1: Data collection and annotation 数据收集与标注<br>Step2: Extract features from images 提取图像特征<br>Step3: Construct BEV representation 构建鸟瞰视图表示<br>Step4: Generate and update vectorized maps 生成并更新矢量化地图<br>Output: Vectorized high-definition maps 矢量化高清地图 |
8.5 | [[8.5] 2503.22963 SuperEIO: Self-Supervised Event Feature Learning for Event Inertial Odometry](https://arxiv.org/abs/2503.22963) <br> [{'name': 'Peiyu Chen, Fuling Lin, Weipeng Guan, Peng Lu'}] | Visual Odometry 视觉里程计 | v2<br>event camera<br>inertial odometry<br>3D reconstruction<br>sensor fusion | Input: Event streams from event cameras 事件相机的事件流<br>Step1: Event feature detection using CNN 使用CNN进行事件特征检测<br>Step2: Descriptor matching for loop closure using GNN 使用GNN进行环路闭合的描述符匹配<br>Step3: Optimize pipeline using TensorRT 优化使用TensorRT的管道<br>Output: Robust event-inertial odometry 可靠的事件惯性里程计 |
8.5 | [[8.5] 2503.22976 From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D](https://arxiv.org/abs/2503.22976) <br> [{'name': 'Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>3D reasoning<br>dataset creation | Input: 2D images and 3D ground-truth data 2D图像和三维真实数据<br>Step1: Data generation and annotation 数据生成与标注<br>Step2: Dataset creation for spatial tasks 数据集创建用于空间任务<br>Step3: Benchmark development for evaluation 基准开发用于评估<br>Output: Enhanced spatial reasoning capabilities 改进的空间推理能力 |
8.5 | [[8.5] 2503.23062 Shape and Texture Recognition in Large Vision-Language Models](https://arxiv.org/abs/2503.23062) <br> [{'name': 'Sagi Eppel, Mor Bismut, Alona Faktor'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>shape recognition<br>texture recognition<br>vision-language models<br>3D understanding | Input: Real-world images 真实世界图像<br>Step1: Dataset creation 数据集创建<br>Step2: Shape and texture recognition tests 形状和纹理识别测试<br>Step3: Evaluation of large vision-language models 大型视觉语言模型的评估<br>Output: Performance metrics on shape and texture recognition 形状和纹理识别的性能指标 |
8.5 | [[8.5] 2503.23105 Open-Vocabulary Semantic Segmentation with Uncertainty Alignment for Robotic Scene Understanding in Indoor Building Environments](https://arxiv.org/abs/2503.23105) <br> [{'name': 'Yifan Xu, Vineet Kamat, Carol Menassa'}] | Robotic Perception 机器人感知 | v2<br>semantic segmentation<br>autonomous assistive robots | Input: Built environment scenes 场景输入<br>Step1: Scene segmentation 场景分割<br>Step2: Semantic recognition 语义识别<br>Step3: Uncertainty alignment 不确定性对齐<br>Output: Adaptive navigation model 自适应导航模型 |
8.5 | [[8.5] 2503.23109 Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction](https://arxiv.org/abs/2503.23109) <br> [{'name': 'Xiaolu Liu, Ruizi Yang, Song Wang, Wentong Li, Junbo Chen, Jianke Zhu'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>HD map construction<br>autonomous vehicles | Input: HD maps 高精度地图<br>Step1: Uncertainty resampling 不确定性重采样<br>Step2: Structural feature extraction 结构特征提取<br>Step3: Map vectorization 地图矢量化<br>Output: Generalized HD maps 泛化的高精度地图 |
8.5 | [[8.5] 2503.23313 SpINR: Neural Volumetric Reconstruction for FMCW Radars](https://arxiv.org/abs/2503.23313) <br> [{'name': 'Harshvardhan Takawale, Nirupam Roy'}] | Volumetric Reconstruction 体积重建 | v2<br>volumetric reconstruction<br>FMCW radar<br>3D modeling | Input: FMCW radar data<br>Step1: Frequency-domain modeling<br>Step2: Implicit neural representation training<br>Step3: 3D volumetric geometry reconstruction<br>Output: High-resolution 3D models |
8.5 | [[8.5] 2503.23331 HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation](https://arxiv.org/abs/2503.23331) <br> [{'name': 'Hongwei Zheng, Han Li, Wenrui Dai, Ziyang Zheng, Chenglin Li, Junni Zou, Hongkai Xiong'}] | 3D Reconstruction 三维重建 | v2<br>3D human pose estimation<br>occlusion<br>hierarchical poses<br>sparse representation | Input: Sparse 2D poses from monocular images 单目图像中的稀疏2D姿势<br>Step1: Multi-scale skeleton tokenization 多尺度骨架标记<br>Step2: Hierarchical pose generation 分层姿势生成<br>Step3: 2D-to-3D lifting with generated poses 通过生成的姿势进行2D到3D的提升<br>Output: Enhanced 3D human poses 改进的3D人体姿势 |
8.5 | [[8.5] 2503.23365 OnSiteVRU: A High-Resolution Trajectory Dataset for High-Density Vulnerable Road Users](https://arxiv.org/abs/2503.23365) <br> [{'name': 'Zhangcun Yan, Jianqing Li, Peng Hang, Jian Sun'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人学 | v2<br>High-resolution trajectory data 高分辨率轨迹数据<br>Vulnerable Road Users 弱势交通参与者<br>Autonomous driving 自动驾驶 | Input: High-resolution trajectory data 高分辨率轨迹数据<br>Step1: Data collection 数据收集<br>Step2: Data integration 数据集成<br>Step3: Analysis of behavioral patterns 行为模式分析<br>Output: Comprehensive dataset for autonomous driving autonomous systems 输出: 自主驾驶系统的综合数据集 |
8.5 | [[8.5] 2503.23519 BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes](https://arxiv.org/abs/2503.23519) <br> [{'name': 'Haruya Ishikawa, Yoshimitsu Aoki'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>semi-supervised segmentation<br>boundary detection<br>autonomous driving | Input: Unlabeled images and labeled data; 输入: 无标签图像和有标签数据<br>Step1: Implement Boundary-Semantic Fusion to combine boundary cues with segmentation; 步骤1: 实施边界-语义融合，将边界线索与分割结合<br>Step2: Integrate Boundary Consistency Regularized Multi-Task Learning; 步骤2: 集成边界一致性正则化多任务学习<br>Step3: Evaluate model performance on various datasets; 步骤3: 在各种数据集上评估模型性能<br>Output: Enhanced segmentation masks with improved boundary delineation; 输出: 改进的分割掩码，具有更好的边界划分 |
8.5 | [[8.5] 2503.23577 Multiview Image-Based Localization](https://arxiv.org/abs/2503.23577) <br> [{'name': 'Cameron Fiore, Hongyi Fan, Benjamin Kimia'}] | 3D Localization 3D定位 | v2<br>3D localization<br>image retrieval<br>autonomous driving<br>multiview correspondences | Input: Query image and anchor images 查询图像和锚图像<br>Step1: Compute NetVLAD descriptors and SuperPoint features 计算NetVLAD描述符和SuperPoint特征<br>Step2: Retrieve top-K anchor images 根据特征描述符检索前K个锚图像<br>Step3: Estimate relative poses 估计相对位姿<br>Step4: Find optimal camera center and orientation 查找最佳相机中心和方向<br>Output: Accurate pose estimation 输出：准确的位姿估计 |
8.5 | [[8.5] 2503.23606 Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries](https://arxiv.org/abs/2503.23606) <br> [{'name': 'Wei Xu, Charles James Wagner, Junjie Luo, Qi Guo'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>depth from defocus<br>neural networks | Input: Pair of differently defocused images 处理：两个不同模糊的图像<br>Step1: Data representation through Blurry-Edges 数据表示，使用模糊边缘<br>Step2: Depth calculation using closed-form DfD relation 深度计算，使用封闭形式的Dfd关系<br>Output: Depth estimation along the boundaries 输出：沿边界的深度估计 |
8.5 | [[8.5] 2503.23647 Introducing the Short-Time Fourier Kolmogorov Arnold Network: A Dynamic Graph CNN Approach for Tree Species Classification in 3D Point Clouds](https://arxiv.org/abs/2503.23647) <br> [{'name': 'Said Ohamouddou, Mohamed Ohamouddou, Hanaa El Afia, Abdellatif El Afia, Rafik Lasri, Raddouane Chiheb'}] | 3D Point Cloud Processing 点云处理 | v2<br>3D point cloud<br>tree species classification<br>deep learning<br>STFT-KAN | Input: 3D point clouds 3D点云<br>Step1: Implementation of STFT-KAN STFT-KAN的实现<br>Step2: Model training and evaluation 模型训练与评估<br>Output: Tree species classification results 树种分类结果 |
8.5 | [[8.5] 2503.23702 3D Dental Model Segmentation with Geometrical Boundary Preserving](https://arxiv.org/abs/2503.23702) <br> [{'name': 'Shufan Xi, Zexian Liu, Junlin Chang, Hongyu Wu, Xiaogang Wang, Aimin Hao'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>segmentation<br>digital dentistry<br>computer vision | Input: 3D intraoral scan mesh 3D口腔扫描网格<br>Step1: Selective downsampling method 选择性下采样方法<br>Step2: Boundary feature extraction 边界特征提取<br>Step3: Model evaluation 模型评估<br>Output: Improved segmentation accuracy 改进的分割精度 |
8.5 | [[8.5] 2503.23980 SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency](https://arxiv.org/abs/2503.23980) <br> [{'name': 'Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR<br>Semantic segmentation<br>Zero-shot learning | Input: General LiDAR point clouds 一般LiDAR点云<br>Step1: Data transformation 数据转换<br>Step2: Zero-shot learning paradigm implementation 零样本学习范式实现<br>Step3: Pre-segmentation result generation 预分割结果生成<br>Output: Enhanced annotation efficiency 改进的注释效率 |
8.5 | [[8.5] 2503.24091 4D mmWave Radar in Adverse Environments for Autonomous Driving: A Survey](https://arxiv.org/abs/2503.24091) <br> [{'name': 'Xiangyuan Peng, Miao Tang, Huawei Sun, Lorenzo Servadei, Robert Wille'}] | Autonomous Driving 自动驾驶 | v2<br>4D mmWave radar<br>autonomous driving<br>adverse environments | Input: 4D mmWave radar data 4D毫米波雷达数据<br>Step1: Review of existing datasets 现有数据集的回顾<br>Step2: Analysis of methods and models 方法和模型的分析<br>Step3: Discussion on challenges and future directions 挑战与未来方向的讨论<br>Output: Comprehensive survey report 综合调查报告 |
8.5 | [[8.5] 2503.24129 It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data](https://arxiv.org/abs/2503.24129) <br> [{'name': 'Dominik Schnaus, Nikita Araslanov, Daniel Cremers'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language correspondence<br>unsupervised learning<br>quadratic assignment problem | Input: Vision and language embeddings 视觉和语言嵌入<br>Step1: Formulate unsupervised matching as a quadratic assignment problem 将无监督匹配形式化为二次分配问题<br>Step2: Develop a heuristic for matching 提出匹配的启发式方法<br>Step3: Evaluate on datasets 在数据集上评估<br>Output: Unsupervised classification results without annotations 无需注释的无监督分类结果 |
8.5 | [[8.5] 2503.24270 Visual Acoustic Fields](https://arxiv.org/abs/2503.24270) <br> [{'name': 'Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, Xueyan Zou, Paul Liang, Hanspeter Pfister, Xiaolong Wang'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>sound localization<br>sound generation | Input: Multiscale features from 3D Gaussian Splatting (3DGS)<br>Step1: Sound generation module utilizing a conditional diffusion model<br>Step2: Sound localization module for querying impact positions in 3D scene<br>Output: Generated sounds and localized impact sources in 3D space |
8.5 | [[8.5] 2503.24306 Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge](https://arxiv.org/abs/2503.24306) <br> [{'name': "Adam Schmidt, Mert Asim Karaoglu, Soham Sinha, Mingang Jang, Ho-Gun Ha, Kyungmin Jung, Kyeongmo Gu, Ihsan Ullah, Hyunki Lee, Jon\\'a\\v{s} \\v{S}er\\'ych, Michal Neoral, Ji\\v{r}\\'i Matas, Rulin Zhou, Wenlong He, An Wang, Hongliang Ren, Bruno Silva, Sandro Queir\\'os, Est\\^ev\\~ao Lima, Jo\\~ao L. Vila\\c{c}a, Shunsuke Kikuchi, Atsushi Kouno, Hiroki Matsuzaki, Tongtong Li, Yulu Chen, Ling Li, Xiang Ma, Xiaojian Li, Mona Sheikh Zeinoddin, Xu Wang, Zafer Tandogdu, Greg Shaw, Evangelos Mazomenos, Danail Stoyanov, Yuxin Chen, Zijian Wu, Alexander Ladikos, Simon DiMaio, Septimiu E. Salcudean, Omid Mohareri"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving<br>surgery<br>algorithms | Input: Infrared video sequences 红外视频序列<br>Step1: Data quantification 数据量化<br>Step2: Algorithm submission 提交算法<br>Step3: Performance evaluation 性能评估<br>Output: Algorithm performance metrics 算法性能指标 |
8.5 | [[8.5] 2503.24381 UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving](https://arxiv.org/abs/2503.24381) <br> [{'name': 'Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, Jiachen Li'}] | Autonomous Driving 自动驾驶 | v2<br>Occupancy forecasting 占用预测<br>Autonomous driving 自动驾驶<br>3D Occupancy labels 三维占用标签 | Input: Camera images 摄像头图像<br>Step1: Data integration 数据集成<br>Step2: Occupancy forecasting 模型预测<br>Step3: Evaluation of performance 性能评估<br>Output: Occupancy predictions 预测的占用情况 |
8.0 | [[8.0] 2503.22932 Bi-Level Multi-View fuzzy Clustering with Exponential Distance](https://arxiv.org/abs/2503.22932) <br> [{'name': 'Kristina P. Sinaga'}] | Multi-view Stereo 多视角立体 | v2<br>multi-view clustering<br>fuzzy c-means<br>exponential distance | Input: Multi-view data 多视角数据<br>Step1: Extend fuzzy c-means clustering 扩展模糊c均值聚类<br>Step2: Incorporate heat-kernel coefficients 引入热核系数<br>Step3: Develop bi-level clustering algorithm 开发双层聚类算法<br>Output: Enhanced clustering results 改进的聚类结果 |
7.5 | [[7.5] 2503.23131 RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning](https://arxiv.org/abs/2503.23131) <br> [{'name': 'Alexander Vogel, Omar Moured, Yufan Chen, Jiaming Zhang, Rainer Stiefelhagen'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>Vision-Language Models<br>chart understanding<br>visual grounding | Input: Chart images 图表图像<br>Step1: Data collection 数据收集<br>Step2: Instruction tuning 指令调优<br>Step3: Visual grounding implementation 可视化基础实现<br>Output: RefChartQA dataset and model outputs RefChartQA数据集及模型输出 |
7.5 | [[7.5] 2503.23452 VideoGen-Eval: Agent-based System for Video Generation Evaluation](https://arxiv.org/abs/2503.23452) <br> [{'name': 'Yuhang Yang, Ke Fan, Shangkun Sun, Hongxiang Li, Ailing Zeng, FeiLin Han, Wei Zhai, Wei Liu, Yang Cao, Zheng-Jun Zha'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>evaluation system | Input: Video generation prompts 视频生成提示<br>Step1: Content structuring 内容结构<br>Step2: Content judgment 内容评估<br>Step3: Dynamic evaluation tools 部件动态评估工具<br>Output: Evaluation results 评估结果 |
7.5 | [[7.5] 2503.23573 DASH: Detection and Assessment of Systematic Hallucinations of VLMs](https://arxiv.org/abs/2503.23573) <br> [{'name': 'Maximilian Augustin, Yannic Neuhaus, Matthias Hein'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>VLMs<br>object hallucination<br>evaluation | Input: Real-world images 真实世界的图像<br>Step1: Data retrieval 数据检索<br>Step2: Systematic hallucination detection 系统性幻觉检测<br>Output: Clusters of hallucinated images 幻觉图像的聚类 |
6.5 | [[6.5] 2503.23508 Re-Aligning Language to Visual Objects with an Agentic Workflow](https://arxiv.org/abs/2503.23508) <br> [{'name': 'Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu, Rui Zhao, Qibin Hou, Ming-Ming Cheng, Yibing Song'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>language-object alignment<br>vision-language models | Input: Detected objects and raw language expressions 检测到的对象和原始语言表达<br>Step1: Reasoning state and planning 状态推理与规划<br>Step2: Adaptive prompt adjustment 自适应提示调整<br>Step3: Feedback analysis from LLM LLM反馈分析<br>Output: Re-aligned language expressions 重新对齐的语言表达 |


## Arxiv 2025-04-01

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.22986 FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2503.22986) <br> [{'name': 'Yunsong Wang, Tianxin Huang, Hanlin Chen, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>indoor scenes | Input: Multi-view images 多视角图像<br>Step1: Low-cost Cross-View Aggregation framework 低成本交叉视图聚合框架<br>Step2: Pixel-wise triplet fusion method 像素级三元组融合方法<br>Step3: Weighted floater removal strategy 加权浮子去除策略<br>Step4: Depth-regularized per-scene fine-tuning 深度规则化每场景微调<br>Output: Enhanced 3D Gaussian primitives 改进的三维高斯原语 |
9.5 | [[9.5] 2503.23044 CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction](https://arxiv.org/abs/2503.23044) <br> [{'name': 'Yuanyuan Gao, Hao Li, Jiaqi Chen, Zhengyu Zou, Zhihang Zhong, Dingwen Zhang, Xiao Sun, Junwei Han'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>large-scale scene reconstruction 大规模场景重建<br>3D Gaussian Splatting 3D高斯点<br>multi-GPU rendering 多GPU渲染 | Input: Multi-view images 多视角图像<br>Step1: Dynamic voxel allocation 动态体素分配<br>Step2: Batch rendering techniques 批量渲染技术<br>Step3: Parallel training and rendering 并行训练与渲染<br>Output: High-fidelity 3D models 高保真3D模型 |
9.5 | [[9.5] 2503.23162 NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations](https://arxiv.org/abs/2503.23162) <br> [{'name': 'Zhenyu Tang, Chaoran Feng, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liu, Xiaoxiao Long, Wenping Wang, Li Yuan'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Gaussian Splatting 3D高斯点云<br>Neural Rendering 神经渲染<br>3D Reconstruction 三维重建 | Input: Original 3D Gaussian Splatting 3DGS 原始3D高斯点云<br>Step1: Importance calculation 重要性计算<br>Step2: Gaussian clustering 高斯聚类<br>Step3: Tiny MLP fitting 小型多层感知机拟合<br>Output: Compact 3D representation 紧凑的3D表示 |
9.5 | [[9.5] 2503.23282 AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos](https://arxiv.org/abs/2503.23282) <br> [{'name': 'Felix Wimbauer, Weirong Chen, Dominik Muhle, Christian Rupprecht, Daniel Cremers'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>camera poses<br>intrinsics<br>3D reconstruction<br>dynamic scenes | Input: Dynamic video sequences 动态视频序列<br>Step1: Predict camera poses and intrinsics 预测相机姿态和内参<br>Step2: Apply uncertainty-based loss formulation 应用基于不确定性的损失函数<br>Step3: Perform trajectory refinement 进行轨迹优化<br>Output: High-quality 4D pointclouds 高质量4D点云 |
9.5 | [[9.5] 2503.23297 ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning](https://arxiv.org/abs/2503.23297) <br> [{'name': 'Zhenyang Liu, Yikai Wang, Sixiao Zheng, Tongying Pan, Longfei Liang, Yanwei Fu, Xiangyang Xue'}] | 3D Visual Grounding 3D视觉定位 | v2<br>3D grounding<br>language models<br>Gaussian features | Input: Implicit language descriptions 语言描述<br>Step1: Use 3D Gaussian feature fields 使用3D高斯特征场<br>Step2: Adaptive grouping based on object scale 根据物体尺度进行自适应分组<br>Step3: Localize occluded objects 进行遮挡物体定位<br>Output: Enhanced 3D grounding 改进的三维定位 |
9.5 | [[9.5] 2503.23463 OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model](https://arxiv.org/abs/2503.23463) <br> [{'name': 'Xingcheng Zhou, Xuyuan Han, Feng Yang, Yunpu Ma, Alois C. Knoll'}] | Autonomous Driving 自动驾驶 | v2<br>Vision-Language Models 视觉语言模型<br>Autonomous Driving 自动驾驶 | Input: Multimodal inputs including 3D environmental perception 3D环境感知, ego vehicle states 自我车辆状态, and driver commands 驾驶员命令<br>Step1: Hierarchical vision-language alignment process 层次化视觉语言对齐过程<br>Step2: Model generates driving trajectories 生成驾驶轨迹<br>Step3: Evaluate agent-env-ego interactions 评估主体-环境-自我交互<br>Output: Reliable driving actions 可靠的驾驶动作 |
9.5 | [[9.5] 2503.23502 Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model](https://arxiv.org/abs/2503.23502) <br> [{'name': 'Jannik Endres, Oliver Hahn, Charles Corbi\\`ere, Simone Schaub-Meyer, Stefan Roth, Alexandre Alahi'}] | Stereo Matching 立体匹配 | v2<br>omnidirectional stereo matching<br>depth estimation<br>mobile robotics<br>3D reconstruction | Input: Equirectangular images captured with two vertically stacked omnidirectional cameras 通过两个垂直堆叠的全向相机采集的等距图像<br>Step1: Integrate depth foundation model into stereo matching architecture 将深度基础模型集成到立体匹配架构中<br>Step2: Two-stage training: Adapt stereo matching head and fine-tune foundation model 两阶段训练：调整立体匹配头和微调基础模型<br>Step3: Evaluate performance on real-world datasets 在真实数据集上评估性能<br>Output: Enhanced disparity estimation and 3D depth maps 改进的视差估计和三维深度图 |
9.5 | [[9.5] 2503.23664 LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds](https://arxiv.org/abs/2503.23664) <br> [{'name': 'Masahiko Tsuji, Hitoshi Niigaki, Ryuichi Tanida'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>LiDAR | Input: 2D keypoints from reference images 参考图像中的2D关键点<br>Step1: Generate 3D reference map using 3D reconstruction 使用3D重建生成3D参考地图<br>Step2: Assign 3D LiDAR point clouds to keypoints 将3D LiDAR点云分配给关键点<br>Step3: Improve pose estimation accuracy 提高姿态估计准确性<br>Output: Dense and accurate 3D reference maps 密集且准确的3D参考地图 |
9.5 | [[9.5] 2503.23670 Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation](https://arxiv.org/abs/2503.23670) <br> [{'name': 'Takeshi Noda, Chao Chen, Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu, Zhizhong Han'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Surface Reconstruction 表面重建<br>Signed Distance Functions 有符号距离函数<br>Sparse Point Clouds 稀疏点云 | Input: Sparse point clouds 稀疏点云<br>Step1: Learn dynamic deformation network 学习动态变形网络<br>Step2: Bijective surface parameterization (BSP) learning 学习双射表面参数化<br>Step3: Grid deformation optimization (GDO) 应用网格变形优化<br>Output: Continuous signed distance functions (SDF) 生成连续的有符号距离函数 |
9.5 | [[9.5] 2503.23684 Detail-aware multi-view stereo network for depth estimation](https://arxiv.org/abs/2503.23684) <br> [{'name': 'Haitao Tian, Junyang Li, Chenxing Wang, Helong Jiang'}] | Multi-view Stereo 多视角立体 | v2<br>depth estimation<br>multi-view stereo<br>3D reconstruction<br>image synthesis | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Geometric depth embedding 算法开发<br>Step3: Adaptive depth interval adjustment 自适应深度间隔调整<br>Output: Accurate depth maps 精确的深度图 |
9.5 | [[9.5] 2503.23747 Consistency-aware Self-Training for Iterative-based Stereo Matching](https://arxiv.org/abs/2503.23747) <br> [{'name': 'Jingyi Zhou, Peng Ye, Haoyu Zhang, Jiakang Yuan, Rao Qiang, Liu YangChenXu, Wu Cailin, Feng Xu, Tao Chen'}] | Stereo Matching 立体匹配 | v2<br>stereo matching<br>self-training<br>depth estimation<br>computer vision<br>autonomous driving | Input: Stereo image pairs 立体图像对<br>Step1: Reliability evaluation 可靠性评估<br>Step2: Soft filtering of pseudo-labels 伪标签软过滤<br>Step3: Model training with weighted loss 使用加权损失进行模型训练<br>Output: Enhanced stereo matching results 改进的立体匹配结果 |
9.5 | [[9.5] 2503.23881 ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image](https://arxiv.org/abs/2503.23881) <br> [{'name': 'Tianyi Gong, Boyan Li, Yifei Zhong, Fangxin Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>single-view image reconstruction<br>Gaussian Splatting<br>panoramic image generation | Input: Single-view image 单视图图像<br>Step1: Generate panoramic image 生成全景图像<br>Step2: Depth estimation 深度估计<br>Step3: Train initial 3D Gaussian Splatting model 训练初始3D高斯点云模型<br>Step4: GS refinement with video diffusion priors 使用视频扩散先验进行GS优化<br>Output: Enhanced immersive 3D scene 改进的沉浸式3D场景 |
9.5 | [[9.5] 2503.23882 GLane3D : Detecting Lanes with Graph of 3D Keypoints](https://arxiv.org/abs/2503.23882) <br> [{'name': 'Halil \\.Ibrahim \\"Ozt\\"urk, Muhammet Esat Kalfao\\u{g}lu, Ozsel Kilinc'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D lane detection<br>autonomous driving | Input: 3D lane data 3D车道数据<br>Step1: Keypoint detection 关键点检测<br>Step2: Connection prediction 连接预测<br>Step3: Lane construction 道路构建<br>Output: 3D lanes 3D车道 |
9.5 | [[9.5] 2503.23963 A Benchmark for Vision-Centric HD Mapping by V2I Systems](https://arxiv.org/abs/2503.23963) <br> [{'name': 'Miao Fan, Shanshan Yu, Shengtong Xu, Kun Jiang, Haoyi Xiong, Xiangzeng Liu'}] | Autonomous Driving 自动驾驶 | v2<br>HD mapping<br>vehicle-to-infrastructure<br>autonomous driving | Input: Collaborative camera frames from vehicles and infrastructure 车辆和基础设施的协作摄像头帧<br>Step1: Data collection and annotation 数据收集和标注<br>Step2: Feature extraction 特征提取<br>Step3: Map encoding and decoding 地图编码和解码<br>Output: Vectorized high-definition maps 向量化高精度地图 |
9.5 | [[9.5] 2503.23965 Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving](https://arxiv.org/abs/2503.23965) <br> [{'name': 'Miao Fan, Xuxu Kong, Shengtong Xu, Haoyi Xiong, Xiangzeng Liu'}] | Autonomous Driving 自动驾驶 | v2<br>traffic light recognition<br>autonomous driving<br>real-time processing<br>end-to-end neural network | Input: Video frames from ego cameras  来自自我摄像机的视频帧<br>Step1: Multi-frame processing  多帧处理<br>Step2: Traffic light detection and classification  交通信号灯检测和分类<br>Step3: Integration with HD maps  与高清地图集成<br>Output: Real-time traffic light recognition  实时交通信号灯识别 |
9.5 | [[9.5] 2503.23993 DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model](https://arxiv.org/abs/2503.23993) <br> [{'name': 'Ming Yuan, Sichao Wang, Chuang Zhang, Lei He, Qing Xu, Jianqiang Wang'}] | Depth Estimation 深度估计 | v2<br>depth completion 深度补全<br>autonomous driving 自动驾驶<br>conditional diffusion model 条件扩散模型 | Input: Sparse depth maps and RGB images 稀疏深度图和RGB图像<br>Step1: Data integration 数据集成<br>Step2: Conditional depth denoising using diffusion model 条件深度去噪<br>Step3: Multi-step iterative refinement 多步迭代优化<br>Output: Enhanced dense depth maps 改进的稠密深度图 |
9.5 | [[9.5] 2503.24210 DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting](https://arxiv.org/abs/2503.24210) <br> [{'name': 'Seungjun Lee, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Gaussian Splatting<br>motion deblurring<br>event streams<br>novel view synthesis | Input: Blurry multi-view images and event streams 模糊多视角图像及事件流<br>Step1: Optimize deblurring 3DGS using event streams and diffusion prior 优化去模糊3DGS，利用事件流和扩散先验<br>Step2: Enhance edge details and color accuracy 强化边缘细节和颜色准确性<br>Output: Improved sharp 3D representations 改进的锐利3D表示 |
9.5 | [[9.5] 2503.24366 StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting](https://arxiv.org/abs/2503.24366) <br> [{'name': 'Shakiba Kheradmand, Delio Vicini, George Kopanas, Dmitry Lagun, Kwang Moo Yi, Mark Matthews, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian splatting<br>stochastic rasterization<br>neural rendering<br>volume rendering | Input: 3D Gaussian splatting data 3D高斯点云数据<br>Step1: Integrate stochastic rasterization techniques 整合随机光栅化技术<br>Step2: Implement unbiased Monte Carlo estimator 实现无偏蒙特卡洛估计器<br>Step3: Optimize rendering performance 优化渲染性能<br>Output: Efficient 3D rendering output 高效的三维渲染输出 |
9.5 | [[9.5] 2503.24374 ERUPT: Efficient Rendering with Unposed Patch Transformer](https://arxiv.org/abs/2503.24374) <br> [{'name': 'Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>3D reconstruction 3D重建<br>novel view synthesis 新视图合成<br>efficient rendering 高效渲染 | Input: Collections of RGB images RGB图像集<br>Step1: Patch-based querying using unposed imagery 基于补丁的查询，使用未定位的图像<br>Step2: Model training with learned latent camera pose 模型训练，使用学习到的潜在相机姿态<br>Step3: Efficient rendering at high frame rates 实现高帧率的高效渲染<br>Output: Novel view synthesis of 3D scenes 3D场景的新视图合成 |
9.5 | [[9.5] 2503.24382 Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views](https://arxiv.org/abs/2503.24382) <br> [{'name': 'Chong Bao, Xiyu Zhang, Zehao Yu, Jiale Shi, Guofeng Zhang, Songyou Peng, Zhaopeng Cui'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>unbounded view synthesis<br>neural rendering | Input: Extremely sparse views (3-4) 极少视角输入（3-4）<br>Step1: Employ dense stereo reconstruction model to recover coarse geometry 使用密集立体重建模型恢复粗略几何<br>Step2: Introduce layered Gaussian-based representation to model scenes 引入分层高斯表示来建模场景<br>Step3: Perform bootstrap optimization for noise refinement and occlusion filling 进行引导优化以消除噪声和填补遮挡区域<br>Step4: Implement iterative fusion of reconstruction and generation 进行重建与生成的迭代融合<br>Output: High-quality 3D reconstruction and novel view synthesis 输出：高质量的三维重建和新视图合成 |
9.5 | [[9.5] 2503.24391 Easi3R: Estimating Disentangled Motion from DUSt3R Without Training](https://arxiv.org/abs/2503.24391) <br> [{'name': 'Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D reconstruction<br>camera pose estimation<br>dynamic scenes | Input: Dynamic video footage 动态视频<br>Step1: Attention map analysis 注意力图分析<br>Step2: Motion disentanglement 运动解耦<br>Step3: Point cloud reconstruction 点云重建<br>Output: Segmented dynamic regions and camera parameters 分割的动态区域和相机参数 |
9.2 | [[9.2] 2503.23024 Empowering Large Language Models with 3D Situation Awareness](https://arxiv.org/abs/2503.23024) <br> [{'name': 'Zhihao Yuan, Yibo Peng, Jinke Ren, Yinghong Liao, Yatong Han, Chun-Mei Feng, Hengshuang Zhao, Guanbin Li, Shuguang Cui, Zhen Li'}] | 3D Scene Understanding 3D场景理解 | v2<br>3D situation awareness 3D情境意识<br>Vision-Language Models 视觉语言模型<br>Large Language Models 大型语言模型 | Input: RGB-D videos RGB-D 视频<br>Step1: Data collection 数据收集<br>Step2: Dataset generation 数据集生成<br>Step3: Situation grounding module integration 情境基础模块集成<br>Output: Enhanced 3D situational awareness 改进的三维情境感知 |
9.2 | [[9.2] 2503.23109 Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction](https://arxiv.org/abs/2503.23109) <br> [{'name': 'Xiaolu Liu, Ruizi Yang, Song Wang, Wentong Li, Junbo Chen, Jianke Zhu'}] | Autonomous Driving 自动驾驶 | v2<br>HD map construction<br>autonomous driving | Input: Images from onboard cameras 车载摄像头图像<br>Step 1: Feature extraction 特征提取<br>Step 2: Uncertainty-aware detection 不确定性感知检测<br>Step 3: Map vectorization 地图向量化<br>Output: Generalized HD maps 泛化的高清地图 |
9.2 | [[9.2] 2503.23313 SpINR: Neural Volumetric Reconstruction for FMCW Radars](https://arxiv.org/abs/2503.23313) <br> [{'name': 'Harshvardhan Takawale, Nirupam Roy'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>volumetric reconstruction<br>neural representation<br>radar imaging | Input: FMCW radar data 频率调制连续波雷达数据<br>Step1: Construct frequency-domain model 构建频率域模型<br>Step2: Integrate neural representations 集成神经表示<br>Step3: Perform volumetric reconstruction 进行体积重建<br>Output: High-resolution 3D scenes 高分辨率3D场景 |
9.2 | [[9.2] 2503.24229 Pre-training with 3D Synthetic Data: Learning 3D Point Cloud Instance Segmentation from 3D Synthetic Scenes](https://arxiv.org/abs/2503.24229) <br> [{'name': 'Daichi Otsuka, Shinichi Mae, Ryosuke Yamada, Hirokatsu Kataoka'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D point cloud<br>instance segmentation<br>synthetic data<br>autonomous driving | Input: 3D point cloud data 3D点云数据<br>Step1: Pre-training with synthetic data 使用合成数据进行预训练<br>Step2: Instance segmentation model training 实例分割模型训练<br>Step3: Evaluation of segmentation performance 分割性能评估<br>Output: Enhanced 3D instance segmentation model 改进的三维实例分割模型 |
9.0 | [[9.0] 2503.23337 Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction](https://arxiv.org/abs/2503.23337) <br> [{'name': 'Jingui Ma, Yang Hu, Luyang Tang, Jiayu Yang, Yongqi Zhai, Ronggang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>compression<br>prediction technique | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Introduce prediction technique 引入预测技术<br>Step2: Compress using spatial condition 基于空间条件进行压缩<br>Step3: Model evaluation using residuals 采用残差进行模型评估<br>Output: Compressed Gaussian representation 压缩的高斯表示 |
8.5 | [[8.5] 2503.22976 From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D](https://arxiv.org/abs/2503.22976) <br> [{'name': 'Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>3D reasoning<br>spatial perception | Input: 2D spatial data 2D空间数据<br>Step1: Data generation 数据生成<br>Step2: Annotation pipeline 注释管道<br>Step3: Model training 模型训练<br>Output: Spatial tasks 数据集成和空间任务 |
8.5 | [[8.5] 2503.23022 MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs](https://arxiv.org/abs/2503.23022) <br> [{'name': 'Xianglong He, Junyi Chen, Di Huang, Zexiang Liu, Xiaoshui Huang, Wanli Ouyang, Chun Yuan, Yangguang Li'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>mesh generation<br>efficient algorithms | Input: Raw meshes 原始网格<br>Step1: Encoding raw meshes into continuous tokens 将原始网格编码为连续的标记<br>Step2: Decoding tokens back into mesh structure 将标记解码为网格结构<br>Step3: Generating mesh topology using diffusion model 使用扩散模型生成网格拓扑<br>Output: High-quality, controllable mesh generation 高质量、可控的网格生成 |
8.5 | [[8.5] 2503.23365 OnSiteVRU: A High-Resolution Trajectory Dataset for High-Density Vulnerable Road Users](https://arxiv.org/abs/2503.23365) <br> [{'name': 'Zhangcun Yan, Jianqing Li, Peng Hang, Jian Sun'}] | Autonomous Driving 自动驾驶 | v2<br>Vulnerable Road Users<br>autonomous driving<br>trajectory dataset | Input: High-resolution trajectory data 高分辨率轨迹数据<br>Step1: Dataset development 数据集开发<br>Step2: Data integration 数据集成<br>Step3: Evaluation of trajectory coverage 轨迹覆盖评估<br>Output: Comprehensive VRU behavior representation 全面的VRU行为表现 |
8.5 | [[8.5] 2503.23368 Towards Physically Plausible Video Generation via VLM Planning](https://arxiv.org/abs/2503.23368) <br> [{'name': 'Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>vision-language models<br>physics-aware motion planning | Input: Image and text prompt 输入：图像和文本提示<br>Step1: VLM planning for motion trajectories VLM规划运动轨迹<br>Step2: Video generation with noise injection 噪声注入的视频生成<br>Output: Physically plausible videos 输出：物理上合理的视频 |
8.5 | [[8.5] 2503.23508 Re-Aligning Language to Visual Objects with an Agentic Workflow](https://arxiv.org/abs/2503.23508) <br> [{'name': 'Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu, Rui Zhao, Qibin Hou, Ming-Ming Cheng, Yibing Song'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>language-based object detection<br>vision-language models<br>data alignment | Input: Image with detected objects 带有检测对象的图像<br>Step1: Generate raw language expressions 生成原始语言表达<br>Step2: Plan actions based on agent's reasoning 根据代理的推理计划行动<br>Step3: Adjust image and text prompts 调整图像和文本提示<br>Output: Re-aligned expressions with improved accuracy 输出: 精确度提高的重新对齐表达 |
8.5 | [[8.5] 2503.23573 DASH: Detection and Assessment of Systematic Hallucinations of VLMs](https://arxiv.org/abs/2503.23573) <br> [{'name': 'Maximilian Augustin, Yannic Neuhaus, Matthias Hein'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucinations<br>image retrieval | Input: Real-world images 真实世界图像<br>Step1: Image-based retrieval imagery-based 检索图像<br>Step2: Clustering similar images 聚类相似图像<br>Step3: Evaluation of hallucinations 评估幻觉<br>Output: Identified clusters of hallucinatory images 识别的幻觉图像簇 |
8.5 | [[8.5] 2503.23577 Multiview Image-Based Localization](https://arxiv.org/abs/2503.23577) <br> [{'name': 'Cameron Fiore, Hongyi Fan, Benjamin Kimia'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>visual localization<br>multiview correspondence<br>autonomous systems | Input: Multiview images 多视角图像<br>Step1: Image feature extraction 图像特征提取<br>Step2: Relative translation estimate 计算相对平移<br>Step3: Optimal pose computation 从多视角相应中计算最优姿态<br>Output: Accurate localization results 准确定位结果 |
8.5 | [[8.5] 2503.23587 PhysPose: Refining 6D Object Poses with Physical Constraints](https://arxiv.org/abs/2503.23587) <br> [{'name': "Martin Malenick\\'y, Martin C\\'ifka, M\\'ed\\'eric Fourmy, Louis Montaut, Justin Carpentier, Josef Sivic, Vladimir Petrik"}] | 6D Pose Estimation 6D对象姿态估计 | v2<br>6D pose estimation<br>physical consistency<br>object-centric scene understanding<br>robotics | Input: Image and geometric description of the scene 图像和场景几何描述<br>Step1: Estimate initial poses for objects 估计对象的初始姿态<br>Step2: Post-processing optimization with physical constraints 引入物理约束的后处理优化<br>Output: Refined 6D object poses 改进的6D对象姿态 |
8.5 | [[8.5] 2503.23606 Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries](https://arxiv.org/abs/2503.23606) <br> [{'name': 'Wei Xu, Charles James Wagner, Junjie Luo, Qi Guo'}] | Depth Estimation 深度估计 | v2<br>depth estimation 深度估计<br>depth from defocus 失焦深度<br>photon-limited images 光子限制图像 | Input: Defocused images 失焦图像<br>Step1: Image patch representation 图像块表示<br>Step2: Neural network prediction 神经网络预测<br>Step3: Depth calculation using DfD equation 使用DfD方程计算深度<br>Output: Depth maps 深度图 |
8.5 | [[8.5] 2503.23647 Introducing the Short-Time Fourier Kolmogorov Arnold Network: A Dynamic Graph CNN Approach for Tree Species Classification in 3D Point Clouds](https://arxiv.org/abs/2503.23647) <br> [{'name': 'Said Ohamouddoua, Mohamed Ohamouddoub, Rafik Lasrib, Hanaa El Afiaa, Raddouane Chiheba, Abdellatif El Afiaa'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Point Cloud<br>Tree Species Classification<br>Kolmogorov-Arnold Network | Input: 3D point clouds from TLS and ALS  三维激光扫描点云<br>Step1: Implement Short-Time Fourier Transform (STFT) 使用短时傅里叶变换<br>Step2: Develop lightweight Dynamic Graph CNN (liteDGCNN) 开发轻量级动态图卷积神经网络<br>Step3: Evaluate performance and parameter reduction 评估性能和参数减少<br>Output: Classified tree species with reduced model complexity 输出：分类树种并降低模型复杂度 |
8.5 | [[8.5] 2503.23702 3D Dental Model Segmentation with Geometrical Boundary Preserving](https://arxiv.org/abs/2503.23702) <br> [{'name': 'Shufan Xi, Zexian Liu, Junlin Chang, Hongyu Wu, Xiaogang Wang, Aimin Hao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>tooth segmentation<br>digital dentistry | Input: 3D intraoral scan mesh 3D口腔扫描网格<br>Step1: Selective downsampling 方法：选择性下采样<br>Step2: Boundary feature extraction 边界特征提取<br>Step3: Model evaluation 模型评估<br>Output: Improved tooth segmentation 提高的牙齿分割效果 |
8.5 | [[8.5] 2503.23980 SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency](https://arxiv.org/abs/2503.23980) <br> [{'name': 'Yanbo Wang, Yongtao Chen, Chuan Cao, Tianchen Deng, Wentao Zhao, Jingchuan Wang, Weidong Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR<br>zero-shot learning<br>semi-automatic labeling | Input: Raw LiDAR data 原始激光雷达数据<br>Step1: Data transformation 数据转换<br>Step2: Zero-shot learning application 零样本学习应用<br>Step3: Pre-segmentation generation 预分割生成<br>Output: High-quality pre-segmented LiDAR data 高质量预分割的激光雷达数据 |
8.5 | [[8.5] 2503.24091 4D mmWave Radar in Adverse Environments for Autonomous Driving: A Survey](https://arxiv.org/abs/2503.24091) <br> [{'name': 'Xiangyuan Peng, Miao Tang, Huawei Sun, Lorenzo Servadei, Robert Wille'}] | Autonomous Driving 自动驾驶 | v2<br>4D mmWave radar<br>autonomous driving<br>perception | Input: 4D mmWave radar data 4D毫米波雷达数据<br>Step1: Review existing datasets 现有数据集综述<br>Step2: Analyze methods for perception and SLAM 感知与SLAM方法分析<br>Step3: Discuss challenges and future directions 挑战与未来方向讨论<br>Output: Comprehensive survey report 综合调查报告 |
8.5 | [[8.5] 2503.24270 Visual Acoustic Fields](https://arxiv.org/abs/2503.24270) <br> [{'name': 'Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, Xueyan Zou, Paul Liang, Hanspeter Pfister, Xiaolong Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Visual Acoustic Fields<br>cross-modal learning | Input: Visual and acoustic data 视觉和声学数据<br>Step1: Collect multiview images 收集多视角图像<br>Step2: Record impact sounds 记录撞击声音<br>Step3: Use structure-from-motion for camera pose estimation 使用运动结构估计摄像机姿态<br>Step4: Implement 3D Gaussian Splatting for scene reconstruction 使用3D高斯点云重建场景<br>Step5: Generate sound based on visual cues 使用视觉线索生成声音<br>Step6: Localize sound sources within the scene 确定场景内声音源的位置<br>Output: Aligned visual-sound pairs 输出对齐的视觉-声音对 |
8.5 | [[8.5] 2503.24381 UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving](https://arxiv.org/abs/2503.24381) <br> [{'name': 'Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, Jiachen Li'}] | Occupancy Forecasting and Prediction in Autonomous Driving 自动驾驶中的占用预测与预测 | v2<br>Occupancy Forecasting  占用预测<br>Autonomous Driving 自动驾驶<br>3D Prediction  三维预测 | Input: Multi-view images 多视角图像<br>Step1: Data unification 数据统一<br>Step2: Novel metric development 新指标开发<br>Step3: Algorithm validation 算法验证<br>Output: Unified occupancy predictions 统一的占用预测 |
8.0 | [[8.0] 2503.24129 It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data](https://arxiv.org/abs/2503.24129) <br> [{'name': 'Dominik Schnaus, Nikita Araslanov, Daniel Cremers'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Correspondence 视觉-语言对应<br>Unsupervised Learning 无监督学习 | Input: Vision and language embeddings 视觉和语言嵌入<br>Step1: Formulate matching as a quadratic assignment problem 将匹配公式化为二次分配问题<br>Step2: Develop a heuristic solver 发展启发式解法<br>Step3: Conduct extensive empirical study 开展广泛的实证研究<br>Output: Unsupervised classification outcomes 无监督分类结果 |
7.5 | [[7.5] 2503.23105 Open-Vocabulary Semantic Segmentation with Uncertainty Alignment for Robotic Scene Understanding in Indoor Building Environments](https://arxiv.org/abs/2503.23105) <br> [{'name': 'Yifan Xu, Vineet Kamat, Carol Menassa'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>semantic segmentation<br>robotic navigation | Input: Scene images 场景图像<br>Step1: Segment different rooms/regions of the scene 划分场景中的不同房间/区域<br>Step2: Leverage VLM to get similarity scores between descriptions and rooms 利用视觉语言模型获得描述与房间之间的相似度分数<br>Step3: Use adaptive conformal prediction (ACP) to select rooms according to similarity scores 使用自适应的保形预测根据相似度分数选择房间<br>Output: Enhanced robot navigation capabilities 提升机器人导航能力 |
7.5 | [[7.5] 2503.23200 A GAN-Enhanced Deep Learning Framework for Rooftop Detection from Historical Aerial Imagery](https://arxiv.org/abs/2503.23200) <br> [{'name': 'Pengyu Chen, Sicheng Wang, Cuizhen Wang, Senrong Wang, Beiao Huang, Lu Huang, Zhe Zang'}] | Image Generation 图像生成 | v2<br>rooftop detection<br>historical imagery<br>Generative Adversarial Networks<br>image enhancement | Input: Historical aerial imagery 历史航空图像<br>Step1: Image colorization using DeOldify 图像上色采用DeOldify<br>Step2: Super-resolution enhancement using Real-ESRGAN 超分辨率增强采用Real-ESRGAN<br>Step3: Train rooftop detection models 训练屋顶检测模型<br>Output: Improved rooftop detection performance 改进的屋顶检测性能 |
7.5 | [[7.5] 2503.23388 COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation](https://arxiv.org/abs/2503.23388) <br> [{'name': 'Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, Zhi Wang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Test-Time Adaptation | Input: Test samples 测试样本<br>Step1: Cache refinement 缓存改进<br>Step2: Semantic graph construction 语义图构建<br>Step3: Hyper-class querying 超类查询<br>Output: Adapted predictions 适应性预测 |
7.5 | [[7.5] 2503.24306 Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge](https://arxiv.org/abs/2503.24306) <br> [{'name': "Adam Schmidt, Mert Asim Karaoglu, Soham Sinha, Mingang Jang, Ho-Gun Ha, Kyungmin Jung, Kyeongmo Gu, Ihsan Ullah, Hyunki Lee, Jon\\'a\\v{s} \\v{S}er\\'ych, Michal Neoral, Ji\\v{r}\\'i Matas, Rulin Zhou, Wenlong He, An Wang, Hongliang Ren, Bruno Silva, Sandro Queir\\'os, Est\\^ev\\~ao Lima, Jo\\~ao L. Vila\\c{c}a, Shunsuke Kikuchi, Atsushi Kouno, Hiroki Matsuzaki, Tongtong Li, Yulu Chen, Ling Li, Xiang Ma, Xiaojian Li, Mona Sheikh Zeinoddin, Xu Wang, Zafer Tandogdu, Greg Shaw, Evangelos Mazomenos, Danail Stoyanov, Yuxin Chen, Zijian Wu, Alexander Ladikos, Simon DiMaio, Septimiu E. Salcudean, Omid Mohareri"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point tracking<br>3D reconstruction<br>surgery<br>autonomous probe-based scanning | Input: Point tracking data for surgery 手术点跟踪数据<br>Step1: Challenge design 挑战设计<br>Step2: Algorithm submission and evaluation 算法提交与评估<br>Step3: Performance measurement based on accuracy and efficiency 性能测量基于准确性和效率<br>Output: Quantitative results for tracking algorithms 跟踪算法的定量结果 |


## Arxiv 2025-03-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.21958 NeRF-based Point Cloud Reconstruction using a Stationary Camera for Agricultural Applications](https://arxiv.org/abs/2503.21958) <br> [{'name': 'Kibon Ku, Talukder Z Jubery, Elijah Rodriguez, Aditya Balu, Soumik Sarkar, Adarsh Krishnamurthy, Baskar Ganapathysubramanian'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>NeRF<br>point cloud<br>agriculture | Input: Images captured by a stationary camera 静态相机捕获的图像<br>Step1: COLMAP-based pose estimation COLMAP基础的姿态估计<br>Step2: Pose transformation to simulate camera movement 姿态转换以模拟相机移动<br>Step3: NeRF training using captured images 使用捕获的图像进行NeRF训练<br>Output: High-resolution point clouds 高分辨率点云 |
9.5 | [[9.5] 2503.22060 Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges](https://arxiv.org/abs/2503.22060) <br> [{'name': 'Ukcheol Shin, Jinsun Park'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>thermal imaging<br>autonomous driving<br>multi-modal dataset<br>robust perception | Input: Synchronized multi-modal data 包含同步的多模态数据<br>Step1: Dataset construction 数据集构建<br>Step2: Depth estimation evaluation 深度估计评估<br>Step3: Benchmark analysis 基准分析<br>Output: Standardized benchmark results 标准化基准结果 |
9.5 | [[9.5] 2503.22087 Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction](https://arxiv.org/abs/2503.22087) <br> [{'name': 'Seokha Moon, Janghyun Baek, Giseop Kim, Jinkyu Kim, Sunwook Choi'}] | 3D Occupancy Prediction 3D占用预测 | v2<br>3D occupancy prediction 3D占用预测<br>autonomous driving 自动驾驶<br>multi-view images 多视角图像 | Input: Multi-view images 多视角图像<br>Step1: Stream-based Voxel Aggregation 流式体素聚合<br>Step2: Query-guided Aggregation 查询引导聚合<br>Step3: Model evaluation 模型评估<br>Output: 3D occupancy prediction 3D占用预测 |
9.5 | [[9.5] 2503.22154 Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds](https://arxiv.org/abs/2503.22154) <br> [{'name': 'Jae-Young Yim, Dongwook Kim, Jae-Young Sim'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D point clouds<br>dataset distillation<br>feature alignment | Input: 3D point clouds 3D点云<br>Step1: Permutation invariant feature matching 排列不变特征匹配<br>Step2: Orientation optimization 方向优化<br>Step3: Dataset distillation 数据集蒸馏<br>Output: Optimized synthetic dataset 优化的合成数据集 |
9.5 | [[9.5] 2503.22204 Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting](https://arxiv.org/abs/2503.22204) <br> [{'name': 'Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, Yu Yin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D segmentation 3D分割<br>Gaussian Splatting 高斯点云<br>autonomous systems 自主系统 | Input: Multi-view images 多视角图像<br>Step1: Object-specific Gaussian initialization 面向对象的高斯初始化<br>Step2: Segmentation via Gaussian Splatting 通过高斯点云分割<br>Step3: Optimization and scene reconstruction 优化和场景重建<br>Output: 3D object segmentation output 3D对象分割结果 |
9.5 | [[9.5] 2503.22231 CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving](https://arxiv.org/abs/2503.22231) <br> [{'name': 'Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, Haiyang Sun, Bing Wang, Tong Lu'}] | 3D Generation 三维生成 | v2<br>3D generation<br>autonomous driving<br>video generation<br>3D consistency | Input: HD maps and bounding boxes 用于视频生成的HD地图和边界框<br>Step1: Generate 3D conditions 生成3D条件<br>Step2: Develop spatially adaptive framework 开发空间自适应框架<br>Step3: Incorporate consistency adapter 添加一致性适配器<br>Output: High-quality driving videos 生成高质量的驾驶视频 |
9.5 | [[9.5] 2503.22324 AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation](https://arxiv.org/abs/2503.22324) <br> [{'name': 'Chenyang Xu, XingGuo Deng, Rui Zhong'}] | 3D Reconstruction 三维重建 | v2<br>3D Gaussian Splatting<br>3D reconstruction<br>Novel View Synthesis | Input: Scene representation using 3D Gaussian Splatting 3D高斯点云<br>Step1: Enhance manifold complexity of input features 加强输入特征的流形复杂性<br>Step2: Implement Adaptive Frequency Encoding Module (AFEM) 实现自适应频率编码模块<br>Step3: Apply high-frequency reinforce loss 使用高频强化损失<br>Output: Improved rendering fidelity and high-frequency detail 改进的渲染保真度和高频细节 |
9.5 | [[9.5] 2503.22328 VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow](https://arxiv.org/abs/2503.22328) <br> [{'name': 'Yancong Lin, Shiming Wang, Liangliang Nan, Julian Kooij, Holger Caesar'}] | Scene Flow Estimation 场景流估计 | v2<br>scene flow<br>motion rigidity<br>autonomous driving | Input: LiDAR scans from autonomous driving applications LiDAR扫描<br>Step1: Data collection 数据收集<br>Step2: Implementation of a Voting Module 投票模块的实施<br>Step3: Scene flow estimation using local rigidity scenes 估计使用局部刚度的场景流<br>Output: Enhanced motion estimation 改进的运动估计 |
9.5 | [[9.5] 2503.22349 GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion](https://arxiv.org/abs/2503.22349) <br> [{'name': 'Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, Hongbo Fu, Hua Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>pose-free reconstruction<br>camera pose estimation<br>sparse view<br>surface reconstruction | Input: Unposed images 未标定图像<br>Step1: Implement Geometric Consistent Ray Diffusion model (GCRayDiffusion) 实施几何一致的射线扩散模型 (GCRayDiffusion)<br>Step2: Use triplane-based signed distance field (SDF) for learning 使用三平面签名距离场 (SDF) 进行学习<br>Step3: Improve camera pose estimation and surface reconstruction through neural rays 改善相机位姿估计和表面重建通过神经射线<br>Output: Accurate pose-free surface reconstruction results 精确的无位姿表面重建结果 |
9.5 | [[9.5] 2503.22430 MVSAnywhere: Zero-Shot Multi-View Stereo](https://arxiv.org/abs/2503.22430) <br> [{'name': 'Sergio Izquierdo, Mohamed Sayed, Michael Firman, Guillermo Garcia-Hernando, Daniyar Turmukhambetov, Javier Civera, Oisin Mac Aodha, Gabriel Brostow, Jamie Watson'}] | Multi-view Stereo 多视角立体 | v2<br>Multi-View Stereo<br>Depth Estimation<br>3D Reconstruction<br>Zero-Shot Learning | Input: Multiple posed RGB images 多个姿态的RGB图像<br>Step 1: Depth estimation using transformer architecture 使用变压器架构进行深度估计<br>Step 2: Cost volume construction using geometric metadata 使用几何元数据构造成本体积<br>Step 3: Model evaluation and comparison with baselines 模型评估及与基线比较<br>Output: Accurate and 3D-consistent depth maps 输出：准确且三维一致的深度图 |
9.5 | [[9.5] 2503.22436 NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](https://arxiv.org/abs/2503.22436) <br> [{'name': 'Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng'}] | 3D Visual Grounding 视觉定位 | v2<br>multi-view 3D visual grounding<br>autonomous driving<br>language grounding<br>object localization | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Instruction processing 指令处理<br>Step3: Localization using 3D geometric information 使用3D几何信息进行定位<br>Output: Localized target objects 本地化目标对象 |
9.5 | [[9.5] 2503.22437 EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting](https://arxiv.org/abs/2503.22437) <br> [{'name': 'Xu Wang, Shuai Zhang, Baoru Huang, Danail Stoyanov, Evangelos B. Mazomenos'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction<br>Endoscopic Surgery<br>Gaussian Splatting | Input: Endoscopic videos 内窥镜视频<br>Step1: Depth estimation 深度估计<br>Step2: Model generation 模型生成<br>Step3: Scene reconstruction 场景重建<br>Output: Complete 3D surgical scenes 完整的三维手术场景 |
9.5 | [[9.5] 2503.22537 LIM: Large Interpolator Model for Dynamic Reconstruction](https://arxiv.org/abs/2503.22537) <br> [{'name': 'Remy Sabathier, Niloy J. Mitra, David Novotny'}] | Dynamic Reconstruction 动态重建 | v2<br>4D reconstruction<br>implicit 3D representations<br>mesh tracking | Input: Implicit 3D representations at times t0 and t1<br>Step1: Interpolation using causal consistency loss<br>Step2: Mesh tracking across time<br>Output: High-speed tracked 4D assets |
9.5 | [[9.5] 2503.22676 TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting](https://arxiv.org/abs/2503.22676) <br> [{'name': 'Boyang (Tony),  Yu, Yanlin Jin, Ashok Veeraraghavan, Akshat Dave, Guha Balakrishnan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>object transfer<br>relighting<br>scene rendering | Input: Multi-view images containing objects from a source scene with delineating masks.<br>Step1: Fit a Gaussian Splatting model to both source and target scenes for object extraction and environment mapping.<br>Step2: Perform 3D object segmentation based on 2D masks to extract precise object geometry.<br>Step3: User-guided insertion of the extracted object into the target scene with automatic position and orientation refinement.<br>Step4: Calculate per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt object's appearance for the target scene lighting.<br>Output: Realistically transferred 3D objects in the target scene. |
9.5 | [[9.5] 2503.22677 DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness](https://arxiv.org/abs/2503.22677) <br> [{'name': 'Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi'}] | 3D Generation 三维生成 | v2<br>3D reconstruction<br>physical stability<br>simulation feedback | Input: 3D object images 物体图像<br>Step1: Construct stability score dataset 构建稳定性评分数据集<br>Step2: Fine-tune 3D generator using stability scores 使用稳定性评分微调3D生成器<br>Step3: Evaluate physical stability 评估物理稳定性<br>Output: Physically stable 3D objects 物理稳定的3D对象 |
9.2 | [[9.2] 2503.22218 ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting](https://arxiv.org/abs/2503.22218) <br> [{'name': 'Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li'}] | Neural Rendering 神经渲染 | v2<br>3D style transfer<br>Neural Rendering<br>3D Gaussian Splatting | Input: Scene content and style images 场景内容和风格图像<br>Step1: Controllable matching of images 可控图像匹配<br>Step2: Feature alignment for style transfer 特征对齐以进行风格转换<br>Step3: Style transfer with depth preservation 保持深度的风格转换<br>Output: Stylized 3D scenes 风格化的三维场景 |
9.2 | [[9.2] 2503.22351 One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images](https://arxiv.org/abs/2503.22351) <br> [{'name': 'Byeongjun Kwon, Munchurl Kim'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>high-resolution images<br>depth discontinuity | Input: High-resolution images 高分辨率图像<br>Step1: Grouped Patch Consistency Training 组块一致性训练<br>Step2: Bias Free Masking 去偏见掩码<br>Step3: Depth refinement on each patch 每个块的深度修正<br>Output: Accurate depth estimation results 准确的深度估计结果 |
8.5 | [[8.5] 2503.21830 Shape Generation via Weight Space Learning](https://arxiv.org/abs/2503.21830) <br> [{'name': 'Maximilian Plattner, Arturs Berzins, Johannes Brandstetter'}] | 3D Generation 三维生成 | v2<br>3D shape generation<br>weight space learning<br>topology<br>geometry<br>phase transition | Input: 3D shape-generative model 3D形状生成模型<br>Step1: Analyze weight space weight space分析<br>Step2: Experiment with phase transitions 进行相变实验<br>Step3: Ensure controlled geometry changes 确保控制几何变化<br>Output: Enhanced shape generation capabilities 改进的形状生成能力 |
8.5 | [[8.5] 2503.22020 CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](https://arxiv.org/abs/2503.22020) <br> [{'name': 'Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin'}] | Robotics and Vision-Language Models 机器人和视觉语言模型 | v2<br>vision-language-action models<br>robot manipulation<br>visual reasoning<br>chain-of-thought reasoning | Input: Visual-language-action models 视觉语言动作模型<br>Step1: Incorporate visual chain-of-thought reasoning 引入视觉思维链推理<br>Step2: Generate subgoal images 生成子目标图像<br>Step3: Predict action sequences 预测动作序列<br>Output: Enhanced robotic control capabilities 增强的机器人控制能力 |
8.5 | [[8.5] 2503.22093 How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](https://arxiv.org/abs/2503.22093) <br> [{'name': 'Ximing Wen, Mallika Mainali, Anik Sen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Theory of Mind<br>Visual Question Answering<br>Human Intentions<br>Multimodal Learning | Input: Visual scenarios and VLMs 输入：视觉场景和视觉语言模型<br>Step1: Develop open-ended question framework 开发开放式问题框架<br>Step2: Curate and annotate benchmark dataset 策划和注释基准数据集<br>Step3: Assess performance of VLMs 评估视觉语言模型的性能<br>Output: Evaluation results and insights 输出：评估结果和见解 |
8.5 | [[8.5] 2503.22194 ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation](https://arxiv.org/abs/2503.22194) <br> [{'name': 'Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung'}] | Image Generation 图像生成 | v2<br>3D orientation grounding<br>text-to-image generation | Input: Text prompts and multi-view objects 文字提示和多视角对象<br>Step 1: 3D orientation estimation for multiple objects 多个对象的3D方向估计<br>Step 2: Reward-guided sampling using Langevin dynamics 奖励引导采样使用Langevin动力学<br>Step 3: Model evaluation and comparison with existing methods 模型评估与现有方法对比<br>Output: 3D orientated images 3D定向图像 |
8.5 | [[8.5] 2503.22201 Multi-modal Knowledge Distillation-based Human Trajectory Forecasting](https://arxiv.org/abs/2503.22201) <br> [{'name': 'Jaewoo Jeong, Seohee Lee, Daehee Park, Giwon Lee, Kuk-Jin Yoon'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>trajectory forecasting<br>knowledge distillation<br>autonomous driving<br>multi-modal systems | Input: Limited modality student model 受限的模态学生模型<br>Step1: Train teacher model with full modalities 训练全模态教师模型<br>Step2: Distill knowledge to student model 从教师模型向学生模型蒸馏知识<br>Step3: Validate with datasets 验证数据集<br>Output: Enhanced prediction accuracy 改进的预测精度 |
8.5 | [[8.5] 2503.22209 Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces](https://arxiv.org/abs/2503.22209) <br> [{'name': 'Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, Sunghoon Im'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>intrinsic image decomposition<br>self-supervised learning | Input: Sequential images 序列图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model training and evaluation 模型训练与评估<br>Output: Depth prediction and intrinsic images 深度预测与内在图像 |
8.5 | [[8.5] 2503.22262 Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion](https://arxiv.org/abs/2503.22262) <br> [{'name': 'Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, Ying Shan, Huchuan Lu'}] | Multi-view and Stereo Vision 多视角立体 | v2<br>stereo conversion<br>evaluation metric<br>3D content production | Input: Monocular images 单眼图像<br>Step1: Dataset creation 数据集创建<br>Step2: Empirical evaluation 实证评估<br>Step3: New metric proposal 新指标提出<br>Output: Enhanced stereo conversion model 改进的立体转换模型 |
8.5 | [[8.5] 2503.22309 A Dataset for Semantic Segmentation in the Presence of Unknowns](https://arxiv.org/abs/2503.22309) <br> [{'name': 'Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, Jiri Matas, Giorgos Tolias, C. V. Jawahar'}] | Image and Video Generation 图像生成 | v2<br>semantic segmentation<br>autonomous driving<br>anomaly detection | Input: Real-world images from diverse environments 真实场景中的图像<br>Step 1: Dataset creation 数据集创建<br>Step 2: Labeling with closed-set and anomaly classes 标签闭集和异常类别<br>Step 3: Controlled evaluation 控制评估<br>Output: Comprehensive anomaly segmentation dataset 综合异常分割数据集 |
8.5 | [[8.5] 2503.22375 Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance](https://arxiv.org/abs/2503.22375) <br> [{'name': 'Christian Steinhauser, Philipp Reis, Hubert Padusinski, Jacob Langner, Eric Sax'}] | Autonomous Driving 自动驾驶 | v2<br>image quality<br>machine learning<br>automotive perception<br>object detection<br>segmentation | Input: Modified images from automotive datasets 经过修改的汽车数据集中的图像<br>Step1: Data preparation 数据准备<br>Step2: Quantification of image deviations 图像偏差的量化<br>Step3: Performance evaluation of ML models 机器学习模型性能评估<br>Step4: Correlation analysis of image quality and performance 图像质量与性能的相关性分析<br>Output: Insights into the impact of image quality on ML performance 输出：图像质量对机器学习性能的影响见解 |
8.5 | [[8.5] 2503.22420 Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis](https://arxiv.org/abs/2503.22420) <br> [{'name': 'Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, Song-Chun Zhu, Siyuan Huang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D vision-language<br>benchmarking<br> QA tasks | Input: 3D vision-language models 3D视觉语言模型<br>Step1: Benchmark evaluation 基准评估<br>Step2: Object-centric testing 物体中心测试<br>Step3: Performance analysis 性能分析<br>Output: Comprehensive metrics for 3D-VL models 3D-VL模型的综合性能指标 |
8.5 | [[8.5] 2503.22462 SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations](https://arxiv.org/abs/2503.22462) <br> [{'name': 'Krispin Wandel, Hesheng Wang'}] | 3D Object Recognition 物体识别 | v2<br>3D object-class representations<br>semantic correspondence | Input: RGB images and monocular depth estimates RGB图像和单目深度估计<br>Step1: Build 3D object-class representations from depth estimates 从深度估计构建3D物体类别表示<br>Step2: Formulate alignment energy using gradient descent 使用梯度下降公式化对齐能量<br>Step3: Minimize alignment energy to establish correspondence 最小化对齐能量以建立对应关系<br>Output: Robust semantic correspondence across varying views 输出：在变化视角下的鲁棒语义对应 |
8.5 | [[8.5] 2503.22622 Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model](https://arxiv.org/abs/2503.22622) <br> [{'name': 'Jangho Park, Taesung Kwon, Jong Chul Ye'}] | Image and Video Generation 图像生成与视频生成 | v2<br>4D video generation<br>video diffusion models<br>spatio-temporal consistency | Input: Single monocular video 单个单目视频<br>Step1: Synthesize edge frames using video diffusion model 使用视频扩散模型合成边缘帧<br>Step2: Interpolate remaining frames to construct a coherent sampling grid 插值剩余帧以构建一致的采样网格<br>Output: Multi-view synchronized 4D video 生成多视角同步4D视频 |


## Arxiv 2025-03-28

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.21082 Can Video Diffusion Model Reconstruct 4D Geometry?](https://arxiv.org/abs/2503.21082) <br> [{'name': 'Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, J\\"urgen Schmidhuber, Bernard Ghanem'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D geometry reconstruction 4D几何重建<br>video diffusion model 视频扩散模型 | Input: Monocular video 单目视频<br>Step1: Adapt a pointmap VAE from a pretrained video VAE 从预训练视频VAE适应一个点图VAE<br>Step2: Finetune a diffusion backbone in combined video and pointmap latent space 在结合视频和点图潜在空间中微调扩散骨干<br>Output: Coherent 4D pointmaps 统一的4D点图 |
9.5 | [[9.5] 2503.21104 StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency](https://arxiv.org/abs/2503.21104) <br> [{'name': 'Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, Xianpeng Lang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction 三维重建<br>autonomous driving 自动驾驶<br>urban simulation 城市模拟 | Input: Street scenes with multi-camera setups 街景与多摄像头设置<br>Step1: Pose optimization for cameras 摄像头姿态优化<br>Step2: Hybrid embedding for scene and style separation 场景与风格分离的混合嵌入<br>Step3: Uncertainty-aware rendering for consistent output 不确定性感知渲染以确保一致性<br>Output: Photo-realistic urban scenes with invariant geometry 输出: 保持几何不变的照片真实感城市场景 |
9.5 | [[9.5] 2503.21214 VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation](https://arxiv.org/abs/2503.21214) <br> [{'name': 'Alan Dao (Gia Tuan Dao), Norapat Buppodom'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D understanding 3D理解<br>Voxel representation 体素表示<br>Vision-Language Models 视觉语言模型 | Input: 3D voxel grid 3D体素网格<br>Step1: Decompose into 2D slices 沿主要轴切分成2D切片<br>Step2: Format and feed into VLM 输入格式化并送入视觉语言模型<br>Step3: Aggregate and interpret features 聚合并解释特征<br>Output: Structured voxel semantics 输出结构化的体素语义 |
9.5 | [[9.5] 2503.21219 GenFusion: Closing the Loop between Reconstruction and Generation via Videos](https://arxiv.org/abs/2503.21219) <br> [{'name': 'Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>video generation<br>novel view synthesis | Input: RGB-D videos RGB-D视频<br>Step1: Fine-tune video model 视频模型微调<br>Step2: Masked 3D reconstruction 3D重建遮罩处理<br>Step3: Cyclic fusion pipeline 循环融合流程<br>Output: Artifact-free 3D models 无伪影的三维模型 |
9.5 | [[9.5] 2503.21226 Frequency-Aware Gaussian Splatting Decomposition](https://arxiv.org/abs/2503.21226) <br> [{'name': 'Yishai Lavi, Leo Segre, Shai Avidan'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>frequency decomposition<br>3D editing<br>view synthesis | Input: Images for Gaussian Splatting 输入图像用于高斯点云处理<br>Step1: Group 3D Gaussians based on frequency subbands 按照频率子带分组3D高斯<br>Step2: Apply dedicated regularization to maintain coherence 应用特殊正则化以保持一致性<br>Step3: Implement a progressive training scheme for optimization 实施渐进培训方案以优化<br>Output: Frequency-aware 3D representation with enhanced editing capabilities 输出: 增强编辑能力的频率感知3D表示 |
9.5 | [[9.5] 2503.21313 HORT: Monocular Hand-held Objects Reconstruction with Transformers](https://arxiv.org/abs/2503.21313) <br> [{'name': 'Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dense point clouds<br>transformers | Input: Monocular images 单目图像<br>Step1: Generate sparse point cloud 生成稀疏点云<br>Step2: Refine to dense representation 精炼到密集表示<br>Step3: Jointly predict object point cloud and pose 共同预测物体点云和姿态<br>Output: High-resolution 3D point clouds 输出高分辨率3D点云 |
9.5 | [[9.5] 2503.21364 LandMarkSystem Technical Report](https://arxiv.org/abs/2503.21364) <br> [{'name': 'Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, Rong Fu, Hengjie Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Neural Radiance Fields<br>3D Gaussian Splatting<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Componentized model adaptation 组件化模型自适应<br>Step2: Distributed parallel computing 分布式并行计算<br>Step3: Dynamic loading strategy 动态加载策略<br>Output: Enhanced 3D reconstruction and rendering 改进的三维重建与渲染 |
9.5 | [[9.5] 2503.21449 Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving](https://arxiv.org/abs/2503.21449) <br> [{'name': 'Lucas Nunes, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss'}] | 3D Generation 三维生成 | v2<br>3D semantic generation<br>data annotation<br>autonomous driving | Input: Semantic scene data 语义场景数据<br>Step1: Train a diffusion model 训练扩散模型<br>Step2: Generate realistic 3D semantic scenes 生成真实的3D语义场景<br>Step3: Evaluate synthetic data for training 评估合成数据的训练效果<br>Output: Improved semantic segmentation performance 改进的语义分割性能 |
9.5 | [[9.5] 2503.21525 ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo](https://arxiv.org/abs/2503.21525) <br> [{'name': 'Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, Runze Yuan, Friedrich Fraundorfer'}] | Multi-view Stereo 多视角立体 | v2<br>Multi-view Stereo<br>3D reconstruction<br>depth estimation | Input: Series of overlapping images 重叠的图像序列<br>Step1: Feature extraction 特征提取<br>Step2: Intra-view feature fusion intra-view特征融合<br>Step3: Cross-view aggregation cross-view聚合<br>Step4: Depth estimation 深度估计<br>Output: 3D point cloud 3D点云 |
9.5 | [[9.5] 2503.21581 AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion](https://arxiv.org/abs/2503.21581) <br> [{'name': 'Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia'}] | 3D Perception and Calibration 三维感知与标定 | v2<br>camera calibration<br>3D perception<br>diffusion model | Input: Video sequences 视频序列<br>Step1: Condition diffusion model with line embeddings 利用线嵌入条件化扩散模型<br>Step2: Edge-aware attention focuses on geometric features 边缘关注观点强调几何特征<br>Step3: Joint estimation of intrinsic and extrinsic parameters 同时估计内外参数<br>Output: Accurate camera calibration outputs 准确的相机标定输出 |
9.5 | [[9.5] 2503.21659 InteractionMap: Improving Online Vectorized HDMap Construction with Interaction](https://arxiv.org/abs/2503.21659) <br> [{'name': 'Kuang Wu, Chuan Yang, Zhanbin Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>HD maps<br>autonomous driving<br>map vectorization | Input: High-definition map data 高精度地图数据<br>Step1: Enhance detectors using position relation embedding 增强检测器的位置信息嵌入<br>Step2: Key-frame-based hierarchical temporal fusion 模块 关键帧基础的分层时间融合<br>Step3: Introduce geometry-aware classification loss 引入几何感知分类损失<br>Output: Improved vectorized HD map outputs 改进的矢量化高清地图输出 |
9.5 | [[9.5] 2503.21692 RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond](https://arxiv.org/abs/2503.21692) <br> [{'name': 'Daniel Bermuth, Alexander Poeppel, Wolfgang Reif'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>pose estimation<br>multi-view | Input: Multi-view images and 2D poses 多视角图像和2D姿势<br>Step1: Predict 2D poses for each image 预测每幅图像的2D姿势<br>Step2: Filter pairs of poses using previous 3D poses 使用先前3D姿势筛选姿势对<br>Step3: Triangulate to create 3D proposals 三角测量生成3D提案<br>Step4: Reproject and evaluate reprojection error 重新投影并评估重投影误差<br>Output: Accurate 3D human poses 准确的3D人类姿势 |
9.5 | [[9.5] 2503.21732 SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling](https://arxiv.org/abs/2503.21732) <br> [{'name': 'Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, Yangguang Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>mesh modeling<br>high-resolution shapes | Input: Sparse-structured isosurface representation 稀疏结构的等值面表示<br>Step1: Frustum-aware sectional voxel training 分段体素训练<br>Step2: Differentiable mesh reconstruction 可微分网格重建<br>Step3: Shape modeling pipeline construction 形状建模管道构建<br>Output: High-resolution 3D models 高分辨率三维模型 |
9.5 | [[9.5] 2503.21745 3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models](https://arxiv.org/abs/2503.21745) <br> [{'name': 'Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, Ziwei Liu'}] | 3D Generation 三维生成 | v2<br>3D evaluation 3D评估<br>3D generation 3D生成<br>human preference 人类偏好 | Input: Text and image prompts 文本和图像提示<br>Step1: Develop 3DGen-Arena platform 开发3DGen-Arena平台<br>Step2: Gather human preferences 收集人类偏好<br>Step3: Train scoring models 训练评分模型<br>Output: 3DGen-Bench dataset 生成3DGen-Bench数据集 |
9.5 | [[9.5] 2503.21761 Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video](https://arxiv.org/abs/2503.21761) <br> [{'name': 'David Yifan Yao, Albert J. Zhai, Shenlong Wang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>4D modeling<br>3D reconstruction<br>dynamic scenes<br>optimization | Input: Casual video inputs 从普通视频输入<br>Step1: Multi-stage optimization framework 多阶段优化框架<br>Step2: Integration of pretrained models 集成预训练模型<br>Step3: Estimation of camera poses, static and dynamic geometry and motion 相机姿态、静态和动态几何与运动的估计<br>Output: Accurate 4D scene models 生成准确的4D场景模型 |
9.5 | [[9.5] 2503.21766 Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence](https://arxiv.org/abs/2503.21766) <br> [{'name': 'Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, Xiaoguang Han'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Shape Correspondence<br>Registration-based Framework<br>Neural Rendering | Input: Source and target mesh 源网格和目标网格<br>Step1: Registration of source mesh to target mesh 源网格注册到目标网格<br>Step2: Establish dense correspondence between shapes 建立形状间的稠密对应关系<br>Step3: Apply Semantic Flow Guided Registration application 使用语义流引导注册<br>Output: Stable dense correspondence output 稳定的稠密对应输出 |
9.5 | [[9.5] 2503.21767 Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying](https://arxiv.org/abs/2503.21767) <br> [{'name': 'Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>open-vocabulary querying<br>point-level querying | Input: 3D Gaussian representation 3D高斯表示<br>Step1: Utilize masklets for ground-truth generation 利用masklet生成基准真相<br>Step2: Implement a two-step querying process 实现两步查询过程<br>Output: Retrieved relevant 3D Gaussians 相关3D高斯的提取 |
9.5 | [[9.5] 2503.21778 HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM](https://arxiv.org/abs/2503.21778) <br> [{'name': 'Ziren Gong, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Matteo Poggi'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>Dense SLAM<br>3D reconstruction<br>Structural Supervision | Input: RGB-D data with potential structure scenes RGB-D 数据与潜在结构场景<br>Step1: Hybrid encoding network to enhance scene representation 集成编码网络以增强场景表示<br>Step2: Structural supervision for scene understanding 结构监督以理解场景<br>Step3: Active global bundle adjustment for consistency 激活全局束调整以确保一致性<br>Output: Accurate dense maps with improved tracking and reconstruction 准确的密集地图及改进的跟踪与重建 |
9.2 | [[9.2] 2503.21751 Reconstructing Humans with a Biomechanically Accurate Skeleton](https://arxiv.org/abs/2503.21751) <br> [{'name': 'Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>biomechanical skeleton<br>transformer | Input: Single image 单幅图像<br>Step1: Generate pseudo ground truth 生成伪真实数据<br>Step2: Train transformer to estimate parameters 训练变换器以估计参数<br>Step3: Iterative refinement of pseudo labels 伪标签的迭代优化<br>Output: 3D human reconstruction 3D人体重建 |
8.5 | [[8.5] 2503.20936 LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos](https://arxiv.org/abs/2503.20936) <br> [{'name': 'Daniel Etaat, Dvij Kalaria, Nima Rahmanian, Shankar Sastry'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anticipatory control<br>table tennis robotics | Input: Monocular videos 单目视频<br>Step1: Data integration 数据集成<br>Step2: 3D reconstruction 3D重建<br>Step3: Anticipatory control algorithm 开发预测控制算法<br>Output: Enhanced ball return rate 改进的击球回报率 |
8.5 | [[8.5] 2503.21099 Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection](https://arxiv.org/abs/2503.21099) <br> [{'name': 'Yun Zhu, Le Hui, Hang Yang, Jianjun Qian, Jin Xie, Jian Yang'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection<br>sparse supervision<br>prototypes<br>indoor and outdoor scenes | Input: Sparse supervised 3D object detection data 稀疏监督3D目标检测数据<br>Step1: Prototype-based object mining module 原型基础的对象挖掘模块<br>Step2: Optimal transport matching 最优传输匹配<br>Step3: Multi-label cooperative refinement module 多标签协同精练模块<br>Output: Enhanced detection performance 改进的检测性能 |
8.5 | [[8.5] 2503.21268 ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate](https://arxiv.org/abs/2503.21268) <br> [{'name': 'Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, Lincai Zhong, Yuexin Ma, Lan Xu, Chenglu Wen, Siqi Shen, Cheng Wang'}] | Human Motion Recovery 人体运动恢复 | v2<br>3D reconstruction<br>human motion recovery<br>autonomous driving | Input: RGB and LiDAR data<br>Step1: Collecting and annotating climbing motion data<br>Step2: Developing ClimbingCap method for motion reconstruction<br>Step3: Evaluating performance on climbing motion recovery<br>Output: Continuous 3D human climbing motion in global coordinates |
8.5 | [[8.5] 2503.21338 UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based on Uncertainty-Guided NeRF Augmentation](https://arxiv.org/abs/2503.21338) <br> [{'name': 'Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen'}] | Visual Place Recognition 视觉地点识别 | v2<br>Visual Place Recognition<br>NeRF<br>Data Augmentation<br>Autonomous Navigation<br>3D reconstruction | Input: Existing VPR dataset 现有VPR数据集<br>Step1: Train NeRF using existing VPR data 使用现有VPR数据训练NeRF<br>Step2: Identify high uncertainty places using uncertainty estimation network 使用不确定性估计网络识别高不确定性的位置<br>Step3: Generate synthetic observations with selected poses through NeRF 通过NeRF生成选定姿态的合成观测<br>Output: Enhanced VPR training data 改进的VPR训练数据 |
8.5 | [[8.5] 2503.21477 Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction Method](https://arxiv.org/abs/2503.21477) <br> [{'name': 'Wenyi Xiong, Jian Chen, Ziheng Qi'}] | Autonomous Systems and Robotics 自主系统与机器人 | trajectory prediction 轨迹预测<br>autonomous driving 自动驾驶<br>lane constraints 车道约束 | Input: Trajectory data 轨迹数据<br>Step1: Behavioral intention recognition 行为意图识别<br>Step2: Lane constraint modeling 车道约束建模<br>Step3: Dual-stream architecture integration 双流架构集成<br>Step4: Trajectory proposal generation 轨迹提议生成<br>Step5: Point-level refinement 点级细化<br>Output: Fine-grained trajectory predictions 精细化轨迹预测 |
8.5 | [[8.5] 2503.21562 uLayout: Unified Room Layout Estimation for Perspective and Panoramic Images](https://arxiv.org/abs/2503.21562) <br> [{'name': 'Jonathan Lee, Bolivar Solarte, Chin-Hsuan Wu, Jin-Cheng Jhang, Fu-En Wang, Yi-Hsuan Tsai, Min Sun'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>room layout estimation<br>3D reconstruction<br>panoramic images | Input: Perspective and panoramic images 透视和全景图像<br>Step1: Project input images into equirectangular coordinates 将输入图像投影到等经纬度坐标<br>Step2: Use shared feature extractor with domain-specific conditioning 使用共享特征提取器并进行领域特定条件处理<br>Step3: Apply column-wise feature regression 应用列-wise 特征回归<br>Output: Estimated room layout geometries 估计的房间布局几何 |
8.5 | [[8.5] 2503.21723 OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation](https://arxiv.org/abs/2503.21723) <br> [{'name': 'Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D hand pose estimation<br>occlusion<br>autonomous systems<br>CNN<br>transformer | Input: RGB image RGB图像<br>Step1: Localizing hand joints using CNN 定位手关节采用CNN<br>Step2: Refining joint estimates using contextual information 使用上下文信息细化关节估计<br>Step3: Identifying joints with self-attention and cross-attention mechanisms 使用自注意力和交叉注意力机制识别关节<br>Output: Accurate 3D hand-object pose estimates 输出: 精确的3D手-物体姿态估计 |
8.5 | [[8.5] 2503.21755 VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](https://arxiv.org/abs/2503.21755) <br> [{'name': 'Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>Video Generation 视频生成<br>Intrinsic Faithfulness 内在真实 | Input: Video generative models 视频生成模型<br>Step1: Establish evaluation metrics 建立评估指标<br>Step2: Benchmark development 基准开发<br>Step3: Model assessment 模型评估<br>Output: Intrinsically faithful video generation outputs 本质真实的视频生成结果 |
8.5 | [[8.5] 2503.21779 X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction](https://arxiv.org/abs/2503.21779) <br> [{'name': 'Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D CT reconstruction<br>Gaussian splatting<br>dynamic imaging<br>respiratory motion learning | Input: Projections of dynamic anatomical structures 3D动态解剖结构的投影<br>Step1: Model continuous anatomical motion 建模连续解剖运动<br>Step2: Apply radiative Gaussian splatting 应用辐射高斯点云<br>Step3: Implement self-supervised learning 实现自监督学习<br>Output: 4D CT reconstruction of continuous motion 输出：连续运动的4D CT重建 |
7.5 | [[7.5] 2503.21483 BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](https://arxiv.org/abs/2503.21483) <br> [{'name': 'Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem'}] | VLM & VLA 视觉语言模型与对齐 | v2<br>Video-Language Models<br>frame selection<br>video understanding | Input: Long-form videos 长视频<br>Step1: Frame selection strategy evaluation 帧选择策略评估<br>Step2: Implementation of inverse transform sampling 逆变换采样的实现<br>Step3: Performance assessment on video benchmarks 视频基准上的性能评估<br>Output: Improved video understanding performance 提升的视频理解性能 |


## Arxiv 2025-03-27

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.20168 EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis](https://arxiv.org/abs/2503.20168) <br> [{'name': 'Sheng Miao, Jiaxin Huang, Dongfeng Bai, Xu Yan, Hongyu Zhou, Yue Wang, Bingbing Liu, Andreas Geiger, Yiyi Liao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>autonomous driving<br>real-time rendering | Input: Multiple sparse images 多张稀疏图像<br>Step 1: Initialize noisy depth predictions 初始化噪声深度预测<br>Step 2: Process point cloud with 3D CNN 使用3D卷积神经网络处理点云<br>Step 3: Predict 3D Gaussian properties 预测3D高斯属性<br>Output: Real-time rendering of urban scenes 实时渲染城市场景 |
9.5 | [[9.5] 2503.20211 Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors](https://arxiv.org/abs/2503.20211) <br> [{'name': 'Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, Robby T. Tan'}] | Depth Estimation 深度估计 | v2<br>Depth Estimation 深度估计<br>Autonomous Driving 自动驾驶<br>Robustness 可靠性 | Input: Monocular images 单目图像<br>Step1: Synthetic adaptation with motion structure knowledge 合成适应与运动结构知识<br>Step2: Real adaptation with consistency-reweighting strategy 实际适应与一致性加权策略<br>Step3: Depth estimation model training 深度估计模型训练<br>Output: Robust depth predictions 可靠的深度预测 |
9.5 | [[9.5] 2503.20220 DINeMo: Learning Neural Mesh Models with no 3D Annotations](https://arxiv.org/abs/2503.20220) <br> [{'name': 'Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D pose estimation<br>neural mesh models<br>unlabeled data<br>autonomous systems<br>robotics | Input: Images of objects without 3D annotations 无3D标注的物体图像<br>Step1: Generate pseudo-correspondence 生成伪对应关系<br>Step2: Train neural mesh model using pseudo labels 使用伪标签训练神经网格模型<br>Step3: Evaluate performance on 3D pose estimation 在3D姿态估计上评估性能<br>Output: Accurate 3D pose estimates 准确的3D姿态估计 |
9.5 | [[9.5] 2503.20221 TC-GS: Tri-plane based compression for 3D Gaussian Splatting](https://arxiv.org/abs/2503.20221) <br> [{'name': 'Taorui Wang, Zitong Yu, Yong Xu'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>Compression<br>Tri-plane | Input: Unorganized 3D Gaussian attributes 非结构化的3D高斯属性<br>Step1: Tri-plane encoding of attributes 三平面编码属性<br>Step2: KNN-based decoding for Gaussian distribution KNN解码高斯分布<br>Step3: Adaptive wavelet loss for high-frequency details 自适应小波损失处理高频细节<br>Output: Compressed 3D Gaussian representation 压缩后的3D高斯表示 |
9.5 | [[9.5] 2503.20519 MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation](https://arxiv.org/abs/2503.20519) <br> [{'name': 'Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, Gim Hee Lee'}] | 3D Generation 三维生成 | v2<br>3D generation<br>masked auto-regressive transformer | Input: 3D data 3D数据<br>Step1: Pyramid VAE architecture development Pyramid VAE架构开发<br>Step2: Cascaded MAR generation implementation 级联MAR生成实现<br>Step3: Training with random masking and auto-regressive denoising 随机掩蔽和自回归去噪训练<br>Output: High-resolution 3D meshes 高分辨率3D网格 |
9.5 | [[9.5] 2503.20523 GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving](https://arxiv.org/abs/2503.20523) <br> [{'name': 'Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado'}] | Generative Models for Autonomous Driving 自动驾驶的生成模型 | v2<br>3D modeling<br>autonomous driving<br>scene simulation<br>generative models | Input: Structured conditioning parameters 结构化条件参数<br>Step1: Multi-camera video generation 多摄像头视频生成<br>Step2: Conditioning on driving scenarios 驾驶场景条件化<br>Step3: Fine-grained control over agent behavior 代理行为的细粒度控制<br>Output: High-resolution, temporally consistent videos 高分辨率、时间一致性的视频 |
9.5 | [[9.5] 2503.20784 FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks](https://arxiv.org/abs/2503.20784) <br> [{'name': 'Jinwei Li, Huan-ang Gao, Wenyi Li, Haohan Chi, Chenyu Liu, Chenxi Du, Yiqian Liu, Mingju Gao, Guiyu Zhang, Zongzheng Zhang, Li Yi, Yao Yao, Jingwei Zhao, Hongyang Li, Yikai Wang, Hao Zhao'}] | 3D Generation 三维生成 | v2<br>4D generation<br>dynamic content generation<br>feature bank<br>spatial-temporal consistency<br>multi-view generation | Input: Multi-view and frame sequences 多视角和帧序列<br>Step1: Feature extraction 特征提取<br>Step2: Feature bank integration 特征库集成<br>Step3: Temporal generation algorithm generation 时间生成算法生成<br>Step4: Model evaluation 模型评估<br>Output: Coherent dynamic 3D content 连贯的动态3D内容 |
9.2 | [[9.2] 2503.19947 Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders](https://arxiv.org/abs/2503.19947) <br> [{'name': 'Paul Koch, J\\"org Kr\\"uger, Ankit Chowdhury, Oliver Heimann'}] | Depth Estimation 深度估计 | v2<br>depth understanding<br>vision-guided robotics<br>self-supervised learning | Input: RGB encoders with depth information RGB编码器与深度信息<br>Step1: Self-supervised training pipeline 自监督训练管道<br>Step2: Depth feature extraction 深度特征提取<br>Step3: Performance evaluation 性能评估<br>Output: Enhanced RGBD encoder 改进的RGBD编码器 |
9.0 | [[9.0] 2503.20654 AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports](https://arxiv.org/abs/2503.20654) <br> [{'name': 'Xiangwen Zhang, Qian Zhang, Longfei Han, Qiang Qu, Xiaoming Chen'}] | Autonomous Driving 自动驾驶 | v2<br>3D reconstruction<br>autonomous driving<br>vehicle collision | Input: Real-world accident reports 从真实事故报告中获取信息<br>Step1: Extract physical clues from reports 从报告中提取物理线索<br>Step2: Use physical simulator to replicate trajectories 使用物理模拟器生成碰撞轨迹<br>Step3: Fine-tune language model for scenario predictions 细调语言模型以预测场景<br>Output: Physically realistic vehicle collision videos 生成物理真实感的车辆碰撞视频 |
8.5 | [[8.5] 2503.19953 Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals](https://arxiv.org/abs/2503.19953) <br> [{'name': 'Stefan Stojanov, David Wendt, Seungwoo Kim, Rahul Venkatesh, Kevin Feigelis, Jiajun Wu, Daniel LK Yamins'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人技术 | v2<br>motion estimation<br>self-supervised learning<br>optical flow | Input: Video data 视频数据<br>Step1: Flow and occlusion estimation 流动和遮挡估计<br>Step2: Optimize counterfactual probes 优化反事实探针<br>Step3: Model evaluation 模型评估<br>Output: Motion estimates 运动估计 |
8.5 | [[8.5] 2503.20011 Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception](https://arxiv.org/abs/2503.20011) <br> [{'name': 'Luke Chen, Junyao Wang, Trier Mortlock, Pramod Khargonekar, Mohammad Abdullah Al Faruque'}] | Autonomous Systems and Robotics 自动驾驶和机器人 | v2<br>Uncertainty Quantification<br>Autonomous Vehicles<br>Multimodal Fusion<br>3D Object Detection | Input: Multimodal sensor inputs 多模态传感器输入<br>Step1: Feature extraction 特征提取<br>Step2: Uncertainty quantification 不确定性量化<br>Step3: Feature fusion 特征融合<br>Output: Enhanced perception and detection 改进的感知与检测 |
8.5 | [[8.5] 2503.20235 Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection](https://arxiv.org/abs/2503.20235) <br> [{'name': 'Ahyun Seo, Minsu Cho'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D symmetry detection 3D对称性检测<br>geometric priors 几何先验 | Input: 2D images 2D图像<br>Step1: Predict rotation centers in 3D space 在3D空间中预测旋转中心<br>Step2: Vertex reconstruction enforcing 3D geometric priors 强制执行3D几何先验的顶点重建<br>Step3: Project results back to 2D 将结果投影回2D<br>Output: Detected rotation symmetry with enhanced accuracy 检测到的旋转对称性，具有更高的准确性 |
8.5 | [[8.5] 2503.20268 EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation](https://arxiv.org/abs/2503.20268) <br> [{'name': 'Ziran Zhang, Xiaohui Li, Yihao Liu, Yujin Wang, Yueting Chen, Tianfan Xue, Shi Guo'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video frame interpolation<br>event cameras<br>diffusion models | Input: Low-frame-rate RGB frames and event signals<br>Step1: Develop Multi-Modal Motion Condition Generator (MMCG) to integrate motion clues<br>Step2: Fine-tune stable video diffusion (SVD) model with conditions from MMCG<br>Step3: Evaluate generated frames for visual quality and fidelity<br>Output: Physically realistic intermediate video frames |
8.5 | [[8.5] 2503.20291 CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets](https://arxiv.org/abs/2503.20291) <br> [{'name': 'Chenwei Zhang, Anne Condon, Khanh Dao Duc'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D cryo-EM<br>protein structure<br>deep learning | Input: 3D cryo-EM density maps 3D 冷冻电子显微镜密度图<br>Step1: Integrate structural information with map features 集成结构信息与图像特征<br>Step2: Train multimodal U-Net on curated datasets 训练多模态U-Net模型<br>Step3: Evaluate performance across various metrics 评估各类指标下的性能<br>Output: Enhanced cryo-EM maps 改进的冷冻电子显微镜图像 |
8.5 | [[8.5] 2503.20321 Recovering Dynamic 3D Sketches from Videos](https://arxiv.org/abs/2503.20321) <br> [{'name': 'Jaeah Lee, Changwoon Choi, Young Min Kim, Jaesik Park'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dynamic sketches<br>motion analysis<br>video-based 3D reconstruction | Input: Video frames 视频帧<br>Step1: Extract 3D point cloud motion guidance 提取3D点云运动引导<br>Step2: Deform parametric 3D curves 变形参数化的3D曲线<br>Step3: Optimize motion guidance 优化运动引导<br>Output: Compact dynamic 3D sketches 输出紧凑的动态3D草图 |
8.5 | [[8.5] 2503.20652 Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification](https://arxiv.org/abs/2503.20652) <br> [{'name': 'Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D CT scans<br>anomaly classification<br>global-local attention | Input: 3D CT volumes 三维CT体积<br>Step1: Emulate scrolling behavior 模拟滚动行为<br>Step2: Global-local attention model development 全球-local注意力模型开发<br>Step3: Model evaluation on datasets 在数据集上评估模型<br>Output: Multi-label anomaly classification results 多标签异常分类结果 |
8.5 | [[8.5] 2503.20663 ARMO: Autoregressive Rigging for Multi-Category Objects](https://arxiv.org/abs/2503.20663) <br> [{'name': 'Mingze Sun, Shiwei Mao, Keyi Chen, Yurun Chen, Shunlin Lu, Jingbo Wang, Junting Dong, Ruqi Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D modeling 三维建模<br>rigging 装配<br>autoregressive models 自回归模型 | Input: 3D meshes 三维网格<br>Step1: Data integration 数据集成<br>Step2: Autoregressive model development 自回归模型开发<br>Step3: Skeleton prediction 骨骼预测<br>Output: Rigged 3D models 装配的三维模型 |
8.5 | [[8.5] 2503.20682 GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D Open-Vocabulary Detection](https://arxiv.org/abs/2503.20682) <br> [{'name': 'Xingyu Peng, Si Liu, Chen Gao, Yan Bai, Beipeng Mu, Xiaofei Wang, Huaxia Xia'}] | 3D Open-Vocabulary Detection 3D开集检测 | v2<br>3D Open-Vocabulary Detection<br>LiDAR<br>point clouds | Input: LiDAR point clouds LiDAR点云<br>Step1: Generate initial detection results 生成初步检测结果<br>Step2: Analyze scene context 分析场景上下文<br>Step3: Refine detection using common sense reasoning 精确利用常识推理修正检测结果<br>Step4: Apply balance schemes to improve class representation 应用平衡机制以改善类别表示<br>Output: Improved detection results with topic adaptability 输出: 改进的具有适应性的检测结果 |
8.5 | [[8.5] 2503.20746 PhysGen3D: Crafting a Miniature Interactive World from a Single Image](https://arxiv.org/abs/2503.20746) <br> [{'name': 'Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, Shenlong Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>interactive simulation<br>video generation | Input: Single image 单一图像<br>Step1: Estimate 3D shapes 估计三维形状<br>Step2: Compute physical and lighting properties 计算物理和光照属性<br>Step3: Generate interactive 3D scene 生成互动的三维场景<br>Output: Realistic video generation 真实视频生成 |
8.5 | [[8.5] 2503.20776 Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields](https://arxiv.org/abs/2503.20776) <br> [{'name': 'Shijie Zhou, Hui Ren, Yijia Weng, Shuwang Zhang, Zhen Wang, Dejia Xu, Zhiwen Fan, Suya You, Zhangyang Wang, Leonidas Guibas, Achuta Kadambi'}] | 4D Representation and Reconstruction 4D 表示与重建 | v2<br>4D representation 4D表示<br>Gaussian Splatting 高斯点云<br>Monocular Video 单目视频 | Input: Monocular video 单目视频<br>Step 1: Dynamic optimization 动态优化<br>Step 2: Gaussian feature field distillation 高斯特征场蒸馏<br>Step 3: 4D scene reconstruction 4D场景重建<br>Output: Interactive 4D agentic AI 交互式4D智能AI |
7.5 | [[7.5] 2503.20314 Wan: Open and Advanced Large-Scale Video Generative Models](https://arxiv.org/abs/2503.20314) <br> [{'name': 'WanTeam,  :, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, Ziyu Liu'}] | Image and Video Generation 图像与视频生成 | v2<br>video generation<br>generative models<br>diffusion models | Input: Large-scale images and videos 大规模图像和视频<br>Step1: Data curation 数据整理<br>Step2: Model design and optimization 模型设计与优化<br>Step3: Benchmarking and evaluation 基准测试与评估<br>Output: Advanced video generative models 高级视频生成模型 |


## Arxiv 2025-03-26

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.19332 Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D Gaussian Spatting](https://arxiv.org/abs/2503.19332) <br> [{'name': 'Zhiying Yan, Yiyuan Liang, Shilv Cai, Tao Zhang, Sheng Zhong, Luxin Yan, Xu Zou'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Dynamic Scene Reconstruction<br>Gaussian Splatting | Input: Dynamic scenes 动态场景<br>Step1: Data separation 数据分离<br>Step2: Hierarchical optimization 分层优化<br>Step3: Gaussian management 高斯管理<br>Output: Enhanced dynamic scene understanding 改进的动态场景理解 |
9.5 | [[9.5] 2503.19340 BADGR: Bundle Adjustment Diffusion Conditioned by GRadients for Wide-Baseline Floor Plan Reconstruction](https://arxiv.org/abs/2503.19340) <br> [{'name': 'Yuguang Li, Ivaylo Boyadzhiev, Zixuan Liu, Linda Shapiro, Alex Colburn'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>bundle adjustment<br>RGB panorama<br>layout generation | Input: Wide-baseline RGB panoramas 宽基线RGB全景图<br>Step1: Camera pose and floor plan initialization 相机位姿和平面布局初始化<br>Step2: Bundle adjustment and refinement 捆绑调整与优化<br>Step3: Integration of layout-structural constraints 布局结构约束的整合<br>Output: Accurate camera poses and floor plans 准确的相机位姿和楼层平面图 |
9.5 | [[9.5] 2503.19373 DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image](https://arxiv.org/abs/2503.19373) <br> [{'name': 'Hyeongjin Nam, Donghwan Kim, Jeongtaek Oh, Kyoung Mu Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human body reconstruction<br>cloth modeling | Input: Single image 单幅图像<br>Step1: Utilize 3D template models for regularization 利用三维模板模型进行正则化<br>Step2: Develop a specialized cloth diffusion model 开发专门的布料扩散模型<br>Step3: Reconstruct 3D cloth and human body based on templates 基于模板重建三维布料和人体<br>Output: Decomposed 3D model of cloth and human body 输出：分解的三维布料和人体模型 |
9.5 | [[9.5] 2503.19443 COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting](https://arxiv.org/abs/2503.19443) <br> [{'name': 'Jiaxin Zhang, Junjun Jiang, Youyu Chen, Kui Jiang, Xianming Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D segmentation<br>Gaussian splatting<br>visual quality<br>scene understanding<br>object boundaries | Input: Multi-view images 多视角图像<br>Step1: Joint optimization of semantics and visual information 联合优化语义与视觉信息<br>Step2: Boundary-adaptive Gaussian splitting technique 边界自适应高斯分裂技术<br>Step3: Texture restoration for visual quality texture restoration 视觉质量的纹理恢复<br>Output: Improved segmentation accuracy and clear boundaries 改进的分割精度和清晰边界 |
9.5 | [[9.5] 2503.19448 Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model](https://arxiv.org/abs/2503.19448) <br> [{'name': 'Changyong He, Jin Zeng, Jiawei Zhang, Jiajie Guo'}] | Depth Estimation 深度估计 | v2<br>Depth Denoising<br>Time-of-Flight<br>Diffusion Models<br>3D Reconstruction | Input: Raw correlation measurements from ToF sensors 从时间飞行传感器的原始相关测量开始<br>Step1: Dynamic range normalization 动态范围归一化<br>Step2: Apply diffusion model in denoising 应用扩散模型进行去噪<br>Step3: Confidence-aware guidance integration 集成基于置信度的指导<br>Output: Enhanced depth maps 改进的深度图 |
9.5 | [[9.5] 2503.19452 SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with Generative Priors](https://arxiv.org/abs/2503.19452) <br> [{'name': 'Yiqing Li, Xuan Wang, Jiawei Wu, Yikun Ma, Zhi Jin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>few-shot learning<br>novel view synthesis<br>occlusion handling | Input: Unconstrained in-the-wild images from various views 来自不同视角的野外图像<br>Step1: Multi-view stereo for camera parameters multi-view立体视觉技术获取相机参数<br>Step2: Gaussian optimization with Constrained Novel-View Enhancement 高斯优化与约束新视角增强模块结合<br>Step3: Occlusion handling to improve view consistency 处理遮挡以提高视角一致性<br>Output: High-quality novel views of the scene 该场景的高质量新视角 |
9.5 | [[9.5] 2503.19458 GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting](https://arxiv.org/abs/2503.19458) <br> [{'name': 'Shujuan Li, Yu-Shen Liu, Zhizhong Han'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>unsigned distance functions<br>multi-view images<br>3D Gaussian Splatting | Input: Multi-view images 多视角图像<br>Step1: Overfit 2D Gaussian planes on surfaces 在表面上过拟合2D高斯平面<br>Step2: Use self-supervision and gradient-based inference for UDF supervision 利用自监督和基于梯度的推理进行UDF监督<br>Step3: Produce continuous UDF representations 生成连续的UDF表示<br>Output: Accurate reconstruction of open surfaces 精确重建开放表面 |
9.5 | [[9.5] 2503.19543 Scene-agnostic Pose Regression for Visual Localization](https://arxiv.org/abs/2503.19543) <br> [{'name': 'Junwei Zheng, Ruiping Liu, Yufan Chen, Zhenfang Chen, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen'}] | Visual Odometry 视觉里程计 | v2<br>Pose Regression 姿态回归<br>Visual Localization 视觉定位<br>Camera Poses 相机姿态 | Input: Sequence of images along a trajectory 图像序列沿着轨迹<br>Step1: Model input preparation 模型输入准备<br>Step2: Pose prediction 相机姿态预测<br>Step3: Evaluation of pose accuracy 姿态精度评估<br>Output: Predictions of 6D camera poses 6D相机姿态预测 |
9.5 | [[9.5] 2503.19703 High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting](https://arxiv.org/abs/2503.19703) <br> [{'name': 'Qian Wang, Zhihao Zhan, Jialei He, Zhituo Tu, Xiang Zhu, Jie Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>digital orthophoto maps<br>2D Gaussian Splatting<br>depth estimation | Input: Multi-view images and terrain data 多视角图像及地形数据<br>Step1: Generate depth maps 生成深度图<br>Step2: Apply 2D Gaussian Splatting method 应用2D高斯点云方法<br>Step3: Render True Digital Orthophoto Maps (TDOMs) 渲染真正数字正交影像图(TDOMs)<br>Output: High-quality spatial reconstruction 高质量空间重建 |
9.5 | [[9.5] 2503.19776 Resilient Sensor Fusion under Adverse Sensor Failures via Multi-Modal Expert Fusion](https://arxiv.org/abs/2503.19776) <br> [{'name': 'Konyul Park, Yecheol Kim, Daehun Kim, Jun Won Choi'}] | Autonomous Systems and Robotics 自动驾驶与机器人 | v2<br>LiDAR<br>camera<br>sensor fusion<br>3D object detection<br>autonomous driving | Input: Multi-modal sensor data 多模态传感器数据<br>Step1: Integration of LiDAR and camera features LiDAR与相机特征的集成<br>Step2: Development of Multi-Expert Decoding framework 多专家解码框架的开发<br>Step3: Performance evaluation on benchmark 数据集上的性能评估<br>Output: Robust 3D object detection results 稳健的三维物体检测结果 |
9.5 | [[9.5] 2503.19912 SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data Pretraining](https://arxiv.org/abs/2503.19912) <br> [{'name': 'Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Qingshan Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR representation learning<br>autonomous driving<br>3D perception<br>spatiotemporal consistency | Input: Consecutive LiDAR-camera pairs LiDAR-相机配对<br>Step1: View consistency alignment view 一致性对齐<br>Step2: Dense-to-sparse consistency regularization 密集到稀疏一致性正则化<br>Step3: Flow-based contrastive learning 基于流的对比学习<br>Step4: Temporal voting strategy 时间投票策略<br>Output: Enhanced LiDAR-based perception 改进的基于LiDAR的感知 |
9.5 | [[9.5] 2503.19913 PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model](https://arxiv.org/abs/2503.19913) <br> [{'name': 'Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, Hao Zhao'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>4D reconstruction 四维重建<br>part-level dynamics 部分级动态<br>autonomous robotics 自主机器人 | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: 4D reconstruction framework development 四维重建框架开发<br>Step3: Motion and appearance learning 动作与外观学习<br>Output: Enhanced representations of part-level dynamics 改进的部分动态表示 |
9.5 | [[9.5] 2503.19914 Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models](https://arxiv.org/abs/2503.19914) <br> [{'name': 'Sangwon Beak, Hyeonwoo Kim, Hanbyul Joo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D spatial relationships<br>object-object relationships<br>diffusion models<br>synthetic 3D samples | Input: Synthesized 2D images 从合成的 2D 图像获取数据<br>Step1: Generate 3D samples from 2D images 从 2D 图像生成 3D 样本<br>Step2: Train score-based OOR diffusion model 训练基于分数的 OOR 扩散模型<br>Step3: Extend to multi-object OOR 扩展到多对象 OOR<br>Output: Distributions of spatial relationships 输出空间关系的分布 |
9.2 | [[9.2] 2503.19011 RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis](https://arxiv.org/abs/2503.19011) <br> [{'name': 'Yifei Feng, Mingxin Yang, Shuhui Yang, Sheng Zhang, Jiaao Yu, Zibo Zhao, Yuhong Liu, Jie Jiang, Chunchao Guo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D-aware texture generation 3D感知纹理生成<br>multi-view consistency 多视角一致性<br>texture synthesis 纹理合成 | Input: 3D geometries and multi-view images 3D几何体和多视角图像<br>Step1: Integrate multi-view image information 整合多视角图像信息<br>Step2: Develop a multi-attention texture synthesis network 开发多注意力纹理合成网络<br>Step3: Apply geometry-related Classifier-Free Guidance (CFG) 应用与几何相关的无分类器引导 (CFG)<br>Output: High-quality and consistent texture maps 输出: 高质量且一致的纹理图 |
9.0 | [[9.0] 2503.19207 FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images](https://arxiv.org/abs/2503.19207) <br> [{'name': 'Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human avatars<br>animation<br>feedforward<br>multi-frame aggregation | Input: Casual phone photos 从手机照片获取输入<br>Step1: 3D canonicalization 进行三维规范化<br>Step2: Multi-frame feature aggregation 多帧特征聚合<br>Step3: Avatar shape and animation inference 推断头像形状和动画<br>Output: Personalized 3D avatar 生成个性化的三维头像 |
8.5 | [[8.5] 2503.19157 HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models](https://arxiv.org/abs/2503.19157) <br> [{'name': 'Mingzhen Huang, Fu-Jen Chu, Bugra Tekin, Kevin J Liang, Haoyu Ma, Weiyao Wang, Xingyu Chen, Pierre Gleize, Hongfei Xue, Siwei Lyu, Kris Kitani, Matt Feiszli, Hao Tang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D hand-object interaction 3D手-物体交互<br>language models 语言模型 | Input: Text prompts or partial HOI sequences 文本提示或部分HOI序列<br>Step1: HOI sequence tokenization HOI序列的标记化<br>Step2: Bidirectional transformation between HOI sequences and text HOI序列与文本间的双向变换<br>Step3: HOI generation or completion HOI生成或补全<br>Output: Generated 3D hand-object interaction sequences 生成的3D手-物体交互序列 |
8.5 | [[8.5] 2503.19199 Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces](https://arxiv.org/abs/2503.19199) <br> [{'name': 'Chenyangguang Zhang, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, Francis Engelmann'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene graphs<br>functional relationships<br>RGB-D images | Input: Posed RGB-D images 从RGB-D图像输入<br>Step1: Predicting objects and interactive elements 预测物体和交互元素<br>Step2: Inferring functional relationships 推断功能关系<br>Output: Functional 3D scene graph 功能3D场景图 |
8.5 | [[8.5] 2503.19276 Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications](https://arxiv.org/abs/2503.19276) <br> [{'name': 'Ben Rahman'}] | Semantic Segmentation 语义分割 | v2<br>Semantic Segmentation<br>Large Language Models<br>Autonomous Driving<br>Context-Aware Systems | Input: Images with complex scenes 复杂场景中的图像<br>Step1: Integrate visual features and language embeddings 整合视觉特征和语言嵌入<br>Step2: Implement a Cross-Attention Mechanism 实现跨注意力机制<br>Step3: Utilize Graph Neural Networks for object relationships 使用图神经网络处理对象间的关系<br>Output: Enhanced pixel-level and contextual understanding 改进的像素级和上下文理解 |
8.5 | [[8.5] 2503.19307 Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation](https://arxiv.org/abs/2503.19307) <br> [{'name': 'Zhuoran Zhao, Linlin Yang, Pengzhan Sun, Pan Hui, Angela Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>hand pose estimation<br>synthetic data | Input: Synthetic and real data合成和真实数据<br>Step1: Synthetic data analysis分析合成数据<br>Step2: Gap analysis分析gap<br>Step3: Data synthesis pipeline proposal提出数据合成流程<br>Output: Enhanced hand pose estimation改进的手部姿态估计 |
8.5 | [[8.5] 2503.19308 A Comprehensive Analysis of Mamba for 3D Volumetric Medical Image Segmentation](https://arxiv.org/abs/2503.19308) <br> [{'name': 'Chaohan Wang, Yutong Xie, Qi Chen, Yuyin Zhou, Qi Wu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D segmentation 三维分割<br>medical imaging 医学成像<br>State Space Models 状态空间模型 | Input: High-resolution 3D medical images 高分辨率3D医学图像<br>Step1: Evaluate Mamba against Transformers 第一阶段：评估Mamba对比Transformers<br>Step2: Implement multi-scale representation learning 实现多尺度表征学习<br>Step3: Benchmark against public datasets 在公开数据集上进行基准测试<br>Output: Comparative analysis of segmentation performance 输出：分割性能比较分析 |
8.5 | [[8.5] 2503.19355 ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2503.19355) <br> [{'name': 'Dohwan Ko, Sihyeon Kim, Yumin Suh, Vijay Kumar B. G, Minseo Yoon, Manmohan Chandraker, Hyunwoo J. Kim'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>spatio-temporal reasoning<br>Vision-Language Models<br>autonomous driving | Input: Real-world videos with 3D annotations 实际视频与3D注释<br>Step1: Dataset construction 数据集构建<br>Step2: Kinematic instruction tuning 运动指令调优<br>Step3: Model training and evaluation 模型训练与评估<br>Output: Enhanced Vision-Language Model 改进的视觉语言模型 |
8.5 | [[8.5] 2503.19358 From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting](https://arxiv.org/abs/2503.19358) <br> [{'name': 'Zhiwei Huang, Hailin Yu, Yichun Shentu, Jin Yuan, Guofeng Zhang'}] | Camera Relocalization 相机重定位 | v2<br>camera relocalization<br>3D reconstruction<br>Gaussian splatting | Input: Query image 查询图像<br>Step1: Sparse feature extraction 稀疏特征提取<br>Step2: Initial pose estimation using sparse matching 根据稀疏匹配初步估计位姿<br>Step3: Dense feature matching for pose refinement 通过密集特征匹配进行位姿精炼<br>Output: Accurate camera pose result 精确的相机位姿结果 |
8.5 | [[8.5] 2503.19391 TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception](https://arxiv.org/abs/2503.19391) <br> [{'name': 'Zhiying Song, Lei Yang, Fuxi Wen, Jun Li'}] | Autonomous Systems and Robotics 自动驾驶与机器人系统 | v2<br>cooperative perception<br>trajectory alignment<br>autonomous driving<br>feature fusion | Input: Multi-frame LiDAR sequences 多帧激光雷达序列<br>Step1: Learning feature trajectories 学习特征轨迹<br>Step2: Generating attention points 生成注意力点<br>Step3: Aligning features against trajectories 将特征与轨迹对齐<br>Output: Enhanced cooperative perception 改进的协作感知 |
8.5 | [[8.5] 2503.19405 Multi-modal 3D Pose and Shape Estimation with Computed Tomography](https://arxiv.org/abs/2503.19405) <br> [{'name': 'Mingxiao Tu, Hoijoon Jung, Alireza Moghadam, Jineel Raythatha, Lachlan Allan, Jeremy Hsu, Andre Kyme, Jinman Kim'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D pose estimation 3D姿态估计<br>shape estimation 形状估计<br>computed tomography 计算机断层扫描<br>multi-modal fusion 多模态融合 | Input: Computed tomography (CT) scans and depth maps 计算机断层扫描(CT)和深度图<br>Step1: Feature extraction 特征提取<br>Step2: Probabilistic correspondence alignment 概率对应对齐<br>Step3: Pose and shape estimation 位置和形状估计<br>Step4: Parameter mixing model 参数混合模型<br>Output: Accurate 3D human mesh model 准确的三维人类网格模型 |
8.5 | [[8.5] 2503.19721 EventMamba: Enhancing Spatio-Temporal Locality with State Space Models for Event-Based Video Reconstruction](https://arxiv.org/abs/2503.19721) <br> [{'name': 'Chengjie Ge, Xueyang Fu, Peng He, Kunyu Wang, Chengzhi Cao, Zheng-Jun Zha'}] | Video Generation 视频生成 | v2<br>event-based video reconstruction<br>spatio-temporal locality<br>Mamba<br>computer vision<br>neural networks | Input: Event data 事件数据<br>Step1: Implement random window offset strategy 实施随机窗口偏移策略<br>Step2: Apply Hilbert space filling curve mechanism 应用希尔伯特空间填充曲线机制<br>Step3: Model evaluation and performance benchmarking 模型评估与性能基准测试<br>Output: Enhanced reconstructed video frames 改进的视频重建帧 |
8.5 | [[8.5] 2503.19755 ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation](https://arxiv.org/abs/2503.19755) <br> [{'name': 'Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>vision-language models<br>trajectory prediction | Input: Vision-language instructed action generation 视觉语言指令的动作生成<br>Step1: Combine QT-Former for temporal context aggregation 结合QT-Former进行时间上下文聚合<br>Step2: Utilize LLM for driving scenario reasoning 利用大型语言模型进行驾驶场景推理<br>Step3: Implement a generative planner for trajectory prediction 实施生成式规划器进行轨迹预测<br>Output: Enhanced closed-loop driving performance 改进的闭环驾驶性能 |
8.5 | [[8.5] 2503.19764 OpenLex3D: A New Evaluation Benchmark for Open-Vocabulary 3D Scene Representations](https://arxiv.org/abs/2503.19764) <br> [{'name': 'Christina Kassab, Sacha Morin, Martin B\\"uchner, Mat\\\'ias Mattamala, Kumaraditya Gupta, Abhinav Valada, Liam Paull, Maurice Fallon'}] | 3D Scene Representation 三维场景表示 | v2<br>3D scene representation<br>open-vocabulary<br>benchmark | Input: 3D scene representations 三维场景表示<br>Step1: Open-set category labeling 开放集类别标注<br>Step2: Benchmark dataset creation 基准数据集创建<br>Step3: Evaluation on semantic segmentation 语义分割评估<br>Step4: Evaluation on object retrieval 对象检索评估<br>Output: OpenLex3D benchmark dataset 开放Lex3D基准数据集 |
8.0 | [[8.0] 2503.19654 RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](https://arxiv.org/abs/2503.19654) <br> [{'name': 'Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models 视觉语言模型<br>RGB-Thermal understanding RGB-热成像理解 | Input: RGB-Thermal image pairs RGB-热成像对<br>Step1: Comprehensive evaluation framework 建立全面评估框架<br>Step2: Annotation of Yes/No questions 对是/否问题的标注<br>Step3: Performance evaluation on VLMs 对视觉语言模型的性能评估<br>Output: Benchmark for assessing VLMs 性能评估基准 |
8.0 | [[8.0] 2503.19794 PAVE: Patching and Adapting Video Large Language Models](https://arxiv.org/abs/2503.19794) <br> [{'name': 'Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>Video LLMs<br>3D reasoning<br>multimodal learning | Input: Pre-trained Video LLMs 和附加信号<br>Step1: 插入轻量级适配器以适应下游任务<br>Step2: 融合视频与其他信号<br>Step3: 评估模型在不同任务上的表现<br>Output: 改进的模型表现 |
7.5 | [[7.5] 2503.19325 Long-Context Autoregressive Video Modeling with Next-Frame Prediction](https://arxiv.org/abs/2503.19325) <br> [{'name': 'Yuchao Gu, Weijia Mao, Mike Zheng Shou'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>autoregressive modeling<br>temporal context | Input: Video data 视频数据<br>Step 1: Introduce FAR for video autoregressive modeling 引入FAR用于视频自回归建模<br>Step 2: Implement FlexRoPE for temporal decay 实现FlexRoPE以进行时间衰减<br>Step 3: Apply long short-term context modeling 应用长短期上下文建模<br>Output: State-of-the-art video generation 先进的视频生成 |
7.5 | [[7.5] 2503.19462 AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset](https://arxiv.org/abs/2503.19462) <br> [{'name': 'Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, Yu Qiao'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>diffusion models<br>synthetic dataset | Input: Pretrained video diffusion model 预训练视频扩散模型<br>Step 1: Generate synthetic dataset from denoising trajectories 从去噪轨迹生成合成数据集<br>Step 2: Design trajectory-based few-step guidance 设计基于轨迹的少步指导<br>Step 3: Implement adversarial training to align output distribution 实施对抗训练以对齐输出分布<br>Output: Accelerated video generation 加速视频生成 |
7.5 | [[7.5] 2503.19839 FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model](https://arxiv.org/abs/2503.19839) <br> [{'name': 'Jun Zhou, Jiahao Li, Zunnan Xu, Hanhui Li, Yiji Cheng, Fa-Ting Hong, Qin Lin, Qinglin Lu, Xiaodan Liang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>image editing<br>vision language models<br>fine-grained editing | Input: User editing instructions 用户编辑指令<br>Step1: Integrate region tokens 集成区域标记<br>Step2: Use VLM for comprehension 使用视觉语言模型进行理解<br>Step3: Apply diffusion model for editing 应用扩散模型进行编辑<br>Output: Edited images 生成的编辑图像 |
7.5 | [[7.5] 2503.19910 CoLLM: A Large Language Model for Composed Image Retrieval](https://arxiv.org/abs/2503.19910) <br> [{'name': 'Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul Chilimbi, Abhinav Shrivastava'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Composed Image Retrieval<br>Vision-Language Models<br>Large Language Models | Input: Image-caption pairs 图像-字幕对<br>Step1: Dynamic triplet synthesis 动态三元组合成<br>Step2: Model training 模型训练<br>Output: Enhanced composed image retrieval systems 改进的组合图像检索系统 |


## Arxiv 2025-03-25

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.17467 High Efficiency Wiener Filter-based Point Cloud Quality Enhancement for MPEG G-PCC](https://arxiv.org/abs/2503.17467) <br> [{'name': 'Yuxuan Wei, Zehan Wang, Tian Guo, Hao Liu, Liquan Shen, Hui Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud compression<br>Wiener filter<br>3D reconstruction 三维重建 | Input: Point clouds 点云<br>Step1: Introduce basic Wiener filter 基本维纳滤波器引入<br>Step2: Improve filter with coefficients inheritance and variance-based classification 改善滤波器，引入系数继承和基于方差的分类<br>Step3: Fast nearest neighbor search using Morton code 快速最近邻搜索，使用Morton编码<br>Output: Enhanced point cloud quality 改进的点云质量 |
9.5 | [[9.5] 2503.17486 ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes](https://arxiv.org/abs/2503.17486) <br> [{'name': 'Zhengqing Gao, Dongting Hu, Jia-Wang Bian, Huan Fu, Yan Li, Tongliang Liu, Mingming Gong, Kun Zhang'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>novel view synthesis<br>efficient rendering | Input: Gaussian primitives 高斯原语<br>Step1: Grouping Gaussians into prototypes 组合同类高斯点为原型<br>Step2: Clustering using K-means 使用K均值聚类<br>Step3: Joint optimization of anchor points and prototypes 对锚点和原型进行联合优化<br>Output: Efficient and high-quality rendering 高效且高质量的渲染 |
9.5 | [[9.5] 2503.17668 3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System](https://arxiv.org/abs/2503.17668) <br> [{'name': 'Usha Kumari, Shuvendu Rana'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Structure From Motion<br>camera movement<br>Affine SIFT | Input: Multi-view images 多视角图像<br>Step1: Extract matching points 提取匹配点<br>Step2: Camera rotation estimation 相机旋转估计<br>Step3: Translation estimation and correction 平移估计与修正<br>Output: Accurate 3D model creation 准确的三维模型生成 |
9.5 | [[9.5] 2503.17798 GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting](https://arxiv.org/abs/2503.17798) <br> [{'name': 'Zexu Huang, Min Xu, Stuart Perry'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>neural rendering<br>photo-realistic rendering | Input: 3D Gaussian representations 三维高斯表示<br>Step1: Patch attention algorithm application 局部关注算法应用<br>Step2: Gaussian constraints implementation 高斯约束实施<br>Step3: Subdivision strategy for large scenes 大场景分割策略<br>Output: Enhanced rendering quality 改进的渲染质量 |
9.5 | [[9.5] 2503.17814 LightLoc: Learning Outdoor LiDAR Localization at Light Speed](https://arxiv.org/abs/2503.17814) <br> [{'name': 'Wen Li, Chen Liu, Shangshu Yu, Dunqiang Liu, Yin Zhou, Siqi Shen, Chenglu Wen, Cheng Wang'}] | Autonomous Driving 自动驾驶 | v2<br>LiDAR localization<br>SLAM<br>autonomous driving | Input: LiDAR data 和 LiDAR 数据<br>Step1: Sample classification guidance 样本分类指导<br>Step2: Redundant sample downsampling 冗余样本下采样<br>Step3: Integration into SLAM and model evaluation 集成到SLAM并进行模型评估<br>Output: Fast-trainable localization model 快速可训练的定位模型 |
9.5 | [[9.5] 2503.17856 ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling](https://arxiv.org/abs/2503.17856) <br> [{'name': 'Radu Beche, Sergiu Nedevschi'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>aerial imagery<br>dataset creation<br>scene complexity | Input: Aerial imagery from UAV 多视角无人机图像<br>Step1: Dataset creation 数据集创建<br>Step2: Scene complexity profiling 场景复杂度分析<br>Step3: Benchmarking reconstruction methods 重建方法基准测试<br>Output: High-quality synthetic dataset 高质量合成数据集 |
9.5 | [[9.5] 2503.17973 PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos](https://arxiv.org/abs/2503.17973) <br> [{'name': 'Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>physics-informed models<br>robotic motion planning<br>deformable objects<br>real-time simulation | Input: Sparse videos of deformable objects 变形物体的稀疏视频<br>Step1: Develop physics-informed representation 发展物理信息表示<br>Step2: Integrate inverse modeling framework 整合反向建模框架<br>Step3: Optimize geometry and physical properties 优化几何和物理属性<br>Output: Interactive digital twin interactive digital twin |
9.5 | [[9.5] 2503.18007 SymmCompletion: High-Fidelity and High-Consistency Point Cloud Completion with Symmetry Guidance](https://arxiv.org/abs/2503.18007) <br> [{'name': 'Hongyu Yan, Zijun Li, Kunming Luo, Li Lu, Ping Tan'}] | Point Cloud Processing 点云处理 | v2<br>Point cloud completion<br>3D reconstruction<br>symmetry guidance | Input: Partial point clouds<br>Step1: Local Symmetry Transformation Network (LSTNet) estimates point-wise local symmetry transformations.<br>Step2: Generate geometry-aligned partial-missing pairs and initial point clouds.<br>Step3: Symmetry-Guidance Transformer (SGFormer) refines the initial point clouds using geometric features.<br>Output: High-fidelity and geometry-consistency final point clouds. |
9.5 | [[9.5] 2503.18100 M3Net: Multimodal Multi-task Learning for 3D Detection, Segmentation, and Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2503.18100) <br> [{'name': 'Xuesong Chen, Shaoshuai Shi, Tao Ma, Jingqiu Zhou, Simon See, Ka Chun Cheung, Hongsheng Li'}] | 3D Detection 三维检测 | v2<br>3D detection 三维检测<br>autonomous driving 自动驾驶<br>multi-task learning 多任务学习 | Input: Multimodal data from sensors and cameras 多模态传感器和相机数据<br>Step1: Feature extraction from images and LiDAR features 从图像和LiDAR特征提取<br>Step2: Modality-Adaptive Feature Integration (MAFI) module implementation 实现模态自适应特征集成（MAFI）模块<br>Step3: Task-specific query initialization for detection and segmentation 目标检测和分割的任务特定查询初始化<br>Step4: Shared BEV features transformation through multi-layer decoders 共享BEV特征的多层解码器变换<br>Output: Enhanced detection, segmentation, and occupancy prediction results 改进的检测、分割和占用预测结果 |
9.5 | [[9.5] 2503.18135 MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation](https://arxiv.org/abs/2503.18135) <br> [{'name': 'Jiaxin Huang, Runnan Chen, Ziwen Li, Zhengqing Gao, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu'}] | 3D Reasoning Segmentation 3D推理分割 | v2<br>3D reasoning segmentation<br>multimodal learning<br>user intent | Input: Multi-view images and text queries 多视角图像和文本查询<br>Step1: Generate multi-view pseudo segmentation masks 生成多视角伪分割掩模<br>Step2: Unproject 2D masks into 3D space 将2D掩模投影到3D空间<br>Step3: Align masks with text embeddings 将掩模与文本嵌入对齐<br>Step4: Implement spatial consistency strategy 实施空间一致性策略<br>Output: Coherent 3D segmentation masks 输出一致的3D分割掩模 |
9.5 | [[9.5] 2503.18361 NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction](https://arxiv.org/abs/2503.18361) <br> [{'name': 'Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neural radiance fields<br>multi-view consistency<br>surface reconstruction<br>signed distance function | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Learn neural radiance fields using volume rendering 学习使用体积渲染的神经辐射场<br>Step2: Impose multi-view consistency constraint 强加多视角一致性约束<br>Step3: Infer signed distance fields (SDF) 推断有符号距离场<br>Step4: Evaluate surface reconstruction against benchmarks 评估表面重建结果 |
9.5 | [[9.5] 2503.18363 MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction](https://arxiv.org/abs/2503.18363) <br> [{'name': 'Wenyuan Zhang, Yixiao Yang, Han Huang, Liang Han, Kanle Shi, Yu-Shen Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neural rendering<br>monocular depth<br>multi-view<br>uncertainty | Input: Multi-view images 多视角图像<br>Step1: Segment multi-view images into consistent instances 将多视角图像分割为一致的实例<br>Step2: Back-project and align estimated depth values 将估计的深度值反投影并对齐<br>Step3: Evaluate point density to measure uncertainty 评估点密度以测量不确定性<br>Output: Uncertainty maps and enhanced geometric priors 不确定性图和增强几何先验 |
9.5 | [[9.5] 2503.18368 MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning](https://arxiv.org/abs/2503.18368) <br> [{'name': 'Xu Han, Yuan Tang, Jinfeng Xu, Xianzhi Li'}] | 3D Representation Learning 3D表示学习 | v2<br>3D representation learning<br>parameter-efficient fine-tuning<br>point clouds | Input: 3D point clouds 3D点云<br>Step1: Parameter-efficient fine-tuning using structured matrices 使用结构化矩阵进行参数高效微调<br>Step2: Model training and evaluation 模型训练与评估<br>Output: Enhanced representation for 3D tasks 改进的3D任务表示 |
9.5 | [[9.5] 2503.18402 DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds](https://arxiv.org/abs/2503.18402) <br> [{'name': 'Youyu Chen, Junjun Jiang, Kui Jiang, Xiao Tang, Zhihao Li, Xianming Liu, Yinyu Nie'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting 3D高斯点云<br>optimization optimization<br>rendering rendering | Input: 3D scenes 3D场景<br>Step1: Optimization complexity analysis 优化复杂度分析<br>Step2: Scheduling rendering resolution 渲染分辨率调度<br>Step3: Adaptive primitive growth primitives 自适应原始增长<br>Output: Accelerated 3D Gaussian Splatting model 加速的3D高斯点云模型 |
9.5 | [[9.5] 2503.18438 ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation](https://arxiv.org/abs/2503.18438) <br> [{'name': 'Guosheng Zhao, Xiaofeng Wang, Chaojun Ni, Zheng Zhu, Wenkang Qin, Guan Huang, Xingang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Domain gap mitigation 域间隙缓解<br>Step2: Spatial deformation learning 空间变形学习<br>Step3: 3D Gaussian modeling 三维高斯建模<br>Output: Improved driving scene representation 改进的驾驶场景表示 |
9.5 | [[9.5] 2503.18458 StableGS: A Floater-Free Framework for 3D Gaussian Splatting](https://arxiv.org/abs/2503.18458) <br> [{'name': 'Luchao Wang, Qian Ren, Kaiming He, Hua Wang, Zhi Chen, Yaohua Tang'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>novel view synthesis<br>floater artifacts | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Analyze gradient vanishing gradient消失分析<br>Step2: Develop cross-view depth consistency constraints 开发视图间深度一致性约束<br>Step3: Integrate a dual-opacity model 集成双透明度模型<br>Output: Enhanced novel view synthesis results 改进的新的视图合成结果 |
9.5 | [[9.5] 2503.18461 MuMA: 3D PBR Texturing via Multi-Channel Multi-View Generation and Agentic Post-Processing](https://arxiv.org/abs/2503.18461) <br> [{'name': 'Lingting Zhu, Jingrui Ye, Runze Zhang, Zeyu Hu, Yingda Yin, Lanjiong Li, Jinnan Chen, Shengju Qian, Xin Wang, Qingmin Liao, Lequan Yu'}] | 3D Generation 三维生成 | v2<br>3D PBR Texturing<br>Multi-Channel Generation<br>Agentic Post-Processing | Input: Untextured mesh and user inputs 未纹理化网格与用户输入<br>Step1: Multi-channel multi-view generation 多通道多视角生成<br>Step2: Agentic post-processing 代理后处理<br>Output: High-fidelity PBR textures 高保真物理基础渲染纹理 |
9.5 | [[9.5] 2503.18476 Global-Local Tree Search for Language Guided 3D Scene Generation](https://arxiv.org/abs/2503.18476) <br> [{'name': 'Wei Deng, Mengshi Qi, Huadong Ma'}] | 3D Scene Generation 3D场景生成 | v2<br>3D indoor scene generation<br>Vision-Language Models (VLMs)<br>tree search algorithm | Input: User-provided scene descriptions 用户提供的场景描述<br>Step1: Hierarchical scene representation construction 层次场景表示构建<br>Step2: Global-local tree search algorithm application 全局-局部树搜索算法应用<br>Step3: Object placement using VLM object recognition 使用VLM对象识别进行物体放置<br>Output: Realistic 3D indoor scenes 真实的室内3D场景 |
9.5 | [[9.5] 2503.18527 AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction](https://arxiv.org/abs/2503.18527) <br> [{'name': 'Soulaimene Turki, Daniel Panangian, Houda Chaabouni-Chouayakh, Ksenia Bittner'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>point cloud<br>building reconstruction<br>aerial image | Input: Single aerial image 单幅航空图像<br>Step1: Feature extraction 特征提取<br>Step2: Concatenate additional conditions 连接附加条件<br>Step3: Point cloud diffusion modeling 点云扩散建模<br>Output: Complete 3D building point cloud 生成完整的三维建筑点云 |
9.5 | [[9.5] 2503.18557 LeanStereo: A Leaner Backbone based Stereo Network](https://arxiv.org/abs/2503.18557) <br> [{'name': 'Rafia Rahim, Samuel Woerz, Andreas Zell'}] | Stereo Matching 立体匹配 | v2<br>Stereo Matching<br>Depth Estimation<br>3D Reconstruction | Input: Rectified stereo images 经过校正的立体图像<br>Step1: Feature extraction 特征提取<br>Step2: Cost volume integration 成本体积集成<br>Step3: Disparity regression 视差回归<br>Output: Depth map 深度图 |
9.5 | [[9.5] 2503.18640 LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment](https://arxiv.org/abs/2503.18640) <br> [{'name': 'Haoran Wang, Jingwei Huang, Lu Yang, Tianchen Deng, Gaojing Zhang, Mingrui Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>multi-view optimization<br>low-light enhancement | Input: Low-light images 低光照图像<br>Step1: Gaussian representation decomposition 高斯表示分解<br>Step2: Image enhancement enhancement 图像增强<br>Step3: Multi-view consistency optimization 多视角一致性优化<br>Output: Enhanced 3D models 改进的三维模型 |
9.5 | [[9.5] 2503.18671 Structure-Aware Correspondence Learning for Relative Pose Estimation](https://arxiv.org/abs/2503.18671) <br> [{'name': 'Yihan Chen, Wenfei Yang, Huan Ren, Shifeng Zhang, Tianzhu Zhang, Feng Wu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Relative Pose Estimation 相对姿态估计<br>3D Correspondences 3D对应<br>Keypoint Extraction 关键点提取<br>Structure-Aware 学习结构感知 | Input: Query and reference images 图像输入<br>Step1: Structure-aware keypoint extraction module 结构感知关键点提取模块<br>Step2: Structure-aware correspondence estimation module 结构感知对应估计模块<br>Step3: 3D-3D correspondence establishment 3D-3D对应建立<br>Output: Estimated relative pose 估计的相对姿态 |
9.5 | [[9.5] 2503.18682 Hardware-Rasterized Ray-Based Gaussian Splatting](https://arxiv.org/abs/2503.18682) <br> [{'name': 'Samuel Rota Bul\\`o, Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>ray-based rendering<br>virtual reality | Input: 3D Gaussian primitives 3D 高斯原语<br>Step1: Mathematical derivation 数学推导<br>Step2: Efficient rendering techniques 高效渲染技术<br>Step3: Performance evaluation 性能评估<br>Output: High-quality rendering output 高质量渲染输出 |
9.5 | [[9.5] 2503.18794 NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting](https://arxiv.org/abs/2503.18794) <br> [{'name': 'Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>sparse view synthesis<br>Neural Radiance Fields<br>3D Gaussian Splatting | Input: Sparse-view images 稀疏视图图像<br>Step 1: Depth computation using optical flow and camera poses 使用光流和相机姿态进行深度计算<br>Step 2: Point cloud densification 点云密化<br>Step 3: Model evaluation and comparison 模型评估与比较<br>Output: Enhanced novel view synthesis 输出：改进的视图合成 |
9.5 | [[9.5] 2503.18897 Online 3D Scene Reconstruction Using Neural Object Priors](https://arxiv.org/abs/2503.18897) <br> [{'name': 'Thomas Chabal, Shizhe Chen, Jean Ponce, Cordelia Schmid'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neural implicit representations | Input: RGB-D video sequence  RGB-D视频序列<br>Step1: Extract object masks and camera poses 提取物体掩模和相机姿态<br>Step2: Continuous optimization of object representation 对物体表示进行连续优化<br>Step3: Utilize shape priors from object library 利用物体库中的形状先验<br>Output: Online reconstructed 3D scene 在线重建的3D场景 |
9.5 | [[9.5] 2503.18945 Aether: Geometric-Aware Unified World Modeling](https://arxiv.org/abs/2503.18945) <br> [{'name': 'Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>4D reconstruction<br>autonomous systems | Input: Synthetic 4D video data 合成的4D视频数据<br>Step1: Data annotation data annotation 数据标注<br>Step2: Multi-task optimization 多任务优化<br>Step3: Model training and evaluation 模型训练与评估<br>Output: Unified world model with geometric reasoning 具有几何推理的统一世界模型 |
9.2 | [[9.2] 2503.17712 Multi-modality Anomaly Segmentation on the Road](https://arxiv.org/abs/2503.17712) <br> [{'name': 'Heng Gao, Zhuolin He, Shoumeng Qiu, Xiangyang Xue, Jian Pu'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>anomaly segmentation<br>autonomous driving<br>multi-modal | Input: Road images with anomalies 具有异常的路面图像<br>Step1: Text-modal extraction using CLIP 通过CLIP提取文本模态<br>Step2: Anomaly score computation 计算异常得分<br>Step3: Ensemble boosting of scores 加权平均多个得分<br>Output: Anomaly segmentation map 异常 сегментация图 |
9.2 | [[9.2] 2503.18052 SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining](https://arxiv.org/abs/2503.18052) <br> [{'name': 'Yue Li, Qi Ma, Runyi Yang, Huapeng Li, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Martin R. Oswald, Danda Pani Paudel'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>3D scene understanding<br>self-supervised learning | Input: 3D Gaussian Splatting (3DGS) data 3D高斯点云数据<br>Step 1: Dataset creation (SceneSplat-7K) 数据集创建（SceneSplat-7K）<br>Step 2: Model training with vision-language pretraining 通过视觉语言预训练训练模型<br>Step 3: Evaluate performance on segmentation benchmarks 在分割基准上评估性能<br>Output: Enhanced understanding of 3D scenes 改进的3D场景理解 |
9.2 | [[9.2] 2503.18107 PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding](https://arxiv.org/abs/2503.18107) <br> [{'name': 'Hongjia Zhai, Hai Li, Zhenzhe Li, Xiaokun Pan, Yijia He, Guofeng Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D panoptic segmentation<br>3D Gaussian Splatting<br>scene understanding<br>language-guided segmentation | Input: Multi-view posed images 多视角图像<br>Step1: Model continuous parametric feature space 建模连续参数特征空间<br>Step2: Use 3D feature decoder 采用三维特征解码器<br>Step3: Perform graph clustering based segmentation 进行基于图聚类的分割<br>Output: 3D consistent instance segmentation 三维一致实例分割 |
9.2 | [[9.2] 2503.18155 Decorum: A Language-Based Approach For Style-Conditioned Synthesis of Indoor 3D Scenes](https://arxiv.org/abs/2503.18155) <br> [{'name': 'Kelly O. Marshall, Omid Poursaeed, Sergiu Oprea, Amit Kumar, Anushrut Jignasu, Chinmay Hegde, Yilei Li, Rakesh Ranjan'}] | 3D Generation 三维生成 | v2<br>3D scene generation<br>natural language processing<br>multimodal learning | Input: User-generated prompts 用户生成的提示<br>Step 1: Text to dense annotation text 转为密集注释<br>Step 2: Layout design for objects 设计对象布局<br>Step 3: Furniture selection from inventory 从库存中选择家具<br>Output: Structured 3D indoor scenes 输出结构化三维室内场景 |
9.2 | [[9.2] 2503.18254 Surface-Aware Distilled 3D Semantic Features](https://arxiv.org/abs/2503.18254) <br> [{'name': 'Lukas Uzolas, Elmar Eisemann, Petr Kellnhofer'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic features | Input: Training with unpaired 3D meshes 使用无配对3D网格进行训练<br>Step1: Learning a surface-aware embedding space 学习表面感知嵌入空间<br>Step2: Implementing a contrastive loss to improve feature distinction 实施对比损失以提高特征区分<br>Output: Robust 3D features for various applications 输出: 适用于多种应用的稳健3D特征 |
9.0 | [[9.0] 2503.17574 Is there anything left? Measuring semantic residuals of objects removed from 3D Gaussian Splatting](https://arxiv.org/abs/2503.17574) <br> [{'name': 'Simona Kocour, Assia Benbihi, Aikaterini Adam, Torsten Sattler'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic residuals<br>object removal<br>privacy-preserving mapping<br>3D Gaussian Splatting | Input: 3D scenes with objects 3D场景与对象<br>Step1: Evaluation of object removal methods 对对象删除方法进行评估<br>Step2: Measurement of semantic residuals 语义残余的测量<br>Step3: Refinement of object removal results 根据空间和语义一致性优化删除结果<br>Output: Evaluated removal quality and refined scenes 评估删除质量和优化后场景 |
9.0 | [[9.0] 2503.18083 Unified Geometry and Color Compression Framework for Point Clouds via Generative Diffusion Priors](https://arxiv.org/abs/2503.18083) <br> [{'name': 'Tianxin Huang, Gim Hee Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud compression<br>generative diffusion models<br>3D modeling<br>autonomous driving | Input: 3D point clouds with color attributes 具有颜色属性的3D点云<br>Step1: Adaptation of pre-trained generative diffusion model 适应预训练生成扩散模型<br>Step2: Compression using prompt tuning 使用提示调优进行压缩<br>Step3: Data encoding into sparse sets 将数据编码为稀疏集合<br>Step4: Decompression through denoising steps 通过去噪步骤进行解压缩<br>Output: Compressed and decompressed point clouds 压缩和解压缩的点云 |
9.0 | [[9.0] 2503.18944 DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation](https://arxiv.org/abs/2503.18944) <br> [{'name': 'Karim Abou Zeid, Kadir Yilmaz, Daan de Geus, Alexander Hermans, David Adrian, Timm Linder, Bastian Leibe'}] | 3D Segmentation 3D分割 | v2<br>3D segmentation 3D分割<br>2D foundation models 2D基础模型<br>semantic segmentation 语义分割 | Input: 2D foundation model features 2D基础模型特征<br>Step1: Feature extraction 特征提取<br>Step2: 2D to 3D projection 2D到3D投影<br>Step3: Integration into 3D segmentation model 集成到3D分割模型中<br>Output: Enhanced 3D segmentation performance 改进的3D分割性能 |
8.5 | [[8.5] 2503.17406 IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes](https://arxiv.org/abs/2503.17406) <br> [{'name': 'Haochen Zhang, Nader Zantout, Pujith Kachana, Ji Zhang, Wenshan Wang'}] | 3D Scene Understanding 3D 场景理解 | v2<br>3D scenes<br>referential grounding<br>benchmark dataset<br>multimodal integration<br>interactive navigation | Input: 3D scanned rooms 3D 扫描房间<br>Step1: Dataset curation 数据集策划<br>Step2: Model evaluation 模型评估<br>Step3: Baseline development 基线开发<br>Output: Resource for interactive navigation systems 为交互导航系统提供资源 |
8.5 | [[8.5] 2503.17415 Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)](https://arxiv.org/abs/2503.17415) <br> [{'name': 'Yicheng Duan, Xi Huang, Duo Chen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs)<br>Video Retrieval 视频检索<br>Contextual Relationships 上下文关系 | Input: Video segments 视频段<br>Step1: Embed video frames using VLM 使用视觉语言模型(VLM)嵌入视频框架<br>Step2: Combine embeddings with contextual metadata 结合嵌入与上下文元数据<br>Step3: Implement vector similarity search with graph structures 实现与图结构的向量相似性搜索<br>Output: Refined video retrieval results 精细化的视频检索结果 |
8.5 | [[8.5] 2503.17499 Event-Based Crossing Dataset (EBCD)](https://arxiv.org/abs/2503.17499) <br> [{'name': "Joey Mul\\'e, Dhandeep Challagundla, Rachit Saini, Riadul Islam"}] | Event-based Vision 事件视觉 | v2<br>Event-based vision 事件视觉<br>object detection 目标检测<br>autonomous systems 自主系统 | Input: Event-based images 事件图像<br>Step1: Data capture using multi-thresholding 多阈值数据捕获<br>Step2: Object detection using CNNs 使用卷积神经网络进行目标检测<br>Step3: Performance evaluation against traditional datasets 性能评估与传统数据集对比<br>Output: Enhanced dataset for event-based detection 改进的事件检测数据集 |
8.5 | [[8.5] 2503.17539 Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks](https://arxiv.org/abs/2503.17539) <br> [{'name': 'Bhishma Dedhia, David Bourgin, Krishna Kumar Singh, Yuheng Li, Yan Kang, Zhan Xu, Niraj K. Jha, Yuchen Liu'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>parallel inference<br>Diffusion Transformers<br>temporal consistency | Input: Short videos 短视频<br>Step1: Encoding video chunks 编码视频块<br>Step2: Parallel inference of video segments 视频段落的并行推理<br>Step3: Denoising video chunks 去噪视频块<br>Output: Long photorealistic videos 长的照相真实视频 |
8.5 | [[8.5] 2503.17695 MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion](https://arxiv.org/abs/2503.17695) <br> [{'name': 'Yikun Ma, Yiqing Li, Jiawei Wu, Zhi Jin'}] | Multi-view and Stereo Vision 多视角立体视觉 | v2<br>motion editing<br>multi-view consistency<br>generative models<br>optical flow | Input: Static scene and user-selected motion priors 静态场景和用户选择的运动先验<br>Step1: Multi-view Flow Estimation Stage (MFES) 多视角流估计阶段<br>Step2: Point Kinematic Model (PKM) to estimate optical flows 使用点运动模型估计光流<br>Step3: Multi-view Motion Diffusion Stage (MMDS) to generate motion results 多视角运动扩散阶段生成运动结果<br>Output: Consistent multi-view motion results 一致的多视角运动结果 |
8.5 | [[8.5] 2503.17752 HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving](https://arxiv.org/abs/2503.17752) <br> [{'name': 'R. D. Lin, Pengcheng Weng, Yinqiao Wang, Han Ding, Jinsong Han, Fei Wang'}] | LiDAR Segmentation 激光雷达分割 | v2<br>LiDAR segmentation<br>autonomous driving<br>semi-supervised learning | Input: Continuous LiDAR frames 连续的激光雷达帧<br>Step1: Learn high and low temporal sensitivity representations 学习高低时间敏感性表示<br>Step2: Enhance representations使用交叉注意力机制增强表示<br>Step3: Teacher-student framework alignment 在标签和未标签分支上对齐表示<br>Output: Segmentation results based on LiDAR frames 基于激光雷达帧的分割结果 |
8.5 | [[8.5] 2503.17788 Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction](https://arxiv.org/abs/2503.17788) <br> [{'name': 'Gaoge Han, Yongkang Cheng, Zhe Chen, Shaoli Huang, Tongliang Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D hand reconstruction<br>occlusion handling<br>multimodal prior integration<br>diffusion models<br>fusion alignment | Input: Monocular images 单目图像<br>Step1: Learn to align fused multimodal priors (keypoints, segmentation maps, depth cues) from foundation models during training 训练期间学习对齐融合的多模态先验（关键点、分割图、深度线索）<br>Step2: Employ a two-hand diffusion model to correct interpenetration artifacts 应用双手扩散模型以修正穿透伪影<br>Output: Occlusion-resistant two-hand reconstruction 具抗遮挡能力的双手重建 |
8.5 | [[8.5] 2503.17938 Selecting and Pruning: A Differentiable Causal Sequentialized State-Space Model for Two-View Correspondence Learning](https://arxiv.org/abs/2503.17938) <br> [{'name': 'Xiang Fang, Shihua Zhang, Hao Zhang, Tao Lu, Huabing Zhou, Jiayi Ma'}] | Multi-view and Stereo Vision 多视角和立体视觉 | v2<br>correspondence learning<br>two-view matching<br>SFM<br>pose estimation | Input: Two-view image pairs 两幅图像对<br>Step1: Develop correspondence filter 研发对应过滤器<br>Step2: Implement causal sequence learning 实现因果序列学习<br>Step3: Integrate local-context enhancement module 集成局部上下文增强模块<br>Step4: Evaluate performance on relative pose estimation 评估相对姿态估计的性能<br>Output: Enhanced matching accuracy 提升匹配精度 |
8.5 | [[8.5] 2503.17982 Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images](https://arxiv.org/abs/2503.17982) <br> [{'name': 'Yara AlaaEldin, Francesca Odone'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>semantic segmentation<br>aerial images<br>autonomous navigation | Input: Aerial images from monocular cameras 单目相机的航拍图像<br>Step1: Joint architecture design 结构设计<br>Step2: Depth estimation map prediction 深度估计图的预测<br>Step3: Semantic segmentation map prediction 语义分割图的预测<br>Output: Depth and semantic segmentation maps 深度和语义分割图 |
8.5 | [[8.5] 2503.17992 Geometric Constrained Non-Line-of-Sight Imaging](https://arxiv.org/abs/2503.17992) <br> [{'name': 'Xueying Liu, Lianfang Wang, Jun Liu, Yong Wang, Yuping Duan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Non-line-of-sight imaging<br>3D reconstruction<br>surface normal<br>geometric constraint | Input: Non-line-of-sight (NLOS) data 近眼不可见数据<br>Step1: Joint estimation of normals and albedo 法线与反照率的联合估计<br>Step2: Apply Frobenius norm regularization 应用弗罗贝纽斯范数正则化<br>Step3: High-precision surface reconstruction 提高准确性的表面重建<br>Output: Accurate geometry of hidden objects 隐藏物体的准确几何形状 |
8.5 | [[8.5] 2503.18016 Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook](https://arxiv.org/abs/2503.18016) <br> [{'name': 'Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Lutao Jiang, Haiwei Xue, Bin Ren, Danda Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Retrieval-Augmented Generation<br>computer vision<br>3D generation | Input: A comprehensive overview of retrieval-augmented generation techniques in computer vision 计算机视觉中的检索增强生成技术概述<br>Step1: Review of visual understanding tasks 视觉理解任务评估<br>Step2: Examination of visual generation applications 视觉生成应用调查<br>Step3: Proposal of future research directions 提出未来研究方向<br>Output: Insights into RAG applications in 3D generation and embodied AI 3D生成和实体AI中的RAG应用见解 |
8.5 | [[8.5] 2503.18073 PanopticSplatting: End-to-End Panoptic Gaussian Splatting](https://arxiv.org/abs/2503.18073) <br> [{'name': 'Yuxuan Xie, Xuan Yu, Changjian Jiang, Sitong Mao, Shunbo Zhou, Rui Fan, Rong Xiong, Yue Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian splatting | Input: Multi-view images 多视角图像<br>Step1: Gaussian segmentation 高斯分割<br>Step2: Label blending 标签混合<br>Step3: Cross attention mechanism 交叉注意机制<br>Output: Consistent 3D panoptic segments 一致的三维全景分段 |
8.5 | [[8.5] 2503.18177 Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles](https://arxiv.org/abs/2503.18177) <br> [{'name': 'Gulnaz Gimaletdinova, Dim Shaiakhmetov, Madina Akpaeva, Mukhammadmuso Abduzhabbarov, Kadyrmamat Momunov'}] | Robotic Perception 机器人感知 | v2<br>traffic sign recognition<br>partially occlusion<br>autonomous vehicles<br>CNN | Input: Dataset of road sign images with occlusions 道路标志图像数据集（包含遮挡）<br>Step1: Data collection 数据收集<br>Step2: Model training using CNN models 模型训练使用卷积神经网络<br>Step3: Comparison of models against transfer learning models 模型与迁移学习模型比较<br>Output: Performance metrics of models 模型性能指标 |
8.5 | [[8.5] 2503.18283 Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context](https://arxiv.org/abs/2503.18283) <br> [{'name': 'Bojun Liu, Yangzhi Ma, Ao Luo, Li Li, Dong Liu'}] | Point Cloud Processing 点云处理 | v2<br>Point cloud compression 点云压缩<br>Sparse convolution 稀疏卷积 | Input: Point cloud data 点云数据<br>Step1: Context model development 上下文模型开发<br>Step2: Geometry residual coding development 几何残差编码开发<br>Step3: Performance evaluation 性能评估<br>Output: Compressed point cloud representation 压缩点云表示 |
8.5 | [[8.5] 2503.18328 TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering](https://arxiv.org/abs/2503.18328) <br> [{'name': 'Chun Gu, Xiaofei Wei, Li Zhang, Xiatian Zhu'}] | Inverse Rendering 逆向渲染 | v2<br>inverse rendering<br>importance sampling<br>3D reconstruction<br>multi-view images | Input: Multi-view images 多视角图像<br>Step1: Importance sampling 重要性采样<br>Step2: Sampler learning sampler 学习采样器<br>Step3: Scene representation scene representation 现场表示<br>Output: Enhanced rendering outputs 改进的渲染输出 |
8.5 | [[8.5] 2503.18341 PS-EIP: Robust Photometric Stereo Based on Event Interval Profile](https://arxiv.org/abs/2503.18341) <br> [{'name': 'Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, Tsuyoshi Takatani'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Photometric Stereo 光度立体<br>Event Camera 事件摄像机<br>3D Reconstruction 三维重建 | Input: Event data from an event camera 事件摄像头的数据<br>Step1: Formulate event interval profiles 形成事件间隔剖面<br>Step2: Introduce outlier detection based on profile shape 引入基于剖面形状的异常值检测<br>Step3: Estimate surface normals using the derived profiles 使用推导的剖面估计表面法线<br>Output: Robustly estimated surface normals 可靠的表面法线估计 |
8.5 | [[8.5] 2503.18384 LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and Perspectives](https://arxiv.org/abs/2503.18384) <br> [{'name': 'Yuan Gao, Shaobo Xia, Pu Wang, Xiaohuan Xi, Sheng Nie, Cheng Wang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>LiDAR remote sensing<br>weakly supervised learning<br>3D reconstruction<br>point clouds | Input: LiDAR data and annotations LiDAR数据和注释<br>Step1: Review of LiDAR interpretation和反演的研究现状<br>Step2: Summary of weakly supervised techniques 柔性监督技术的总结<br>Step3: Discussion of future research directions 未来研究方向的讨论<br>Output: Comprehensive review of LiDAR remote sensing 综述LiDAR遥感 |
8.5 | [[8.5] 2503.18393 PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes](https://arxiv.org/abs/2503.18393) <br> [{'name': 'Xinhua Xu, Hong Liu, Jianbing Wu, Jinfu Liu'}] | Image and Video Generation 图像生成 | v2<br>RGB segmentation<br>pseudo depth<br>semantic segmentation | Input: RGB images and pseudo depth maps RGB图像和伪深度图<br>Step1: Generate pseudo depth maps 生成伪深度图<br>Step2: Integrate RGB and pseudo depth 结合RGB和伪深度<br>Step3: Apply Pseudo Depth Aggregation Module (PDAM) 应用伪深度聚合模块 (PDAM)<br>Step4: Utilize diffusion model for feature extraction 利用扩散模型进行特征提取<br>Output: Segmentation results segmentation结果 |
8.5 | [[8.5] 2503.18408 Fast and Physically-based Neural Explicit Surface for Relightable Human Avatars](https://arxiv.org/abs/2503.18408) <br> [{'name': 'Jiacheng Wu, Ruiqi Zhang, Jie Chen, Hui Zhang'}] | Neural Rendering 神经渲染 | v2<br>3D reconstruction 三维重建<br>neural rendering 神经渲染<br>autonomous systems 自动驾驶 | Input: Sparse-view videos 稀疏视图视频<br>Step1: Learning pose-dependent geometry and texture 学习与姿态相关的几何和纹理<br>Step2: Physically-based rendering and relighting 物理基础渲染与重光照<br>Output: Relightable human avatars 可重光照的人类化身 |
8.5 | [[8.5] 2503.18421 4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video](https://arxiv.org/abs/2503.18421) <br> [{'name': 'Qiang Hu, Zihan Zheng, Houqiang Zhong, Sihua Fu, Li Song,  XiaoyunZhang, Guangtao Zhai, Yanfeng Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D Gaussian compression<br>Free-Viewpoint Video<br>motion-aware representation | Input: Free-Viewpoint Video (FVV) sequences 自由视角视频序列<br>Step1: Establish dynamic Gaussian representation 建立动态高斯表示<br>Step2: Integrate motion-aware encoding 结合运动感知编码<br>Step3: Optimize rate-distortion trade-off 优化速率-失真权衡<br>Output: Compressed and rendered FVV 经过压缩和渲染的FVV |
8.5 | [[8.5] 2503.18470 MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse](https://arxiv.org/abs/2503.18470) <br> [{'name': 'Zhenyu Pan, Han Liu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D spatial reasoning<br>reinforcement learning<br>vision-language models<br>scene generation | Input: Room image, user preferences, and object status 房间图像、用户偏好和物体状态<br>Step1: Generate a reasoning trace alongside a JSON-formatted layout 生成推理跟踪和JSON格式布局<br>Step2: Evaluate layout using reward signals 通过奖励信号评估布局<br>Step3: Optimize spatial structures through reinforcement learning 通过强化学习优化空间结构<br>Output: Enhanced 3D scene generation 改进的三维场景生成 |
8.5 | [[8.5] 2503.18513 LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene](https://arxiv.org/abs/2503.18513) <br> [{'name': 'Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao'}] | 3D Rendering 三维渲染 | v2<br>Neural Radiance Fields<br>3D rendering<br>view synthesis<br>frequency analysis<br>autonomous systems | Input: Scenes with varying frequency details 场景含有变化的频率细节<br>Step1: 3D frequency quantification 进行3D频率量化<br>Step2: Frequency-aware rendering 实现频率感知渲染<br>Step3: Model evaluation and comparison with baselines 评估模型并与基准进行比较<br>Output: High-fidelity view synthesis outputs 高保真视图合成输出 |
8.5 | [[8.5] 2503.18540 HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications](https://arxiv.org/abs/2503.18540) <br> [{'name': 'Guneet Mutreja, Philipp Schuegraf, Ksenia Bittner'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>digital surface models<br>building analysis<br>remote sensing | Input: High-resolution RGB and DSM data 高分辨率RGB和DSM数据<br>Step1: Data curation and pairing 数据策划和配对<br>Step2: Dual-encoder architecture development 双编码器架构开发<br>Step3: Joint representation learning 联合表示学习<br>Step4: Comprehensive evaluation on downstream tasks 全面评估下游任务<br>Output: Improved performance on building-level analysis 改进的建筑水平分析性能 |
8.5 | [[8.5] 2503.18541 UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach](https://arxiv.org/abs/2503.18541) <br> [{'name': 'Kangli Wang, Wei Gao'}] | Point Cloud Processing 点云处理 | v2<br>point cloud compression<br>3D reconstruction<br>efficiency<br>variable rate | Input: Point cloud data 点云数据<br>Step1: Implement Uneven 8-Stage Lossless Coder (UELC) 在无损模式下实施不均匀8阶段无损编码器 (UELC)<br>Step2: Apply Variable Rate and Complexity Module (VRCM) 在有损模式下应用变量速率和复杂性模块 (VRCM)<br>Step3: Combine UELC and VRCM 动态组合UELC和VRCM<br>Output: Compressed point cloud representations 压缩点云表示 |
8.5 | [[8.5] 2503.18544 Distilling Stereo Networks for Performant and Efficient Leaner Networks](https://arxiv.org/abs/2503.18544) <br> [{'name': 'Rafia Rahim, Samuel Woerz, Andreas Zell'}] | Multi-view Stereo 多视角立体 | v2<br>Knowledge Distillation<br>Stereo Matching<br>Depth Estimation | Input: Stereo image pairs 立体图像对<br>Step1: Design of student network 学生网络设计<br>Step2: Knowledge distillation pipeline knowledge knowledge distillation流水线<br>Step3: Evaluation of network performance 网络性能评估<br>Output: Leaner and faster student networks 精简且快速的学生网络 |
8.5 | [[8.5] 2503.18631 Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling](https://arxiv.org/abs/2503.18631) <br> [{'name': 'Kunyang Li, Ming Hou'}] | Autonomous Driving 自动驾驶 | v2<br>lane detection<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model evaluation 模型评估<br>Output: Enhanced 3D models 改进的三维模型 |
8.5 | [[8.5] 2503.18673 Any6D: Model-free 6D Pose Estimation of Novel Objects](https://arxiv.org/abs/2503.18673) <br> [{'name': 'Taeyeop Lee, Bowen Wen, Minjun Kang, Gyuree Kang, In So Kweon, Kuk-Jin Yoon'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>6D pose estimation<br>object detection<br>computer vision | Input: Single RGB-D anchor image 单个RGB-D锚图像<br>Step1: Joint object alignment process 物体对齐处理<br>Step2: Render-and-compare strategy 渲染与比较策略<br>Step3: Pose hypothesis generation 生成目标假设<br>Output: Accurate 6D pose and size estimation 准确的6D姿势和尺寸估计 |
8.5 | [[8.5] 2503.18711 Accenture-NVS1: A Novel View Synthesis Dataset](https://arxiv.org/abs/2503.18711) <br> [{'name': "Thomas Sugg, Kyle O'Brien, Lekh Poudel, Alex Dumouchelle, Michelle Jou, Marc Bosch, Deva Ramanan, Srinivasa Narasimhan, Shubham Tulsiani"}] | Novel View Synthesis 新颖视图合成 | v2<br>novel view synthesis<br>3D reconstruction<br>multi-view scenes | Input: Multi-view images 多视角图像<br>Step1: Data collection 数据收集<br>Step2: Calibration and geolocation 校准与地理定位<br>Step3: Dataset integration 数据集整合<br>Output: ACC-NVS1 dataset ACC-NVS1 数据集 |
8.5 | [[8.5] 2503.18718 GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting](https://arxiv.org/abs/2503.18718) <br> [{'name': 'Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>watermarking<br>3D models | Input: 3D Gaussian models 3D高斯模型<br>Step1: Message embedding 消息嵌入<br>Step2: Distortion enhancement 扭曲增强<br>Step3: Watermark extraction 水印提取<br>Output: Robust watermarked models 可靠水印模型 |
8.5 | [[8.5] 2503.18725 FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching](https://arxiv.org/abs/2503.18725) <br> [{'name': 'Zimin Xia, Alexandre Alahi'}] | 3D Localization and Mapping 3D定位与地图构建 | v2<br>3D localization<br>fine-grained feature matching<br>autonomous navigation | Input: Ground-level image and aerial image 地面图像与航空图像<br>Step1: Map ground image features to 3D point cloud 将地面图像特征映射到3D点云<br>Step2: Select features along height dimension along 选择高度维度的特征<br>Step3: Compute point correspondences using Procrustes alignment 使用Procrustes对齐计算点对应关系<br>Output: Estimated 3 Degrees of Freedom pose of the ground image 估计地面图像的3个自由度姿态 |
8.5 | [[8.5] 2503.18767 Good Keypoints for the Two-View Geometry Estimation Problem](https://arxiv.org/abs/2503.18767) <br> [{'name': 'Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer'}] | Visual Odometry 视觉里程计 | v2<br>keypoint detection 关键点检测<br>homography estimation 单应性估计<br>structure from motion 运动结构估计<br>visual SLAM 视觉SLAM | Input: Image pairs for keypoint detection 图像对用于关键点检测<br>Step1: Develop a theoretical model for keypoint scoring 建立关键点评分的理论模型<br>Step2: Identify properties of good keypoints 确定良好关键点的特性<br>Step3: Design and implement the BoNeSS-ST keypoint detector 设计并实现BoNeSS-ST关键点检测器<br>Output: Enhanced keypoint performance 改进的关键点表现 |
8.5 | [[8.5] 2503.18853 3DSwapping: Texture Swapping For 3D Object From Single Reference Image](https://arxiv.org/abs/2503.18853) <br> [{'name': 'Xiao Cao, Beibei Lin, Bo Wang, Zhiyong Huang, Robby T. Tan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D texture swapping<br>view consistency<br>gradient guidance | Input: Single reference image 单个参考图像<br>Step1: Progressive generation 逐步生成<br>Step2: View-consistency gradient guidance 视图一致性梯度引导<br>Step3: Prompt-tuning based guidance 提示调优引导<br>Output: High-fidelity texture swaps 高保真纹理交换 |
8.5 | [[8.5] 2503.18903 Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection](https://arxiv.org/abs/2503.18903) <br> [{'name': 'Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, Sahin Albayrak'}] | Object Detection 目标检测 | v2<br>semi-supervised object detection<br>autonomous driving<br>pseudo-labeling | Input: Real-world datasets with labeled and unlabeled data 真实世界数据集，含标记和未标记数据<br>Step1: Identify challenges in SSOD under real conditions 确定真实条件下的半监督目标检测中的挑战<br>Step2: Propose building blocks for performance improvement 提出性能改进的构建模块<br>Step3: Validate the methods through experiments on autonomous driving datasets 通过在自动驾驶数据集上的实验验证方法<br>Output: Enhanced semi-supervised object detection performance 改进的半监督目标检测性能 |
8.5 | [[8.5] 2503.18933 SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction](https://arxiv.org/abs/2503.18933) <br> [{'name': 'Enrico Pallotta, Sina Mokhtarzadeh Azar, Shuai Li, Olga Zatsarynna, Juergen Gall'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video prediction<br>multi-modal<br>depth<br>RGB | Input: Past video frames 过去的视频帧<br>Step1: Modality integration 模态集成<br>Step2: Multi-modal video prediction 多模态视频预测<br>Step3: Performance evaluation 性能评估<br>Output: Future video frames 未来的视频帧 |
8.5 | [[8.5] 2503.18950 Target-Aware Video Diffusion Models](https://arxiv.org/abs/2503.18950) <br> [{'name': 'Taeksoo Kim, Hanbyul Joo'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>human-object interaction<br>robotics<br>3D motion synthesis | Input: An input image and segmentation mask to indicate the target<br>Step1: Extend a baseline image-to-video diffusion model to incorporate the target mask<br>Step2: Introduce a special token to describe the target in the text prompt<br>Step3: Fine-tune the model using a novel cross-attention loss<br>Output: Generated video of actor interacting with the specified target |
8.0 | [[8.0] 2503.18556 Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2503.18556) <br> [{'name': 'Bin Li, Dehong Gao, Yeyuan Wang, Linbo Jin, Shanqing Yu, Xiaoyan Cai, Libin Yang'}] | VLM & VLA 视觉语言模型与对齐 | v2<br>Large Vision-Language Models<br>hallucinations<br>contrastive decoding | Input: Image tokens 图像标记<br>Step1: Attention calculation 注意力计算<br>Step2: Instruction-based adjustment 基于指令的调整<br>Step3: Contrastive decoding 对比解码<br>Output: Adjusted logits 调整后的逻辑值 |
7.5 | [[7.5] 2503.17700 MAMAT: 3D Mamba-Based Atmospheric Turbulence Removal and its Object Detection Capability](https://arxiv.org/abs/2503.17700) <br> [{'name': 'Paul Hill, Zhiming Liu, Nantheera Anantrasirichai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D convolutions<br>atmospheric turbulence<br>object detection | Input: Video sequences affected by atmospheric turbulence 受大气湍流影响的视频序列<br>Step1: Non-rigid registration using deformable 3D convolutions 采用可变形3D卷积进行非刚性配准<br>Step2: Contrast and detail enhancement using 3D Mamba architecture 采用3D Mamba架构进行对比度和细节增强<br>Output: Enhanced video with improved object detection capabilities 提升视频质量并改善物体检测能力 |
7.5 | [[7.5] 2503.18278 TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](https://arxiv.org/abs/2503.18278) <br> [{'name': 'Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>token pruning<br>Vision-Language Models<br>optimization | Input: Visual tokens 视觉标记<br>Step1: Token selection 选择标记<br>Step2: Optimization formulation 优化公式化<br>Step3: Pruning execution 修剪执行<br>Output: Reduced token set 减少的标记集 |
7.5 | [[7.5] 2503.18623 Training-Free Personalization via Retrieval and Reasoning on Fingerprints](https://arxiv.org/abs/2503.18623) <br> [{'name': 'Deepayan Das, Davide Talon, Yiming Wang, Massimiliano Mancini, Elisa Ricci'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>personalization<br>multimodal reasoning | Input: Pre-trained Vision-Language Models (VLMs) 预训练视觉语言模型<br>Step 1: Extract concept fingerprints 提取概念指纹<br>Step 2: Retrieve similar fingerprints from the database 检索相似的指纹<br>Step 3: Validate scores through cross-modal verification 验证得分通过跨模态验证<br>Output: Personal concept identification 输出: 个人概念识别 |


## Arxiv 2025-03-24

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.16591 UniK3D: Universal Camera Monocular 3D Estimation](https://arxiv.org/abs/2503.16591) <br> [{'name': 'Luigi Piccinelli, Christos Sakaridis, Mattia Segu, Yung-Hsu Yang, Siyuan Li, Wim Abbeloos, Luc Van Gool'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>monocular 3D estimation<br>3D reconstruction | Input: Single image from any camera type 任何类型的单幅图像<br>Step1: Spherical representation modeling 球面表示建模<br>Step2: Camera ray decomposition 相机光线分解<br>Step3: Metric 3D reconstruction metrics 计量3D重建评估<br>Output: Coherent 3D point cloud coherent 3D点云 |
9.5 | [[9.5] 2503.16653 iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation](https://arxiv.org/abs/2503.16653) <br> [{'name': 'Hanxiao Wang, Biao Zhang, Weize Quan, Dong-Ming Yan, Peter Wonka'}] | Mesh Reconstruction 网格重建 | v2<br>mesh generation<br>3D modeling<br>transformer architecture<br>attention mechanisms | Input: Mesh representations 网格表示<br>Step1: Interleaving full and linear attention mechanisms 全部与线性注意机制交错<br>Step2: Hourglass architecture integration 入集成沙漏架构<br>Step3: Efficiency enhancements 效率增强<br>Output: High-quality 3D meshes 高质量三维网格 |
9.5 | [[9.5] 2503.16707 Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2503.16707) <br> [{'name': 'Jinlong Li, Cristiano Saltori, Fabio Poiesi, Nicu Sebe'}] | 3D Scene Understanding 3D场景理解 | v2<br>3D scene understanding 3D场景理解<br>vision-language models 视觉语言模型<br>uncertainty estimation 不确定性评估 | Input: Multiple foundation models 多个基础模型<br>Step1: Feature embedding extraction 特征嵌入提取<br>Step2: Cross-modal knowledge distillation 跨模态知识蒸馏<br>Step3: Uncertainty estimation and harmonization 不确定性评估与协调<br>Output: Enhanced 3D scene understanding 强化的3D场景理解 |
9.5 | [[9.5] 2503.16710 4D Gaussian Splatting SLAM](https://arxiv.org/abs/2503.16710) <br> [{'name': 'Yanyan Li, Youxu Fang, Zunjie Zhu, Kunyi Li, Yong Ding, Federico Tombari'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>4D Gaussian Splatting<br>SLAM<br>camera localization<br>dynamic scenes<br>3D reconstruction | Input: Sequence of RGB-D images RGB-D图像序列<br>Step1: Generate motion masks 生成运动掩码<br>Step2: Classify Gaussian primitives into static and dynamic 静态和动态高斯原语分类<br>Step3: Model transformation fields with sparse control points and MLP 使用稀疏控制点和MLP建模变换场<br>Output: 4D Gaussian radiance fields 4D高斯辐射场 |
9.5 | [[9.5] 2503.16776 OpenCity3D: What do Vision-Language Models know about Urban Environments?](https://arxiv.org/abs/2503.16776) <br> [{'name': 'Valentin Bieri, Marco Zamboni, Nicolas S. Blumer, Qingxuan Chen, Francis Engelmann'}] | 3D Scene Understanding 三维场景理解 | v2<br>Vision-Language Models<br>3D Reconstruction<br>Urban Analytics | Input: Aerial multi-view images from urban environments 城市环境的多视角航拍图像<br>Step1: Generate enriched point cloud from 3D reconstructions 从三维重建生成丰富的点云<br>Step2: Integrate vision-language models to query urban features 集成视觉语言模型以查询城市特征<br>Step3: Analyze socio-economic properties using language input 使用语言输入分析社会经济属性<br>Output: Insights into urban characteristics and analytics 对城市特征和分析的洞察 |
9.5 | [[9.5] 2503.16811 Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision](https://arxiv.org/abs/2503.16811) <br> [{'name': 'Maoji Zheng, Ziyu Xu, Qiming Xia, Hai Wu, Chenglu Wen, Cheng Wang'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection 3D物体检测<br>semantic segmentation 语义分割<br>LiDAR<br>autonomous driving 自动驾驶 | Input: Point cloud data 点云数据<br>Step1: Multi-Frame Multi-Scale Clustering (MFMS-C) for pseudo-label generation 多帧多尺度聚类生成伪标签<br>Step2: Semantic-Guiding Iterative-Mining Self-Training (SGIM-ST) for refining labels 语义引导的迭代挖掘自我训练<br>Output: Enhanced 3D object detection results 改进的三维物体检测结果 |
9.5 | [[9.5] 2503.16822 RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos](https://arxiv.org/abs/2503.16822) <br> [{'name': 'Yuxin Yao, Zhi Deng, Junhui Hou'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian representation<br>dynamic modeling<br>novel view synthesis | Input: Monocular videos of articulated objects 单目视频<br>Step1: Extract skeleton-aware nodes from 3D Gaussians 从三维高斯中提取关节感知节点<br>Step2: Simplify skeleton using geometric and semantic information 使用几何和语义信息简化骨骼<br>Step3: Bind skeleton to 3D Gaussian representation 绑定骨骼到三维高斯表示<br>Output: Skeleton-driven dynamic model 支驱动态模型 |
9.5 | [[9.5] 2503.16924 Optimized Minimal 3D Gaussian Splatting](https://arxiv.org/abs/2503.16924) <br> [{'name': 'Joo Chan Lee, Jong Hwan Ko, Eunbyung Park'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>3D rendering<br>storage optimization | Input: 3D scenes 3D场景<br>Step1: Minimize redundancy in Gaussians 最小化高斯冗余<br>Step2: Create compact attribute representation 创建紧凑属性表示<br>Step3: Implement sub-vector quantization 实施子向量量化<br>Output: Reduced storage with minimal Gaussians 减少存储需求的最小高斯 |
9.5 | [[9.5] 2503.16964 DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery](https://arxiv.org/abs/2503.16964) <br> [{'name': 'Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>drone imagery<br>Gaussian Splatting<br>dynamic scenes | Input: Drone imagery 无人机图像<br>Step1: Data integration 数据集成<br>Step2: Masking and segmentation 伪影与分割<br>Step3: Gaussian splatting algorithm implementation 高斯点云算法实现<br>Step4: 3D model reconstruction 三维模型重建<br>Output: Robust 3D reconstruction of scenes 稳健的场景三维重建 |
9.5 | [[9.5] 2503.16970 Distilling Monocular Foundation Model for Fine-grained Depth Completion](https://arxiv.org/abs/2503.16970) <br> [{'name': 'Yingping Liang, Yutao Hu, Wenqi Shao, Ying Fu'}] | Depth Estimation 深度估计 | v2<br>Depth Completion 深度补全<br>Monocular Models 单目模型<br>Knowledge Distillation 知识蒸馏 | Input: Sparse LiDAR inputs 稀疏LiDAR输入<br>Step1: Generate diverse training data 生成多样化训练数据<br>Step2: Distill geometric knowledge 提取几何知识<br>Step3: Fine-tune with SSI Loss fine-tune 采用SSI Loss进行微调<br>Output: Enhanced depth completion models 改进的深度补全模型 |
9.5 | [[9.5] 2503.17032 TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting](https://arxiv.org/abs/2503.17032) <br> [{'name': 'Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>augmented reality<br>3D Gaussian Splatting | Input: Multi-view sequences 多视角序列<br>Step1: Creation of parametric template 创建参数模板<br>Step2: Pre-training of StyleUnet-based network 预训练StyleUnet网络<br>Step3: Baking deformations into MLP network 将变形转化为MLP网络<br>Output: Real-time rendering of avatars 实时渲染头像 |
9.5 | [[9.5] 2503.17093 ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration](https://arxiv.org/abs/2503.17093) <br> [{'name': "Johan Edstedt, Andr\\'e Mateus, Alberto Jaenal"}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction<br>SfM<br>Point Cloud Registration | Input: SfM point clouds (3D Maps) 输入: SfM点云 (三维地图)<br>Step1: Estimation of joint reference frame 第一步: 估计联合参考框架<br>Step2: Point cloud registration point clouds for SfM registration 第二步: SfM 注册的点云注册<br>Step3: Neural refiner application 第三步: 应用神经修整器<br>Output: Merged and registered 3D maps 输出: 合并和注册的三维地图 |
9.5 | [[9.5] 2503.17097 R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model](https://arxiv.org/abs/2503.17097) <br> [{'name': 'Boyuan Zheng, Shouyi Lu, Renbo Huang, Minqing Huang, Fan Lu, Wei Tian, Guirong Zhuo, Lu Xiong'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>4D radar<br>point clouds<br>super-resolution<br>LiDAR<br>autonomous driving | Input: Paired raw radar and LiDAR point clouds 原始雷达和激光雷达点云对<br>Step1: Represent point clouds using voxel features 使用体素特征表示点云<br>Step2: Apply Latent Voxel Diffusion Model (LVDM) 应用潜在体素扩散模型<br>Step3: Utilize Latent Point Cloud Reconstruction (LPCR) to rebuild point clouds 使用潜在点云重建模块重建点云<br>Output: Dense LiDAR-like point clouds 输出：密集的激光雷达样点云 |
9.5 | [[9.5] 2503.17106 GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects](https://arxiv.org/abs/2503.17106) <br> [{'name': 'Yizhe Liu, Tong Jia, Da Cai, Hao Wang, Dongyue Chen'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D structural features | Input: RGB-D input including depth and RGB images (输入: 包含深度和RGB图像的RGB-D输入)<br>Step1: Extract 2D features from RGB-D data (步骤1: 从RGB-D数据中提取2D特征)<br>Step2: Back-project depth to a point cloud for 3D feature extraction (步骤2: 将深度反投影到点云以提取3D特征)<br>Step3: Use gated cross-modal fusion modules for integrating 2D and 3D features (步骤3: 使用门控跨模态融合模块整合2D和3D特征)<br>Output: Enhanced depth estimation for transparent and specular objects (输出: 针对透明和高光物体的增强深度估计) |
9.5 | [[9.5] 2503.17153 Enhancing Steering Estimation with Semantic-Aware GNNs](https://arxiv.org/abs/2503.17153) <br> [{'name': 'Fouad Makiyeh, Huy-Dung Nguyen, Patrick Chareyre, Ramin Hasani, Marc Blanchon, Daniela Rus'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>steering estimation<br>3D spatial information<br>autonomous driving<br>Graph Neural Networks<br>point clouds | Input: Monocular images and LiDAR-based point clouds<br>Step1: Estimate depth and semantic maps from 2D images using a unified model<br>Step2: Generate pseudo-3D point clouds from estimated depth<br>Step3: Integrate 3D point clouds with Graph Neural Network (GNN) and Recurrent Neural Network (RNN) for steering estimation<br>Output: Enhanced steering predictions using spatial information |
9.5 | [[9.5] 2503.17168 Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving](https://arxiv.org/abs/2503.17168) <br> [{'name': 'Alexandra Arzberger, Ramin Tavakoli Kolagari'}] | 3D Object Detection 3D目标检测 | v2<br>LiDAR<br>object detection<br>autonomous driving<br>robustness | Input: LiDAR point cloud data LiDAR点云数据<br>Step1: Implement Hi-ALPS framework 实现Hi-ALPS框架<br>Step2: Evaluate robustness of object detection systems 评估目标检测系统的鲁棒性<br>Step3: Analyze perturbation effects on OD systems 分析对OD系统的扰动影响<br>Output: Robustness metrics for 3D object detection systems 3D目标检测系统的鲁棒性指标 |
9.5 | [[9.5] 2503.17182 Radar-Guided Polynomial Fitting for Metric Depth Estimation](https://arxiv.org/abs/2503.17182) <br> [{'name': 'Patrick Rim, Hyoungseob Park, Vadim Ezhov, Jeffrey Moon, Alex Wong'}] | Depth Estimation 深度估计 | v2<br>3D reconstruction<br>depth estimation<br>autonomous driving<br>radar<br>polynomial fitting | Input: Radar data and monocular depth predictions 雷达数据与单目深度预测<br>Step1: Polynomial fitting of depth predictions 深度预测的多项式拟合<br>Step2: Adaptive adjustment of depth non-uniformly 适应性地对深度进行非均匀调整<br>Step3: Model training with monotonicity regularization 使用单调性正则化进行模型训练<br>Output: Metric depth maps and error metrics 精确度量的深度图和误差指标 |
9.5 | [[9.5] 2503.17316 Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors](https://arxiv.org/abs/2503.17316) <br> [{'name': 'Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>depth completion<br>multi-view stereo | Input: Multi-view images, camera intrinsics, and depth inputs (RGB, intrinsics, poses)<br>Step 1: Data integration by incorporating known camera and scene priors<br>Step 2: Lightweight transformer-based model that allows for modality selection during training<br>Step 3: Output pointmaps for relative pose estimation and high-resolution reconstruction |
9.2 | [[9.2] 2503.16611 A Recipe for Generating 3D Worlds From a Single Image](https://arxiv.org/abs/2503.16611) <br> [{'name': 'Katja Schwarz, Denys Rozumnyi, Samuel Rota Bul\\`o, Lorenzo Porzi, Peter Kontschieder'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>depth estimation<br>image generation | Input: Single image (2D panorama) 单幅图像（2D全景）<br>Step1: Generate coherent panoramas using a diffusion model 生成连贯的全景图像，使用扩散模型<br>Step2: Lift panorama into 3D with a metric depth estimator 利用测量深度估计将全景提升到3D<br>Step3: Inpaint unobserved regions using point clouds 使用点云对未观察区域进行修复<br>Output: Immersive 3D world 逼真的3D世界 |
9.2 | [[9.2] 2503.17175 Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection](https://arxiv.org/abs/2503.17175) <br> [{'name': 'Duanrui Yu, Jing You, Xin Pei, Anqi Qu, Dingyu Wang, Shaocheng Jia'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection 3D目标检测<br>collaborative perception 协作感知<br>semantic detection boxes 语义检测框 | Input: Multi-agent sparse features 多智能体稀疏特征<br>Step1: Feature extraction 特征提取<br>Step2: Temporal fusion 时序融合<br>Step3: Sparse decoding 稀疏解码<br>Output: 3D object detection boxes 3D目标检测框 |
9.0 | [[9.0] 2503.16825 SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion](https://arxiv.org/abs/2503.16825) <br> [{'name': 'Xiyue Guo, Jiarui Hu, Junjie Hu, Hujun Bao, Guofeng Zhang'}] | 3D Semantic Scene Completion 3D语义场景补全 | v2<br>3D semantic scene completion<br>satellite-ground fusion<br>autonomous driving | Input: Satellite and ground images 卫星和地面图像<br>Step1: Parallel encoding of satellite and ground views 卫星和地面视图的并行编码<br>Step2: Feature alignment and correction 特征对齐与修正<br>Step3: Adaptive fusion of multi-view features 多视角特征的自适应融合<br>Output: Completed 3D semantic scene 完成的3D语义场景 |
9.0 | [[9.0] 2503.16979 Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2503.16979) <br> [{'name': 'Jinbo Yan, Rui Peng, Zhiyan Wang, Luyang Tang, Jiayu Yang, Jie Liang, Jiahao Wu, Ronggang Wang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>dynamic scene reconstruction<br>Gaussian splatting | Input: Multi-view images 多视角图像<br>Step1: Generalized Anchor-driven Gaussian Motion Network 引入通用锚点驱动高斯运动网络<br>Step2: Key-frame-guided Streaming Strategy 关键帧引导流媒体策略<br>Step3: Real-time evaluation 实时评估<br>Output: Streamed dynamic scene reconstruction 流媒体动态场景重建 |
8.5 | [[8.5] 2503.16535 Vision-Language Embodiment for Monocular Depth Estimation](https://arxiv.org/abs/2503.16535) <br> [{'name': 'Jinchang Zhang, Guoyu Lu'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>monocular<br>robotic perception | Input: RGB images and camera intrinsic properties RGB图像和相机内在特性<br>Step1: Calculate embodied scene depth 计算具体现场深度<br>Step2: Integrate depth with image features 深度与图像特征集成<br>Step3: Use language priors for scene understanding 利用语言先验进行场景理解<br>Output: Enhanced depth estimations 改进的深度估计 |
8.5 | [[8.5] 2503.16538 Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking](https://arxiv.org/abs/2503.16538) <br> [{'name': 'Bastian P\\"atzold, Jan Nogga, Sven Behnke'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Instance Segmentation<br>Open-Vocabulary Detection<br>Robotics | Input: Structured descriptions from vision-language models (VLMs) 视觉语言模型生成的结构化描述<br>Step1: Identify visible object instances 识别可见物体实例<br>Step2: Inform open-vocabulary detector 通知开放词汇探测器<br>Step3: Extract bounding boxes 提取边界框<br>Step4: Process image streams in real time 以实时处理图像流<br>Output: Segmentation masks and tracking capabilities 分割掩码和跟踪能力 |
8.5 | [[8.5] 2503.16579 World Knowledge from AI Image Generation for Robot Control](https://arxiv.org/abs/2503.16579) <br> [{'name': 'Jonas Krumme, Christoph Zetzsche'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Generative AI<br>Image Generation<br>Robot Control<br>Implicit Knowledge | Input: Images generated by AI 由AI生成的图像<br>Step1: Analyze world knowledge 分析世界知识<br>Step2: Apply knowledge to robot tasks 将知识应用于机器人任务<br>Step3: Generate contextually relevant images 生成上下文相关的图像<br>Output: Enhanced robot task performance 提高机器人的任务表现 |
8.5 | [[8.5] 2503.16709 QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge](https://arxiv.org/abs/2503.16709) <br> [{'name': 'Xuan Shen, Weize Ma, Jing Liu, Changdi Yang, Rui Ding, Quanyi Wang, Henghui Ding, Wei Niu, Yanzhi Wang, Pu Zhao, Jun Lin, Jiuxiang Gu'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation<br>Post-Training Quantization<br>3D Reconstruction | Input: Monocular images 单目图像<br>Step1: Analyze outlier distribution 分析异常分布<br>Step2: Apply LogNP polishing optimization 应用LogNP平滑优化<br>Step3: Update weights for activation compensation 更新权重以补偿激活<br>Step4: Perform weight quantization with reconstruction 进行带重构的权重量化<br>Output: Efficient depth estimation model 高效的深度估计模型 |
8.5 | [[8.5] 2503.16742 Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data](https://arxiv.org/abs/2503.16742) <br> [{'name': 'Esther Y. H. Lin, Yimin Ding, Jogendra Kundu, Yatong An, Mohamed T. El-Haddad, Alexander Fix'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D synthetic data<br>eye tracking<br>NeRF<br>hardware evaluation<br>augmented reality | Input: Real 3D eyes data from light dome captures<br>Step1: Create a hybrid mesh-NeRF representation for eye modeling<br>Step2: Develop an optical simulator for camera effects<br>Step3: Synthesize novel viewpoints and evaluate performance<br>Output: Enhanced predictions of eye tracker hardware performance |
8.5 | [[8.5] 2503.16910 Salient Object Detection in Traffic Scene through the TSOD10K Dataset](https://arxiv.org/abs/2503.16910) <br> [{'name': 'Yu Qiu, Yuhang Sun, Jie Mei, Lin Xiao, Jing Xu'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>salient object detection<br>traffic scenes<br>TSOD10K | Input: Traffic images 交通图像<br>Step1: Data collection 数据收集<br>Step2: Dataset creation 数据集创建<br>Step3: Model development 模型开发<br>Step4: Evaluation of models 模型评估<br>Output: Traffic salient object detection results 交通显著性对象检测结果 |
8.5 | [[8.5] 2503.16976 GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation](https://arxiv.org/abs/2503.16976) <br> [{'name': 'Weihao Yu, Xiaoqing Guo, Chenxin Li, Yifan Liu, Yixuan Yuan'}] | Point Cloud Processing 点云处理 | v2<br>3D segmentation<br>tooth point clouds<br>semi-supervised learning | Input: Intra-oral scans 口腔内扫描<br>Step1: Introduce geometric priors 引入几何先验<br>Step2: Estimate instance-dependent transition matrix (IDTM) 估计实例相关转移矩阵<br>Step3: Perform segmentation segmentation 执行分割<br>Output: Segmented tooth point clouds 分割的牙齿点云 |
8.5 | [[8.5] 2503.17044 ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail](https://arxiv.org/abs/2503.17044) <br> [{'name': 'Chandan Yeshwanth, David Rozenberszki, Angela Dai'}] | 3D Scene Understanding 三维场景理解 | v2<br>3D captioning<br>3D scene understanding<br>Vision-Language Models | Input: 3D scene scans 3D场景扫描<br>Step1: Object detection and 3D understanding 对象检测与3D理解<br>Step2: Multi-level caption generation 多级描述生成<br>Output: Object- and part-level detailed captions 对象和部分级别的详细描述 |
8.5 | [[8.5] 2503.17122 R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception](https://arxiv.org/abs/2503.17122) <br> [{'name': 'Jonas Mirlach, Lei Wan, Andreas Wiedholz, Hannan Ejaz Keen, Andreas Eich'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>LiDAR<br>thermal imaging<br>autonomous driving<br>Vulnerable Road Users | Input: Multi-modal sensors (LiDAR, RGB, and thermal) 多模态传感器 (激光雷达、RGB和热成像)<br>Step1: Data collection 数据收集<br>Step2: Annotation and alignment 标注与对齐<br>Step3: Dataset release 数据集发布<br>Output: R-LiViT dataset R-LiViT 数据集 |
8.5 | [[8.5] 2503.17197 FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy](https://arxiv.org/abs/2503.17197) <br> [{'name': 'Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D texture recovery<br>facial UV textures | Input: Single-view 2D images 单视角二维图像<br>Step1: Appearance feature extraction 外观特征提取<br>Step2: Structural consistency training 结构一致性训练<br>Step3: Cross-Assembly inference integration 交叉组装推理集成<br>Output: Realistic 3D facial UV textures 逼真的三维面部UV纹理 |
8.5 | [[8.5] 2503.17352 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement](https://arxiv.org/abs/2503.17352) <br> [{'name': 'Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang'}] | Vision-Language Models 视觉语言模型 | v2<br>vision-language models<br>reasoning capabilities<br>reinforcement learning | Input: Large vision-language models (LVLMs) 大型视觉语言模型<br>Step1: Distill reasoning capabilities from text models 从文本模型中提取推理能力<br>Step2: Generate reasoning steps using image captions 使用图像说明生成推理步骤<br>Step3: Utilize supervised fine-tuning (SFT) for initial training 利用监督微调进行初始训练<br>Step4: Apply reinforcement learning (RL) for iterative improvement 应用强化学习进行迭代改进<br>Output: Improved LVLM with enhanced reasoning capabilities 改进的LVLM，具有增强的推理能力 |
8.5 | [[8.5] 2503.17358 Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image](https://arxiv.org/abs/2503.17358) <br> [{'name': 'Jerred Chen, Ronald Clark'}] | Visual Odometry 视觉里程计 | v2<br>camera motion estimation<br>visual odometry<br>motion blur<br>single image | Input: Single motion-blurred image 单张运动模糊图像<br>Step1: Predict motion flow field and monocular depth map 预测运动流场和单目深度图<br>Step2: Solve linear least squares problem 解决线性最小二乘问题<br>Output: Instantaneous camera velocity estimate 瞬时相机速度估计 |
7.5 | [[7.5] 2503.17142 Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models](https://arxiv.org/abs/2503.17142) <br> [{'name': 'Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>compositionality<br>visual embeddings<br>image generation | Input: Pre-trained VLMs input visual embeddings 预训练的视觉语言模型输入视觉嵌入<br>Step1: Analyze visual compositionality 分析视觉组成性<br>Step2: Develop Geodesically Decomposable Embeddings (GDE) 开发几何可分解嵌入<br>Step3: Evaluate on compositional classification and group robustness 在组合分类和组鲁棒性上评估<br>Output: Enhanced understanding of visual embeddings 提升视觉嵌入理解 |


## Arxiv 2025-03-21

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.15671 CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image](https://arxiv.org/abs/2503.15671) <br> [{'name': 'Arindam Dutta, Meng Zheng, Zhongpai Gao, Benjamin Planche, Anwesha Choudhuri, Terrence Chen, Amit K. Roy-Chowdhury, Ziyan Wu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>occlusion management<br>human modeling | Input: Single occluded image 单个被遮挡的图像<br>Step1: Generate occlusion-free views 生成无遮挡视图<br>Step2: Apply multiview diffusion model 应用多视角扩散模型<br>Step3: Predict 3D Gaussians 预测3D高斯<br>Output: Cohesive 3D representation 连续的3D表示 |
9.5 | [[9.5] 2503.15672 GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving](https://arxiv.org/abs/2503.15672) <br> [{'name': 'William Ljungbergh, Adam Lilja, Adam Tonderski. Arvid Laveno Ling, Carl Lindstr\\"om, Willem Verbeke, Junsheng Fu, Christoffer Petersson, Lars Hammarstrand, Michael Felsberg'}] | Autonomous Driving 自动驾驶 | v2<br>self-supervised learning<br>occupancy prediction<br>autonomous driving | Input: Future lidar scans, camera images, and ego poses 未来激光雷达扫描、相机图像和自我姿态<br>Step1: Model geometric and semantic occupancy prediction 模型几何和语义占用预测<br>Step2: Learn unified representation  学习统一表示<br>Step3: Validate on autonomous driving benchmarks 在自动驾驶基准上验证<br>Output: Structured, generalizable representation of the environment 结构化、可泛化的环境表示 |
9.5 | [[9.5] 2503.15712 SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints](https://arxiv.org/abs/2503.15712) <br> [{'name': 'Weiwen Hu, Niccol\\`o Parodi, Marcus Zepp, Ingo Feldmann, Oliver Schreer, Peter Eisert'}] | 3D Segmentation 3D 分割 | v2<br>3D segmentation 3D 分割<br>Neural Radiance Fields 神经辐射场<br>geometric primitives 几何原语<br>CLIP | Input: 3D scenes with CLIP features 处理含有 CLIP 特征的 3D 场景<br>Step1: Integrate geometric primitives into NeRF 在 NeRF 中整合几何原语<br>Step2: Generate primitive-wise CLIP features 生成原语级 CLIP 特征<br>Step3: Apply primitive-based merging with affinity scoring 使用具有亲和力评分的原语合并<br>Output: Improved 3D segmentation results 改进的 3D 分割结果 |
9.5 | [[9.5] 2503.15742 Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes](https://arxiv.org/abs/2503.15742) <br> [{'name': 'Sarosij Bose, Arindam Dutta, Sayak Nag, Junge Zhang, Jiachen Li, Konstantinos Karydis, Amit K. Roy Chowdhury'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>view synthesis<br>uncertainty quantification | Input: Single RGB image 单一RGB图像<br>Step1: Gaussian parameter optimization 高斯参数优化<br>Step2: Iterative refinement 迭代精炼<br>Step3: Scene rendering 场景渲染<br>Output: Enhanced 3D scene 改进的3D场景 |
9.5 | [[9.5] 2503.15763 OffsetOPT: Explicit Surface Reconstruction without Normals](https://arxiv.org/abs/2503.15763) <br> [{'name': 'Huan Lei'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>surface reconstruction<br>3D point clouds<br>neural networks<br>geometry processing | Input: 3D point clouds 三维点云<br>Step1: Train a neural network to predict surface triangles 训练神经网络以预测表面三角形<br>Step2: Optimize per-point offsets to improve triangle predictions 优化每个点的偏移以提高三角形预测<br>Output: Reconstructed explicit surfaces 还原的显式表面 |
9.5 | [[9.5] 2503.15835 BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting](https://arxiv.org/abs/2503.15835) <br> [{'name': 'Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>dynamic scene reconstruction<br>3D Gaussian Splatting<br>motion blur<br>camera motion<br>object motion | Input: Blurry images with dynamic scenes 含动态场景的模糊图像<br>Step1: Camera motion deblurring 相机运动去模糊<br>Step2: Object motion deblurring 物体运动去模糊<br>Step3: Image alignment with sharp inputs 与清晰输入图像对齐<br>Output: High-quality dynamic scene reconstructions 高质量动态场景重建 |
9.5 | [[9.5] 2503.15855 VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling](https://arxiv.org/abs/2503.15855) <br> [{'name': 'Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, Changick Kim'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>text-to-3D generation<br>multi-view images | Input: Text prompts 文本提示<br>Step1: Dual-stream architecture development 双流架构开发<br>Step2: Joint modeling of multi-view images and camera poses 多视角图像和相机姿态的联合建模<br>Step3: Asynchronous sampling strategy implementation 异步采样策略实施<br>Output: Realistic 3D Gaussian splats 真实的3D高斯点云 |
9.5 | [[9.5] 2503.15897 Learning 3D Scene Analogies with Neural Contextual Scene Maps](https://arxiv.org/abs/2503.15897) <br> [{'name': 'Junho Kim, Gwangtak Bae, Eun Sun Lee, Young Min Kim'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>3D scene analogy<br>neural contextual scene maps<br>trajectory transfer<br>object placement | Input: 3D scenes with regions having spatial relationships 3D场景与空间关系区域<br>Step1: Extract descriptor fields from scenes 从场景中提取描述符字段<br>Step2: Align descriptor fields using smooth maps 使用平滑映射对齐描述符字段<br>Step3: Estimate dense mappings between scene regions 估计场景区域之间的密集映射<br>Output: Neural contextual scene maps neural contextual scene maps |
9.5 | [[9.5] 2503.15898 Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions](https://arxiv.org/abs/2503.15898) <br> [{'name': 'Boran Wen, Dingbang Huang, Zichen Zhang, Jiahong Zhou, Jianbin Deng, Jingyu Gong, Yulong Chen, Lizhuang Ma, Yong-Lu Li'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>human-object interaction<br>autonomous systems | Input: Single images 单幅图像<br>Step1: Data acquisition 数据获取<br>Step2: 3D annotation pipeline development 3D注释管道开发<br>Step3: Use Gaussian-HOI optimizer 高斯HOI优化器<br>Output: Open-vocabulary 3D HOI dataset 开放词汇3D HOI数据集 |
9.5 | [[9.5] 2503.15908 Enhancing Close-up Novel View Synthesis via Pseudo-labeling](https://arxiv.org/abs/2503.15908) <br> [{'name': 'Jiatong Xia, Libo Sun, Lingqiao Liu'}] | Neural Rendering 神经渲染 | v2<br>novel view synthesis<br>pseudo-labeling<br>Neural Radiance Fields<br>close-up views | Input: Training images with distant viewpoints 远处视角的训练图像<br>Step1: Generate virtual close-up viewpoints 生成虚拟近距离视角<br>Step2: Create wrapped images from original training images 根据原始训练图像创建包装图像<br>Step3: Evaluate consistency and occlusion for pseudo-training data 评估伪训练数据的一致性和遮挡<br>Step4: Train radiance fields with the pseudo-training data 使用伪训练数据训练辐射场模型<br>Output: Enhanced rendering of close-up views 改进的近距离视角渲染 |
9.5 | [[9.5] 2503.15917 Learning to Efficiently Adapt Foundation Models for Self-Supervised Endoscopic 3D Scene Reconstruction from Any Cameras](https://arxiv.org/abs/2503.15917) <br> [{'name': 'Beilei Cui, Long Bai, Mobarakol Islam, An Wang, Zhiqi Ma, Yiming Huang, Feng Li, Zhen Chen, Zhongliang Jiang, Nassir Navab, Hongliang Ren'}] | 3D Reconstruction 三维重建 | v2<br>3D scene reconstruction<br>self-supervised learning<br>depth estimation<br>endoscopic surgery | Input: Surgical videos from any cameras 从任意相机获取的手术视频<br>Step1: Efficient adaptation of foundation models 基础模型的高效适应<br>Step2: Simultaneous estimation of depth maps, poses, and camera parameters 同时估计深度图、姿态和相机参数<br>Step3: 3D scene reconstruction pipeline using estimated parameters 使用估计的参数进行三维场景重建<br>Output: Optimized 3D scene reconstruction 优化的三维场景重建 |
9.5 | [[9.5] 2503.15975 Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation](https://arxiv.org/abs/2503.15975) <br> [{'name': 'Kendong Liu, Zhiyu Zhu, Hui Liu, Junhui Hou'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image-to-3D generation<br>diffusion models | Input: Single images 单张图像<br>Step1: Edge consistency-based refinement 边缘一致性基于的改进<br>Step2: Score function regularization 分数函数正则化<br>Step3: Adversarial augmentation 对抗性增强<br>Output: High-quality 3D models 高质量三维模型 |
9.5 | [[9.5] 2503.15997 Automating 3D Dataset Generation with Neural Radiance Fields](https://arxiv.org/abs/2503.15997) <br> [{'name': 'P. Schulz, T. Hempel, A. Al-Hamadi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D dataset generation<br>neural radiance fields<br>pose estimation | Input: 2D images of target objects 目标对象的2D图像<br>Step1: 3D model creation 3D模型创建<br>Step2: Dataset generation 数据集生成<br>Output: Annotated 3D datasets 带注释的3D数据集 |
9.5 | [[9.5] 2503.16263 From Monocular Vision to Autonomous Action: Guiding Tumor Resection via 3D Reconstruction](https://arxiv.org/abs/2503.16263) <br> [{'name': "Ayberk Acar, Mariana Smith, Lidia Al-Zogbi, Tanner Watts, Fangjie Li, Hao Li, Nural Yilmaz, Paul Maria Scheikl, Jesse F. d'Almeida, Susheela Sharma, Lauren Branscombe, Tayfun Efe Ertop, Robert J. Webster III, Ipek Oguz, Alan Kuntz, Axel Krieger, Jie Ying Wu"}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>monocular vision<br>tumor resection<br>structure from motion | Input: RGB images  RGB图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm evaluation 算法评估<br>Step3: Segmentation generation 分割生成<br>Output: Segmented point clouds with 3D reconstruction 带有三维重建的分割点云 |
9.5 | [[9.5] 2503.16282 Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model](https://arxiv.org/abs/2503.16282) <br> [{'name': 'Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie'}] | 3D point cloud segmentation 点云分割 | v2<br>3D point cloud segmentation<br>Vision-Language Models<br>few-shot learning | Input: 3D point cloud data 3D点云数据<br>Step1: Pseudo-label selection 伪标签选择<br>Step2: Adaptive infilling strategy 自适应填充策略<br>Step3: Base mix strategy 基础混合策略<br>Output: Enhanced segmentation model 改进的分割模型 |
9.5 | [[9.5] 2503.16302 Unleashing Vecset Diffusion Model for Fast Shape Generation](https://arxiv.org/abs/2503.16302) <br> [{'name': 'Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qinxiang Lin, Jinwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, Xiangyu Yue'}] | 3D Generation 三维生成 | v2<br>3D shape generation 3D形状生成<br>diffusion models 扩散模型<br>VAE Variational Autoencoder 变分自编码器 | Input: 3D shape data 3D形状数据<br>Step1: Analyze diffusion sampling 分析扩散采样<br>Step2: Implement FlashVDM framework 实现FlashVDM框架<br>Step3: Optimize VAE decoding 优化VAE解码<br>Output: High-speed 3D shape generation 高速三维形状生成 |
9.5 | [[9.5] 2503.16318 Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction](https://arxiv.org/abs/2503.16318) <br> [{'name': 'Edgar Sucar, Zihang Lai, Eldar Insafutdinov, Andrea Vedaldi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Dynamic Point Maps<br>3D Reconstruction<br>Video Depth Prediction | Input: Pair of images 图像对<br>Step1: Define point maps 定义点图<br>Step2: Predict dynamic point maps 预测动态点图<br>Step3: Evaluate across benchmarks 在基准上评估<br>Output: Enhanced dynamic reconstruction 改进的动态重建 |
9.5 | [[9.5] 2503.16338 Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images](https://arxiv.org/abs/2503.16338) <br> [{'name': 'Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, Yueqi Duan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Gaussian Graph Network<br>multi-view images<br>3D Gaussian Splatting<br>efficient representation<br>novel view synthesis | Input: Multi-view images 多视角图像<br>Step1: Construct Gaussian Graphs 建立高斯图<br>Step2: Message passing at Gaussian level 高斯级别的消息传递<br>Step3: Gaussian pooling aggregation 高斯池化聚合<br>Output: Efficient Gaussian representations 高效的高斯表示 |
9.5 | [[9.5] 2503.16399 SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World](https://arxiv.org/abs/2503.16399) <br> [{'name': 'Chen Chen, Zhirui Wang, Taowei Sheng, Yi Jiang, Yundu Li, Peirui Cheng, Luning Zhang, Kaiqiang Chen, Yanfeng Hu, Xue Yang, Xian Sun'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D occupancy prediction<br>satellite imagery<br>autonomous driving | Input: Historical satellite imagery and street-view images 历史卫星图像与街道视图图像<br>Step1: Data integration with GPS & IMU data 数据集成与 GPS 和 IMU 数据<br>Step2: Implement Dynamic-Decoupling Fusion for inconsistencies 进行动态解耦融合以解决不一致问题<br>Step3: Use 3D-Proj Guidance for feature extraction 使用 3D 投影引导进行特征提取<br>Step4: Apply Uniform Sampling Alignment for sampling adjustments 使用均匀采样对齐进行采样调整<br>Output: Enhanced 3D occupancy prediction model 输出: 改进的 3D 占用预测模型 |
9.5 | [[9.5] 2503.16412 DreamTexture: Shape from Virtual Texture with Analysis by Augmentation](https://arxiv.org/abs/2503.16412) <br> [{'name': 'Ananta R. Bhattarai, Xingzhe He, Alla Sheffer, Helge Rhodin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>monocular depth<br>texture alignment | Input: Monocular images 单眼图像<br>Step1: Texture alignment 纹理对齐<br>Step2: Depth reconstruction 深度重建<br>Step3: Texture optimization 纹理优化<br>Output: 3D object representation 3D对象表示 |
9.5 | [[9.5] 2503.16413 M3: 3D-Spatial MultiModal Memory](https://arxiv.org/abs/2503.16413) <br> [{'name': 'Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye, Sifei Liu, Xiaolong Wang'}] | 3D Spatial Memory 3D空间记忆 | v2<br>3D Gaussian Splatting<br>multimodal memory<br>autonomous systems | Input: Scene video clips 场景视频片段<br>Step 1: Implement Gaussian splatting 技术实现高斯点云<br>Step 2: Integrate features from foundation models 集成基础模型特征<br>Step 3: Optimize memory structure 优化记忆结构<br>Output: Compressed multimodal memory compressed multi-modal memory |
9.5 | [[9.5] 2503.16429 Sonata: Self-Supervised Learning of Reliable Point Representations](https://arxiv.org/abs/2503.16429) <br> [{'name': 'Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, Julian Straub'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D self-supervised learning<br>point cloud<br>representation quality | Input: Point clouds 点云<br>Step1: Identify geometric shortcuts 识别几何捷径<br>Step2: Apply self-supervised learning techniques 应用自监督学习技术<br>Step3: Enhance representation quality 提升表示质量<br>Output: Reliable point representations 可靠的点表示 |
9.2 | [[9.2] 2503.15667 DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis](https://arxiv.org/abs/2503.15667) <br> [{'name': 'Yuming Gu, Phong Tran, Yujian Zheng, Hongyi Xu, Heyuan Li, Adilbek Karmanov, Hao Li'}] | Image Generation 图像生成 | v2<br>360-degree synthesis<br>human head generation<br>neural rendering | Input: Single-view portrait images 单视角肖像图像<br>Step1: Generate back-of-head details using ControlNet 生成后脑勺细节<br>Step2: Dual appearance module ensures consistency 采用双重外观模块确保一致性<br>Step3: Train on continuous view sequences 训练于连续视图序列<br>Output: Generate 360-degree consistent head views 生成360度一致的头部视图 |
9.0 | [[9.0] 2503.15666 Toward Scalable, Flexible Scene Flow for Point Clouds](https://arxiv.org/abs/2503.15666) <br> [{'name': 'Kyle Vedder'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>scene flow<br>point clouds<br>3D motion estimation<br>scalability | Input: Temporally successive point cloud observations 时间上连续的点云观测<br>Step1: Contextualize scene flow and prior methods 上下文化场景流及其先前方法<br>Step2: Build scalable scene flow estimators 构建可扩展的场景流估计器<br>Step3: Introduce a benchmark for estimate quality 引入估计质量基准<br>Step4: Develop an unsupervised scene flow estimator 开发无监督场景流估计器<br>Output: Enhanced scene flow estimations 改进的场景流估计 |
9.0 | [[9.0] 2503.15877 Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation](https://arxiv.org/abs/2503.15877) <br> [{'name': 'Tiange Xiang, Kai Li, Chengjiang Long, Christian H\\"ane, Peihong Guo, Scott Delp, Ehsan Adeli, Li Fei-Fei'}] | 3D Generation 三维生成 | v2<br>3D generation<br>diffusion models<br>Gaussian fitting | Input: Pre-trained 2D diffusion models 预训练的2D扩散模型<br>Step1: Create Gaussian Atlas from 3D objects 从3D对象创建高斯图<br>Step2: Fine-tune 2D models for 3D output 对2D模型进行微调以生成3D输出<br>Output: Generated 3D Gaussian structures 生成的3D高斯结构 |
9.0 | [[9.0] 2503.16422 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering](https://arxiv.org/abs/2503.16422) <br> [{'name': 'Yuheng Yuan, Qiuhong Shen, Xingyi Yang, Xinchao Wang'}] | Dynamic Scene Rendering 动态场景渲染 | v2<br>4D Gaussian Splatting<br>dynamic scene reconstruction<br>real-time rendering | Input: Dynamic scene data 动态场景数据<br>Step1: Analyze temporal redundancy 分析时间冗余<br>Step2: Implement 4DGS-1K framework 实施4DGS-1K框架<br>Step3: Prune short-lifespan Gaussians 修剪短暂生命周期的高斯<br>Step4: Filter inactive Gaussians 过滤非活动高斯<br>Output: Optimized scene representation 优化的场景表示 |
8.5 | [[8.5] 2503.15676 High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight](https://arxiv.org/abs/2503.15676) <br> [{'name': "C\\'edric Vincent, Taehyoung Kim, Henri Mee{\\ss}"}] | Autonomous Systems and Robotics 自动驾驶机器人 | v2<br>video semantic segmentation<br>autonomous systems<br>temporal consistency | Input: Aerial video frames 航空视频帧<br>Step1: Semantic segmentation using image model  使用图像模型进行语义分割<br>Step2: Temporal prediction propagation  时间预测传播<br>Step3: Knowledge distillation for semi-supervised training  半监督训练的知识蒸馏<br>Output: Consistent and accurate segmentation predictions  一致和准确的分割预测 |
8.5 | [[8.5] 2503.15778 AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models](https://arxiv.org/abs/2503.15778) <br> [{'name': 'Boshra Khalili, Andrew W. Smyth'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>autonomous driving<br>question answering<br>vision-language models | Input: Driving QA datasets 驾驶问答数据集<br>Step1: Data integration 数据集成<br>Step2: MCQ conversion methodology MCQ转换方法<br>Step3: Evaluation on public datasets 在公共数据集上评估<br>Output: Standardized evaluation framework 标准化评估框架 |
8.5 | [[8.5] 2503.15818 Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection](https://arxiv.org/abs/2503.15818) <br> [{'name': 'Haotian Ma, Lin Gu, Siyi Wu, Yingying Zhu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud<br>privacy protection<br>flow-based generative model | Input: 3D point cloud data 3D点云数据<br>Step1: Define the 3D point cloud privacy problem 定义3D点云隐私问题<br>Step2: Implement the PointFlowGMM framework 实现PointFlowGMM框架<br>Step3: Project point cloud into latent Gaussian mixture space 将点云投影到潜在高斯混合空间<br>Step4: Apply rotation for privacy protection 应用旋转以保护隐私<br>Output: Encrypted 3D point clouds with preserved classification capabilities 输出: 具有保留分类能力的加密3D点云 |
8.5 | [[8.5] 2503.15875 MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving](https://arxiv.org/abs/2503.15875) <br> [{'name': 'Haiguang Wang, Daqi Liu, Hongwei Xie, Haisong Liu, Enhui Ma, Kaicheng Yu, Limin Wang, Bing Wang'}] | Video Generation 视频生成 | v2<br>video generation<br>autonomous driving<br>world models | Input: Multi-view video data 多视角视频数据<br>Step1: Generate high-fidelity long videos 生成高保真长时间视频<br>Step2: Stabilize video generation 稳定视频生成<br>Step3: Correct distortion of dynamic objects 修正动态物体的失真<br>Output: Long-duration coherent videos 长时段的一致视频 |
8.5 | [[8.5] 2503.15905 Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation](https://arxiv.org/abs/2503.15905) <br> [{'name': 'Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, Yao Zhao'}] | Depth Estimation 深度估计 | v2<br>Depth Estimation 深度估计<br>Self-supervised Learning 自监督学习<br>Stable Diffusion 稳定扩散 | Input: Monocular images 单目图像<br>Step1: Hybrid image reconstruction construction 混合图像重建<br>Step2: Scale-Shift GRU development 比例-偏移GRU开发<br>Step3: Self-supervised depth estimation self-supervised depth estimation 自监督深度估计<br>Output: Accurate depth maps 精确的深度图 |
8.5 | [[8.5] 2503.15910 No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather](https://arxiv.org/abs/2503.15910) <br> [{'name': 'Junsung Park, Hwijeong Lee, Inha Kang, Hyunjung Shim'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>LiDAR semantic segmentation<br>autonomous driving<br>adverse weather | Input: LiDAR point cloud data<br>Step1: Identify performance gaps in existing models<br>Step2: Develop methods to bind point features to superclasses<br>Step3: Define local regions for cleaning data<br>Output: Improved predictions for 'things' categories |
8.5 | [[8.5] 2503.16000 SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks](https://arxiv.org/abs/2503.16000) <br> [{'name': 'Haojia Gao, Haohua Que, Hoiian Au, Weihao Shan, Mingkai Liu, Yusen Qin, Lei Mu, Rong Zhao, Xinghua Yang, Qi Wei, Fei Qiao'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous exploration<br>prediction network<br>Generative Adversarial Networks (GANs)<br>Robotics<br>efficient frameworks | Input: Partial observations captured by the robot's onboard sensors 通过机器人的传感器捕获的部分观测<br>Step1: Local map prediction 基于局部地图的预测<br>Step2: Model integration with GANs, Transformers, and FFC 用GAN、Transformer和FFC集成模型<br>Step3: Efficiency evaluation 评估效率<br>Output: Efficient autonomous exploration framework 高效的自主探索框架 |
8.5 | [[8.5] 2503.16125 Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection](https://arxiv.org/abs/2503.16125) <br> [{'name': 'Jiangyi Wang, Na Zhao'}] | 3D Object Detection 在室内环境中的应用 | v2<br>3D object detection<br>active learning<br>indoor environments<br>uncertainty<br>diversity | Input: Indoor 3D object data 室内3D物体数据<br>Step1: Sample uncertainty assessment 样本不确定性评估<br>Step2: Diversity optimization 多样性优化<br>Step3: Active sample selection 主动样本选择<br>Output: Annotated samples for indoor 3D detection 为室内3D检测注释的样本 |
8.5 | [[8.5] 2503.16289 SceneMI: Motion In-betweening for Modeling Human-Scene Interactions](https://arxiv.org/abs/2503.16289) <br> [{'name': 'Inwoo Hwang, Bing Zhou, Young Min Kim, Jian Wang, Chuan Guo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>motion in-betweening<br>human-scene interactions<br>generative modeling | Input: Noisy keyframes and scenes 噪声关键帧和场景<br>Step1: Scene encoding through dual descriptors 场景编码通过双重描述符<br>Step2: Dual scene context processing 双重场景上下文处理<br>Step3: Denoising and keyframe interpolation 去噪和关键帧插值<br>Output: Smooth motion transitions and scene reconstructions 平滑的运动过渡和场景重建 |
8.5 | [[8.5] 2503.16378 Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions](https://arxiv.org/abs/2503.16378) <br> [{'name': 'Tzu-Yun Tseng, Alexey Nekrasov, Malcolm Burdorf, Bastian Leibe, Julie Stephany Berrio, Mao Shan, Stewart Worrall'}] | Autonomous Driving 自动驾驶 | v2<br>LiDAR<br>autonomous driving<br>dataset<br>panoptic segmentation | Input: Synchronized sensor data 同步传感器数据<br>Step1: Data collection 数据收集<br>Step2: Annotation of LiDAR and image data LiDAR 和图像数据的标注<br>Step3: Model evaluation and analysis 模型评估与分析<br>Output: Baseline results for segmentation methods 语义分割方法的基线结果 |
8.5 | [[8.5] 2503.16396 SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation](https://arxiv.org/abs/2503.16396) <br> [{'name': 'Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, Varun Jampani'}] | Image and Video Generation 图像生成与视频生成 | v2<br>3D asset generation<br>multi-view video<br>4D generation<br>video diffusion model | Input: Monocular video 单目视频<br>Step1: Network architecture modification 网络架构修改<br>Step2: Data curation 数据整理<br>Step3: Progressive training strategy 逐步训练策略<br>Step4: 4D optimization 4D优化<br>Output: High-quality multi-view videos 高质量多视角视频 |
8.5 | [[8.5] 2503.16420 SynCity: Training-Free Generation of 3D Worlds](https://arxiv.org/abs/2503.16420) <br> [{'name': 'Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, Andrea Vedaldi'}] | 3D Generation 三维生成 | v2<br>3D generation<br>textual descriptions<br>tile-based generation | Input: Textual descriptions 文本描述<br>Step1: Generate 3D tiles 生成3D瓦片<br>Step2: Stitch tiles together 拼接瓦片<br>Step3: Ensure geometric consistency 确保几何一致性<br>Output: Large and immersive 3D worlds 输出: 大型且沉浸式的3D世界 |
7.5 | [[7.5] 2503.15886 Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance](https://arxiv.org/abs/2503.15886) <br> [{'name': 'Hui Liu, Wenya Wang, Kecheng Chen, Jie Liu, Yibing Liu, Tiexin Qin, Peisong He, Xinghao Jiang, Haoliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Zero-Shot Generalization<br>Vision Language Models<br>Concept-guided Reasoning | Input: Zero-shot image recognition data 零样本图像识别数据<br>Step1: Concept modeling 概念建模<br>Step2: Importance sampling algorithm 重要性采样算法<br>Step3: Generate discriminative concepts 生成可区分的概念<br>Output: Enhanced zero-shot recognition results 改进的零样本识别结果 |
6.5 | [[6.5] 2503.16365 JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse](https://arxiv.org/abs/2503.16365) <br> [{'name': 'Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, Yitao Liang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision Language Models<br>decision-making | Input: Vision Language models 视觉语言模型<br>Step1: Visual Language Post-Training 视觉语言后训练<br>Step2: Action decision-making 行为决策<br>Output: Enhanced decision-making capabilities 改进的决策能力 |
6.5 | [[6.5] 2503.16397 Scale-wise Distillation of Diffusion Models](https://arxiv.org/abs/2503.16397) <br> [{'name': 'Nikita Starodubcev, Denis Kuznedelev, Artem Babenko, Dmitry Baranchuk'}] | Image Generation 图像生成 | v2<br>Diffusion Models<br>Text-to-Image Generation<br>Generative Models | Input: Low-resolution data 低分辨率数据<br>Step1: Scale-wise generation 按比例生成<br>Step2: Distribution matching 分布匹配<br>Step3: Resolution upscaling 分辨率上升<br>Output: High-quality generated images 高质量生成图像 |


## Arxiv 2025-03-19

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.13587 Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception](https://arxiv.org/abs/2503.13587) <br> [{'name': 'Dingkang Liang, Dingyuan Zhang, Xin Zhou, Sifan Tu, Tianrui Feng, Xiaofan Li, Yumeng Zhang, Mingyang Du, Xiao Tan, Xiang Bai'}] | Unified World Model 统一世界模型 | v2<br>driving world model<br>future prediction<br>depth estimation<br>autonomous driving | Input: Current image 当前图像<br>Step1: Dual-Latent Sharing scheme 双潜在共享方案<br>Step2: Multi-scale Latent Interaction mechanism 多尺度潜在交互机制<br>Step3: Predict future image-depth pairs 预测未来图像-深度对<br>Output: Unified future predictions 统一的未来预测 |
9.5 | [[9.5] 2503.13710 Improving Geometric Consistency for 360-Degree Neural Radiance Fields in Indoor Scenarios](https://arxiv.org/abs/2503.13710) <br> [{'name': 'Iryna Repinetska, Anna Hilsmann, Peter Eisert'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D reconstruction<br>depth estimation | Input: 360-degree indoor images 360度室内图像<br>Step1: Dense depth priors calculation 密集深度先验计算<br>Step2: Novel depth loss function formulation 新的深度损失函数设计<br>Step3: Patch-based depth regularization implementation 贴片深度正则化实施<br>Output: Improved rendering quality 提高渲染质量 |
9.5 | [[9.5] 2503.13721 SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint](https://arxiv.org/abs/2503.13721) <br> [{'name': 'Zhenlong Yuan, Zhidong Yang, Yujun Cai, Kuangxin Wu, Mufan Liu, Dapeng Zhang, Hao Jiang, Zhaoxin Li, Zhaoqi Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction 三维重建<br>Multi-View Stereo 多视角立体<br>occlusion-aware reconstruction 遮挡感知重建 | Input: Multi-view images 多视角图像<br>Step1: Panoptic segmentation for depth edge guidance 采用全景分割作为深度边缘指导<br>Step2: Multi-trajectory diffusion strategy to align patches with depth edges 多轨迹扩散策略以确保补丁与深度边缘对齐<br>Step3: Combine sparse points and monocular depth map to restore reliable depth map 结合稀疏点和单目深度图以恢复可靠的深度图<br>Output: Accurate 3D reconstruction of the scene or object 输出: 场景或对象的准确三维重建 |
9.5 | [[9.5] 2503.13739 Learning from Synchronization: Self-Supervised Uncalibrated Multi-View Person Association in Challenging Scenes](https://arxiv.org/abs/2503.13739) <br> [{'name': 'Keqi Chen, Vinkle Srivastav, Didier Mutter, Nicolas Padoy'}] | Multi-view Stereo 多视角立体 | v2<br>multi-view person association<br>self-supervised learning<br>geometric constraints | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Encoder-decoder model encoding 准编码解码模型编码<br>Step2: Self-supervised learning framework训练自监督学习框架<br>Step3: Synchronization task for image pairs image pairs image pair 的同步任务<br>Output: Geometric feature encoding 几何特征编码 |
9.5 | [[9.5] 2503.13743 MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent Teacher Models](https://arxiv.org/abs/2503.13743) <br> [{'name': 'Johannes Meier, Louis Inchingolo, Oussema Dhaouadi, Yan Xia, Jacques Kaiser, Daniel Cremers'}] | 3D Object Detection 3D目标检测 | v2<br>Monocular 3D detection 单目3D检测<br>Depth estimation 深度估计<br>Domain adaptation 域适应 | Input: Monocular RGB images 单目RGB图像<br>Step1: Generalized Depth Enhancement (GDE) module development 开发广义深度增强(GDE)模块<br>Step2: Pseudo Label Scoring (PLS) module design 设计伪标签评分(PLS)模块<br>Step3: Extensive experiments on multiple benchmarks 在多个基准上进行广泛实验<br>Output: Improved monocular 3D detection performance 改进的单目3D检测性能 |
9.5 | [[9.5] 2503.13816 MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments](https://arxiv.org/abs/2503.13816) <br> [{'name': 'Zhixuan Liu, Haokun Zhu, Rui Chen, Jonathan Francis, Soonmin Hwang, Ji Zhang, Jean Oh'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>privacy-preserving<br>multi-view images<br>depth images | Input: Depth images only 仅深度图像<br>Step1: Multi-view overlapped scene alignment 多视角重叠场景对齐<br>Step2: Inference-time optimization 推断时优化<br>Step3: Generation of consistent RGB images 生成一致的RGB图像<br>Output: Privacy-preserving digital twins 保持隐私的数字双胞胎 |
9.5 | [[9.5] 2503.13861 RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2503.13861) <br> [{'name': 'Yujin Wang, Quanfeng Liu, Zhengxin Jiang, Tianyi Wang, Junfeng Jiao, Hongqing Chu, Bingzhao Gao, Hong Chen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>autonomous driving<br>decision-making<br>spatial perception | Input: Vision-language models for autonomous driving 视觉语言模型的自主驾驶<br>Step1: Embedding flow for scene encoding 场景编码的嵌入流<br>Step2: Retrieval flow to fetch relevant scenes 检索流获取相关场景<br>Step3: Generating flow to produce meta-actions 生成流生成元动作<br>Output: Decision-making enhancements for autonomous driving decisions 提升自主驾驶决策的决策能力 |
9.5 | [[9.5] 2503.13914 PSA-SSL: Pose and Size-aware Self-Supervised Learning on LiDAR Point Clouds](https://arxiv.org/abs/2503.13914) <br> [{'name': 'Barza Nisar, Steven L. Waslander'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>3D semantic segmentation<br>LiDAR point clouds<br>self-supervised learning | Input: LiDAR point clouds LiDAR点云<br>Step1: Define bounding box regression as pretext task 定义边界框回归作为预训练任务<br>Step2: Incorporate LiDAR beam pattern augmentation 融入激光雷达束模式增强<br>Step3: Train model using contrastive learning 采用对比学习训练模型<br>Output: Object pose and size-aware features 输出：物体姿态和尺寸感知特征 |
9.5 | [[9.5] 2503.13948 Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via Context Model](https://arxiv.org/abs/2503.13948) <br> [{'name': 'Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li, Yiling Xu'}] | 3D Gaussian Splatting 3D高斯点云 | v2<br>4D Gaussian Splatting<br>3D Reconstruction<br>Novel View Synthesis | Input: Temporal deformation primitives 时间变形原语<br>Step1: Spatio-temporal significance pruning 空间-时间显著性修剪<br>Step2: Deep context model integration 深度上下文模型集成<br>Output: Compressed lightweight dynamic 3DGS 压缩轻量级动态3DGS |
9.5 | [[9.5] 2503.14002 MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling](https://arxiv.org/abs/2503.14002) <br> [{'name': 'Damian Boborzi, Phillip Mueller, Jonas Emrich, Dominik Schmid, Sebastian Mueller, Lars Mikelsons'}] | 3D Generation 三维生成 | v2<br>3D reconstruction<br>3D dataset<br>generative modeling<br>vehicle models | Input: 3D models from Objaverse-XL 来自Objaverse-XL的3D模型<br>Step1: Create a manually labeled subset 创建手动标记子集<br>Step2: Train a quality classifier 训练质量分类器<br>Step3: Apply automated filtering 应用自动化过滤<br>Output: High-quality filtered 3D vehicle dataset 输出：高质量过滤的3D车辆数据集 |
9.5 | [[9.5] 2503.14029 Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting](https://arxiv.org/abs/2503.14029) <br> [{'name': 'Runsong Zhu, Shi Qiu, Zhengzhe Liu, Ka-Hei Hui, Qianyi Wu, Pheng-Ann Heng, Chi-Wing Fu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D segmentation<br>Gaussian splatting<br>computer vision | Input: Multi-view 2D instance segmentation 2D实例分割<br>Step1: Gaussian-level feature augmentation 高斯级特征增强<br>Step2: Object-level codebook learning 对象级别的词汇表学习<br>Step3: Association learning 关联学习<br>Step4: Noisy label filtering 噪声标签过滤<br>Output: Accurate 3D scene segmentation 准确的3D场景分割 |
9.5 | [[9.5] 2503.14198 RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2503.14198) <br> [{'name': 'Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, Wei-Shi Zheng'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>novel view synthesis<br>Gaussian splatting<br>autonomous driving | Input: Sparse multi-view images 稀疏多视角图像<br>Step1: Lift SMPL vertices to 3D points 提升SMPL顶点到3D点<br>Step2: Predict image-aligned 3D prior points 预测与图像对齐的3D先验点<br>Step3: Regress coarse and fine Gaussian parameters 回归粗糙和细粒度的高斯参数<br>Output: High-fidelity novel views 高保真新视图 |
9.5 | [[9.5] 2503.14219 Segmentation-Guided Neural Radiance Fields for Novel Street View Synthesis](https://arxiv.org/abs/2503.14219) <br> [{'name': 'Yizhou Li, Yusuke Monno, Masatoshi Okutomi, Yuuichi Tanaka, Seiichi Kataoka, Teruaki Kosiba'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D reconstruction<br>novel view synthesis<br>outdoor scenes | Input: Monocular video clips captured by a video recorder mounted on a car.<br>Step1: Segmentation mask generation using Grounded SAM.<br>Step2:处理 transient objects by excluding them from training.<br>Step3: Modeling the sky with a specialized representation.<br>Step4: Regularizing the ground plane to conform to planar geometry.<br>Step5: Adapting to inconsistent lighting through appearance embeddings.<br>Output: Improved novel view synthesis quality with fewer artifacts. |
9.5 | [[9.5] 2503.14274 Improving Adaptive Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2503.14274) <br> [{'name': 'Glenn Grubert, Florian Barthel, Anna Hilsmann, Peter Eisert'}] | 3D Gaussian Splatting 三维高斯点云 | v2<br>3D reconstruction<br>Gaussian Splatting<br>novel view synthesis | Input: Multi-view images 多视角图像<br>Step1: Adaptive density control for Gaussian management 自适应密度控制以管理高斯<br>Step2: Implement exponential gradient thresholding 实施指数梯度阈值<br>Step3: Calculate corrected scene extent 计算纠正后的场景范围<br>Step4: Execute significance-aware pruning 执行重要性感知修剪<br>Output: Enhanced rendering quality 改进的渲染质量 |
9.5 | [[9.5] 2503.14346 3D Densification for Multi-Map Monocular VSLAM in Endoscopy](https://arxiv.org/abs/2503.14346) <br> [{'name': "X. Anad\\'on, Javier Rodr\\'iguez-Puigvert, J. M. M. Montiel"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>endoscopy<br>visual SLAM<br>CudaSIFT<br>depth estimation | Input: Monocular endoscopic sequences 单目内窥镜序列<br>Step1: Remove outliers 去除异常值<br>Step2: Densify maps 加密地图<br>Step3: Align predictions and submaps 对齐预测和子地图<br>Output: Reliable densified 3D maps 可靠的加密3D地图 |
9.5 | [[9.5] 2503.14445 Bolt3D: Generating 3D Scenes in Seconds](https://arxiv.org/abs/2503.14445) <br> [{'name': 'Stanislaw Szymanowicz, Jason Y. Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan T. Barron, Philipp Henzler'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene generation<br>latent diffusion model<br>multiview images | Input: One or multiple images 任选入图像<br>Step1: Create large-scale multiview-consistent dataset 创建大规模多视角一致性数据集<br>Step2: Train latent diffusion model 训练潜在扩散模型<br>Step3: Generate 3D scene representation 生成三维场景表示<br>Output: Fast 3D scene representation generation 快速生成三维场景表示 |
9.5 | [[9.5] 2503.14463 SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model](https://arxiv.org/abs/2503.14463) <br> [{'name': 'Yucheng Mao, Boyang Wang, Nilesh Kulkarni, Jeong Joon Park'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>image restoration<br>multi-view<br>diffusion model | Input: Multi-view images 多视角图像<br>Step1: Jointly denoise multiple photographs 联合去噪多个影像<br>Step2: Implement a multi-view diffusion model 实施多视角扩散模型<br>Step3: Maintain 3D consistency 维护三维一致性<br>Output: Restored images with improved quality 修复后图像，质量提升 |
9.5 | [[9.5] 2503.14483 Multi-view Reconstruction via SfM-guided Monocular Depth Estimation](https://arxiv.org/abs/2503.14483) <br> [{'name': 'Haoyu Guo, He Zhu, Sida Peng, Haotong Lin, Yunzhi Yan, Tao Xie, Wenguan Wang, Xiaowei Zhou, Hujun Bao'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction 三维重建<br>Monocular Depth Estimation 单目深度估计<br>SfM-guided Reconstruction SfM引导重建<br>Multi-view Geometry 多视角几何 | Input: Multi-view images 多视角图像<br>Step1: Recover the SfM point cloud 恢复SfM点云<br>Step2: Inject SfM information into the diffusion model 将SfM信息注入扩散模型<br>Step3: Predict depth maps 预测深度图<br>Step4: Fuse depth maps for 3D reconstruction 进行深度图融合以实现3D重建<br>Output: High-quality 3D models 高质量的3D模型 |
9.2 | [[9.2] 2503.13869 Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception](https://arxiv.org/abs/2503.13869) <br> [{'name': 'Jinge Ma, Jiangpeng He, Fengqing Zhu'}] | 3D Perception 3D感知 | v2<br>3D perception<br>class-incremental learning<br>autonomous driving | Input: 3D point cloud data 3D点云数据<br>Step1: Develop a robust 3D point cloud class-incremental learning framework 设计一个稳健的3D点云类增量学习框架<br>Step2: Implement an exemplar selection strategy based on Farthest Point Sampling 实施基于最远点采样的样本选择策略<br>Step3: Introduce a point cloud downsampling-based replay method 引入基于点云降采样的重放方法<br>Output: Improved adaptability and robustness in 3D perception models 输出: 提高3D感知模型的适应性和稳健性 |
9.2 | [[9.2] 2503.13952 SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model](https://arxiv.org/abs/2503.13952) <br> [{'name': 'Xinqing Li, Ruiqi Song, Qingyu Xie, Ye Wu, Nanxin Zeng, Yunfeng Ai'}] | Autonomous Driving 自动驾驶 | v2<br>simulator-conditioned scene generation<br>autonomous driving<br>data generation | Input: Simulation conditions based on real-world data 真实数据的模拟条件<br>Step1: Scene simulation for data generation 场景模拟以生成数据<br>Step2: Label alignment with real-world conditions 标签与真实世界条件的对齐<br>Step3: Benchmark evaluation for generated data 生成数据的基准评估<br>Output: Large-scale diverse datasets for autonomous driving applications 大规模多样化数据集，用于自动驾驶应用 |
9.2 | [[9.2] 2503.13982 A-SCoRe: Attention-based Scene Coordinate Regression for wide-ranging scenarios](https://arxiv.org/abs/2503.13982) <br> [{'name': 'Huy-Hoang Bui, Bach-Thuan Bui, Quang-Vinh Tran, Yasuyuki Fujii, Joo-Ho Lee'}] | Visual Localization 视觉定位 | v2<br>scene coordinate regression<br>visual localization<br>robotics | Input: Images from multiple modalities 多种模式的图像<br>Step1: Descriptor extraction 描述符提取<br>Step2: Attention-based scene coordinate regression 基于注意力的场景坐标回归<br>Step3: Camera pose estimation 相机姿态估计<br>Output: Estimated camera poses 估计的相机姿态 |
9.2 | [[9.2] 2503.14493 State Space Model Meets Transformer: A New Paradigm for 3D Object Detection](https://arxiv.org/abs/2503.14493) <br> [{'name': 'Chuxin Wang, Wenfei Yang, Xiang Liu, Tianzhu Zhang'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection<br>state space model<br>transformer | Input: 3D point clouds 3D点云<br>Step1: Model state-dependent parameters 模型状态依赖参数<br>Step2: Implement interaction mechanisms 实现互动机制<br>Step3: Conduct experiments on datasets 在数据集上进行实验<br>Output: Enhanced object detection performance 改进的目标检测性能 |
9.2 | [[9.2] 2503.14498 Tracking Meets Large Multimodal Models for Driving Scenario Understanding](https://arxiv.org/abs/2503.14498) <br> [{'name': 'Ayesha Ishaq, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer'}] | Autonomous Driving 自动驾驶 | v2<br>Large Multimodal Models<br>Autonomous Driving<br>3D Spatial Understanding | Input: Tracking information and visual data 跟踪信息和视觉数据<br>Step1: Integrate tracking data into Large Multimodal Models (LMMs) 将跟踪数据集成到大型多模态模型中<br>Step2: Self-supervised pretraining of the tracking encoder 跟踪编码器的自监督预训练<br>Step3: Enhance perception, planning, and prediction tasks 增强感知、规划和预测任务<br>Output: Improved decision-making in dynamic driving environments 输出：在动态驾驶环境中改善决策 |
8.5 | [[8.5] 2503.13778 Using 3D reconstruction from image motion to predict total leaf area in dwarf tomato plants](https://arxiv.org/abs/2503.13778) <br> [{'name': 'Dmitrii Usenko, David Helman, Chen Giladi'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>leaf area estimation<br>machine learning<br>precision agriculture | Input: Sequential 3D reconstructions from RGB images 从RGB图像的序列3D重建<br>Step1: Data integration 数据集成<br>Step2: 3D reconstruction algorithms development 3D重建算法开发<br>Step3: Leaf area estimation leaf area estimation 叶面积估计<br>Output: Estimated total leaf area (TLA) 估计的总叶面积(TLA) |
8.5 | [[8.5] 2503.13792 Identifying and Mitigating Position Bias of Multi-image Vision-Language Models](https://arxiv.org/abs/2503.13792) <br> [{'name': 'Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang'}] | VLM & VLA 视觉语言模型与对齐 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>Position Bias 位置偏差<br>Multi-Image Reasoning 多图像推理 | Input: Multi-image inputs 多图像输入<br>Step1: Introduce Position-wise Question Answering (PQA) 引入位置敏感问答任务<br>Step2: Analyze position bias 分析位置偏差<br>Step3: Propose SoFt Attention (SoFA) 提出SoFt Attention方法<br>Output: Mitigated position bias 减轻位置偏差 |
8.5 | [[8.5] 2503.13858 MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations](https://arxiv.org/abs/2503.13858) <br> [{'name': 'Hongyu Ke, Jack Morris, Kentaro Oguchi, Xiaofei Cao, Yongkang Liu, Haoxin Wang, Yi Ding'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D visual perception<br>autonomous driving<br>bird's-eye view | Input: Multi-camera images 多摄像头图像<br>Step1: Spatial Cross Mamba integration 空间交叉Mamba集成<br>Step2: Unified BEV representation generation 统一的BEV表示生成<br>Step3: Computational efficiency assessment 计算效率评估<br>Output: Enhanced BEV representation 改进的BEV表示 |
8.5 | [[8.5] 2503.13891 Where do Large Vision-Language Models Look at when Answering Questions?](https://arxiv.org/abs/2503.13891) <br> [{'name': 'Xiaoying Xing, Chia-Wen Kuo, Li Fuxin, Yulei Niu, Fan Chen, Ming Li, Ying Wu, Longyin Wen, Sijie Zhu'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>Vision-Language Models 视觉语言模型<br>visual attention 视觉关注<br>multimodal tasks 多模态任务 | Input: Large Vision-Language Models (LVLMs) 大型视觉语言模型<br>Step1: Extend heatmap visualization methods 扩展热图可视化方法<br>Step2: Select visually relevant tokens 选择视觉相关标记<br>Step3: Conduct analysis on LVLMs 进行LVLM分析<br>Output: Insights into visual understanding and attention regions 输出：视觉理解和注意区域的洞察 |
8.5 | [[8.5] 2503.13926 Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation](https://arxiv.org/abs/2503.13926) <br> [{'name': 'Huan Ren, Wenfei Yang, Xiang Liu, Shifeng Zhang, Tianzhu Zhang'}] | 3D Pose Estimation 3D姿态估计 | v2<br>object pose estimation<br>spherical representations<br>3D reconstruction | Input: Observed object points 观察目标点<br>Step1: Feature extraction 特征提取<br>Step2: Spherical projection to HEALPix grids 将点投影到HEALPix网格<br>Step3: Correspondence prediction 对应关系预测<br>Output: Predict object pose and size 预测目标的姿态和尺寸 |
8.5 | [[8.5] 2503.13938 ChatBEV: A Visual Language Model that Understands BEV Maps](https://arxiv.org/abs/2503.13938) <br> [{'name': 'Qingyao Xu, Siheng Chen, Guang Chen, Yanfeng Wang, Ya Zhang'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>BEV maps<br>traffic scene understanding<br>Vision-Language Models<br>autonomous driving | Input: BEV maps (Bird's-Eye View maps) BEV地图<br>Step1: Dataset construction using novel collection pipeline 数据集构建使用新收集管道<br>Step2: Fine-tune vision-language model ChatBEV on the dataset 在数据集上微调视觉语言模型ChatBEV<br>Step3: Implement language-driven traffic scene generation pipeline 实施语言驱动的交通场景生成管道<br>Output: Enhanced understanding and generation of traffic scenarios 改进的交通场景理解与生成 |
8.5 | [[8.5] 2503.13946 Is Discretization Fusion All You Need for Collaborative Perception?](https://arxiv.org/abs/2503.13946) <br> [{'name': 'Kang Yang, Tianci Bu, Lantao Li, Chunxu Li, Yongcai Wang, Deying Li'}] | Autonomous Systems and Robotics 自动驾驶与机器人 | v2<br>Collaborative perception 协作感知<br>3D object detection 三维物体检测 | Input: Features from multi-view images 由多视角图像提取的特征<br>Step1: Generate anchor proposals 生成锚点提案<br>Step2: Select confident features 选择自信特征<br>Step3: Perform local-global fusion 执行局部-全局融合<br>Output: Enhanced object detection improvements 改进的物体检测结果 |
8.5 | [[8.5] 2503.13951 FrustumFusionNets: A Three-Dimensional Object Detection Network Based on Tractor Road Scene](https://arxiv.org/abs/2503.13951) <br> [{'name': 'Lili Yang, Mengshuai Chang, Xiao Guo, Yuxin Feng, Yiwen Mei, Caicong Wu'}] | 3D Object Detection 三维对象检测 | v2<br>3D object detection 三维对象检测<br>frustum-based methods 棱锥法<br>agricultural machinery 农业机械 | Input: Multi-source sensor data (LiDAR and camera) 输入: 多源传感器数据（激光雷达和相机）<br>Step1: Generate 2D object detection results to narrow search areas in 3D point cloud 第一步: 生成二维对象检测结果以缩小三维点云的搜索区域<br>Step2: Apply Gaussian mask to enhance point cloud information 第二步: 应用高斯掩模以增强点云信息<br>Step3: Extract features from both frustum point cloud and crop images 第三步: 从棱锥点云和作物图像中提取特征<br>Output: Concatenated features for 3D object detection 输出: 用于三维对象检测的连接特征 |
8.5 | [[8.5] 2503.14001 Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body Dimensions and Weight](https://arxiv.org/abs/2503.14001) <br> [{'name': 'Yi Xiao, Qiannan Han, Guiping Liang, Hongyan Zhang, Song Wang, Zhihao Xu, Weican Wan, Chuang Li, Guitao Jiang, Wenbo Xiao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point clouds<br>multimodal data<br>deep learning<br>weight estimation<br>body dimension prediction | Input: 2D RGB images, depth images, 3D point clouds from multiple views 2D RGB图像、深度图像和来自多个视角的3D点云<br>Step1: Data collection and preprocessing 数据收集与预处理<br>Step2: Feature extraction using PointNet++ 特征提取使用PointNet++<br>Step3: Fusion of 2D and 3D features 2D和3D特征融合<br>Step4: Model training and evaluation 模型训练与评估<br>Output: Predicted body dimensions and weight 预测的体型尺寸和体重 |
8.5 | [[8.5] 2503.14097 SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human Pose Estimation](https://arxiv.org/abs/2503.14097) <br> [{'name': 'Weihong Chen, Xuemiao Xu, Haoxin Yang, Yi Xie, Peng Xiao, Cheng Xu, Huaidong Zhang, Pheng-Ann Heng'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human pose estimation<br>knowledge distillation | Input: Multi-frame input sequences 多帧输入序列<br>Step1: Sparse correlation input sequence downsampling 稀疏相关输入序列下采样<br>Step2: Dynamic joint spatial attention distillation 动态关节空间注意力蒸馏<br>Step3: Temporal consistency distillation 时间一致性蒸馏<br>Output: Accurate 3D human pose predictions 精确的三维人体姿态预测 |
8.5 | [[8.5] 2503.14154 RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using Radial Basis Function Interpolation](https://arxiv.org/abs/2503.14154) <br> [{'name': 'Zhang Chen, Shuai Wan, Siyu Ren, Fuzheng Yang, Mengting Yu, Junhui Hou'}] | Point Cloud Processing 点云处理 | v2<br>point cloud<br>quality assessment<br>perceptual quality<br>compression | Input: Distorted point clouds 失真点云<br>Step1: Convert discrete point features to continuous feature function 将离散点特征转换为连续特征函数<br>Step2: Establish bijective feature sets 建立双射特征集<br>Step3: Evaluate perceptual quality 评估感知质量<br>Output: Enhanced quality assessment 改进的质量评估 |
8.5 | [[8.5] 2503.14171 Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images](https://arxiv.org/abs/2503.14171) <br> [{'name': 'Simon Niedermayr, Christoph Neuhauser R\\"udiger Westermann'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>image upscaling<br>novel view synthesis | Input: Low-resolution 3D Gaussian Splatting renderings 低分辨率3D高斯点云渲染<br>Step1: Image gradient analysis 图像梯度分析<br>Step2: Gradient-based bicubic spline interpolation 基于梯度的双三次样条插值<br>Step3: Integration into 3DGS optimization 将其集成到3DGS优化中<br>Output: High-resolution images with enhanced quality 高分辨率图像和增强质量 |
8.5 | [[8.5] 2503.14244 Deep Unsupervised Segmentation of Log Point Clouds](https://arxiv.org/abs/2503.14244) <br> [{'name': 'Fedor Zolotarev, Tuomas Eerola, Tomi Kauppi'}] | Point Cloud Processing 点云处理 | v2<br>point cloud segmentation<br>timber logs<br>3D reconstruction | Input: Surface point clouds 表面点云<br>Step1: Unsupervised segmentation 无监督分割<br>Step2: Geometrical property analysis 几何属性分析<br>Step3: Model evaluation 模型评估<br>Output: Accurate log surface points 准确的日志表面点 |
8.5 | [[8.5] 2503.14359 ImViD: Immersive Volumetric Videos for Enhanced VR Engagement](https://arxiv.org/abs/2503.14359) <br> [{'name': 'Zhengxian Yang, Shi Pan, Shengqi Wang, Haoxiang Wang, Li Lin, Guanjun Li, Zhengqi Wen, Borong Lin, Jianhua Tao, Tao Yu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>immersive volumetric videos<br>3D reconstruction<br>multi-view capture | Input: Multi-view, multi-modal audio-video data 多视角， 多模态音视频数据<br>Step1: Data capture 进行数据捕获<br>Step2: Benchmarking existing methods 对现有方法进行基准测试<br>Step3: Developing a pipeline for reconstruction 开发重建管道<br>Output: Immersive volumetric videos 生成沉浸式体积视频 |
8.5 | [[8.5] 2503.14405 DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers](https://arxiv.org/abs/2503.14405) <br> [{'name': 'Mert Bulent Sariyildiz, Philippe Weinzaepfel, Thomas Lucas, Pau de Jorge, Diane Larlus, Yannis Kalantidis'}] | 3D Understanding 3D理解 | v2<br>heterogeneous teacher distillation<br>depth estimation<br>3D understanding | Input: Various heterogeneous teacher models 诸多异构教师模型<br>Step1: Define heterogeneous teacher distillation 定义异构教师蒸馏<br>Step2: Explore data-sharing strategies 探索数据共享策略<br>Step3: Design and evaluate the projector architecture 设计并评估投影器架构<br>Output: Universal encoder capable of 2D and 3D tasks 能够进行2D和3D任务的通用编码器 |
8.5 | [[8.5] 2503.14489 Stable Virtual Camera: Generative View Synthesis with Diffusion Models](https://arxiv.org/abs/2503.14489) <br> [{'name': 'Jensen (Jinghao),  Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, Varun Jampani'}] | Multi-view and Stereo Vision 多视角和立体视觉 | v2<br>Novel View Synthesis 新视图合成<br>Diffusion Models 扩散模型<br>3D Reconstruction 三维重建 | Input: Any number of input views and target cameras 任意数量的输入视图和目标相机<br>Step1: Model design 模型设计<br>Step2: Training strategy 训练策略<br>Step3: Sampling method 采样方法<br>Output: Novel views of a scene 场景的新视图 |
8.5 | [[8.5] 2503.14492 Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control](https://arxiv.org/abs/2503.14492) <br> [{'name': 'NVIDIA,  :, Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, Dieter Fox, Yunhao Ge, Jinwei Gu, Ali Hassani, Michael Isaev, Pooya Jannaty, Shiyi Lan, Tobias Lasser, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Fabio Ramos, Xuanchi Ren, Tianchang Shen, Shitao Tang, Ting-Chun Wang, Jay Wu, Jiashu Xu, Stella Xu, Kevin Xie, Yuchong Ye, Xiaodong Yang, Xiaohui Zeng, Yu Zeng'}] | Image and Video Generation 图像生成 | v2<br>world generation<br>diffusion models<br>autonomous driving<br>robotics | Input: Conditional multi-modal inputs (segmentation, depth, edge) 条件多模态输入（分割，深度，边缘）<br>Step1: Adaptive weighting of conditional inputs 自适应加权条件输入<br>Step2: World generation using Conditional Diffusion Model 使用条件扩散模型生成世界<br>Output: Real-time world simulations 实时世界模拟 |
8.5 | [[8.5] 2503.14501 Advances in 4D Generation: A Survey](https://arxiv.org/abs/2503.14501) <br> [{'name': 'Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Yawei Luo'}] | Image and Video Generation 图像生成与视频生成 | v2<br>4D generation<br>autonomous driving<br>dynamic modeling | Input: 4D data representations 4D数据表示<br>Step1: Survey of existing technologies 现有技术的调查<br>Step2: Literature review 文献综述<br>Step3: Challenges and opportunities analysis 挑战与机遇分析<br>Output: Comprehensive understanding of 4D generation 4D生成的全面理解 |
8.0 | [[8.0] 2503.13652 Web Artifact Attacks Disrupt Vision Language Models](https://arxiv.org/abs/2503.13652) <br> [{'name': 'Maan Qraitem, Piotr Teterwak, Kate Saenko, Bryan A. Plummer'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>artifact attacks<br>model robustness | Input: Vision-language models (VLMs) 视觉语言模型<br>Step1: Identify artifact-based attacks 识别伪影攻击<br>Step2: Develop automated mining pipeline 开发自动化挖掘管道<br>Step3: Optimize attacks and evaluate effectiveness 优化攻击并评估有效性<br>Output: Enhanced understanding of model vulnerabilities 改进对模型脆弱性的理解 |
8.0 | [[8.0] 2503.13939 Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2503.13939) <br> [{'name': 'Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Medical Imaging<br>Reinforcement Learning | Input: Medical imaging data 医学图像数据<br>Step1: Implement reinforcement learning framework 实施强化学习框架<br>Step2: Optimize reasoning paths using GRPO 优化推理路径使用GRPO<br>Step3: Evaluate model across different imaging modalities 评估模型在不同成像模式下的性能<br>Output: Enhanced generalization and trustworthiness 增强的泛化和可信性 |
7.5 | [[7.5] 2503.13966 FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks](https://arxiv.org/abs/2503.13966) <br> [{'name': 'Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Longteng Guo, Zhihua Wei, Jing Liu'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>Vision-and-Language Navigation<br>Large Language Models | Input: Visual input and natural language instructions 视觉输入与自然语言指令<br>Step1: Generate high-level navigation plan 生成高层导航计划<br>Step2: Validate guidance feasibility 验证指导的可行性<br>Step3: Execute navigation actions 执行导航动作<br>Output: Target location reached 到达目标位置 |
7.5 | [[7.5] 2503.14161 CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models](https://arxiv.org/abs/2503.14161) <br> [{'name': 'Yiqi Zhu, Ziyue Wang, Can Zhang, Peng Li, Yang Liu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Continuous Space Perception<br>Spatial Reasoning | Input: Multi-image sequences 多图像序列<br>Step1: Define continuous space perception 定义连续空间感知<br>Step2: Develop benchmark tasks 开发基准任务<br>Step3: Evaluate models across tasks 在任务中评估模型<br>Output: Performance metrics 绩效指标 |
7.5 | [[7.5] 2503.14277 Towards synthetic generation of realistic wooden logs](https://arxiv.org/abs/2503.14277) <br> [{'name': 'Fedor Zolotarev, Borek Reich, Tuomas Eerola, Tomi Kauppi, Pavel Zemcik'}] | 3D Generation 三维生成 | v2<br>3D representation<br>synthetic generation<br>wooden logs | Input: Specifications of wooden logs 木材参数<br>Step1: Internal knot generation 内部结的生成<br>Step2: Centerline generation 中心线的生成<br>Step3: Surface generation 表面生成<br>Output: Realistic 3D models of wooden logs 逼真的木材三维模型 |
7.5 | [[7.5] 2503.14402 Diffusion-based Facial Aesthetics Enhancement with 3D Structure Guidance](https://arxiv.org/abs/2503.14402) <br> [{'name': 'Lisha Li, Jingwen Hou, Weide Liu, Yuming Fang, Jiebin Yan'}] | Image Generation 图像生成 | v2<br>Facial Aesthetics Enhancement<br>3D structure guidance<br>Diffusion model<br>Facial beautification | Input: 2D facial images 2D面部图像<br>Step1: Nearest Neighbor Face Searching (NNFS) module 寻找最近邻面孔<br>Step2: Facial Guidance Extraction (FGE) module 提取面部引导<br>Step3: Face Beautification (FB) module 面部美化<br>Output: Enhanced facial images 改进的面部图像 |
7.0 | [[7.0] 2503.14075 Growing a Twig to Accelerate Large Vision-Language Models](https://arxiv.org/abs/2503.14075) <br> [{'name': 'Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models 视觉语言模型<br>VLM acceleration VLM加速<br>Token pruning 标记修剪 | Input: Base VLM architecture 基础视觉语言模型架构<br>Step1: Twig-guided token pruning twig引导的标记修剪<br>Step2: Self-speculative decoding 自我推测解码<br>Output: Accelerated VLM performance 加速的视觉语言模型性能 |


## Arxiv 2025-03-12

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.07739 SIRE: SE(3) Intrinsic Rigidity Embeddings](https://arxiv.org/abs/2503.07739) <br> [{'name': 'Cameron Smith, Basile Van Hoorick, Vitor Guizilini, Yue Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dynamic scene reconstruction<br>self-supervised learning | Input: Videos from casual scenes 来自休闲场景的视频<br>Step1: Estimate scene rigidity and geometry 估计场景刚性与几何<br>Step2: Use a least-squares solver to lift 2D trajectories into SE(3) tracks 使用最小二乘解算器将2D轨迹提升至SE(3)轨迹<br>Step3: Re-project back to 2D and compare against original trajectories 重新投影回2D并与原始轨迹比较<br>Output: Rigid scene structure and embeddings 刚性场景结构及嵌入 |
9.5 | [[9.5] 2503.07743 SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration](https://arxiv.org/abs/2503.07743) <br> [{'name': 'Michael Adlerstein, Jo\\~ao Carlos Virgolino Soares, Angelo Bratta, Claudio Semini'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud registration<br>3D modeling | Input: Point cloud data 点云数据<br>Step1: Initial outlier detection 初始异常值检测<br>Step2: Robust optimization with GNC 采用GNC的稳健优化<br>Step3: Splitting strategy implementation 拆分策略实现<br>Output: Accurate point cloud alignment 准确的点云对齐 |
9.5 | [[9.5] 2503.07819 POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality](https://arxiv.org/abs/2503.07819) <br> [{'name': 'Joey Wilson, Marcelino Almeida, Sachit Mahajan, Martin Labrie, Maani Ghaffari, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnab Sen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>active perception<br>uncertainty quantification | Input: Multi-view images 多视角图像<br>Step1: Derivation of covariance matrix 协方差矩阵的推导<br>Step2: Application of optimal experimental design 最优实验设计的应用<br>Step3: Quantification of information gain 信息增益的量化<br>Output: Enhanced strategies for 3D Gaussian Splatting 改进的三维高斯点云策略 |
9.5 | [[9.5] 2503.07828 Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments](https://arxiv.org/abs/2503.07828) <br> [{'name': 'Andrei Chubarau, Yinan Wang, James J. Clark'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>visual attention<br>3D environments<br>gaze prediction | Input: 2D images of a 3D scene 3D场景的2D图像<br>Step1: NeRF training NeRF训练<br>Step2: Gaze prediction network training 注视预测网络训练<br>Step3: Gaze visualization and mapping to 3D structure 注意力可视化和3D结构映射<br>Output: Visual attention patterns and rendered images 可视化注意模式和渲染图像 |
9.5 | [[9.5] 2503.07874 Topology-Preserving Loss for Accurate and Anatomically Consistent Cardiac Mesh Reconstruction](https://arxiv.org/abs/2503.07874) <br> [{'name': 'Chenyu Zhang, Yihao Luo, Yinzhe Wu, Choon Hwai Yap, Guang Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>cardiac mesh reconstruction<br>topology-preserving loss | Input: Volumetric data 体积数据<br>Step1: Identify topology-violating points 确定违反拓扑结构的点<br>Step2: Apply Topology-Preserving Mesh Loss 应用拓扑保护网格损失<br>Step3: Perform mesh deformation and optimization 执行网格形变与优化<br>Output: Accurate and anatomically consistent cardiac meshes 准确且解剖上一致的心脏网格 |
9.5 | [[9.5] 2503.07940 BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes](https://arxiv.org/abs/2503.07940) <br> [{'name': 'Minkyun Seo, Hyungtae Lim, Kanghee Lee, Luca Carlone, Jaesik Park'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Registration 点云注册<br>Generalization 泛化能力<br>Zero-Shot Learning 零样本学习 | Input: Point cloud data 点云数据<br>Step1: Identify limitations of existing methods 识别现有方法的局限性<br>Step2: Develop zero-shot registration framework 开发零样本注册框架<br>Step3: Implement adaptive voxel size and search radii 实现自适应体素大小和搜索半径<br>Output: Robust point cloud registration pipeline 稳健的点云注册管道 |
9.5 | [[9.5] 2503.07952 NeRF-VIO: Map-Based Visual-Inertial Odometry with Initialization Leveraging Neural Radiance Fields](https://arxiv.org/abs/2503.07952) <br> [{'name': 'Yanyu Zhang, Dongming Wang, Jie Xu, Mengyuan Liu, Pengxiang Zhu, Wei Ren'}] | 3D Reconstruction and Modeling  三维重建 | v2<br>visual-inertial odometry<br>neural radiance fields<br>augmented reality | Input: Captured images and pre-trained NeRF model 采集的图像和预训练的NeRF模型<br>Step1: Initialize first IMU state 初始化第一个IMU状态<br>Step2: Define loss function based on geodesic distance 构建基于测地距离的损失函数<br>Step3: Integrate captured and rendered images 更新状态<br>Output: Updated poses and NeRF-based rendering 更新的位姿和基于NeRF的渲染 |
9.5 | [[9.5] 2503.08005 CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction](https://arxiv.org/abs/2503.08005) <br> [{'name': 'Zhiyuan Wu, Xibin Song, Senbo Wang, Weizhe Liu, Jiayu Yang, Ziang Cheng, Shenzhou Chen, Taizhang Shang, Weixuan Sun, Shan Luo, Pan Ji'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image-to-3D generation<br>multi-view consistency<br>2D diffusion models | Input: Single RGB image 单个RGB图像<br>Step1: Generate main views using a 2D diffusion model 使用2D扩散模型生成主要视图<br>Step2: Apply Dense View Interpolation (DVI) for additional view synthesis 使用密集视图插值（DVI）进行附加视图合成<br>Step3: Tri-plane-based mesh reconstruction to create 3D mesh 使用三平面网格重建创建3D网格<br>Output: High-quality 3D meshes with improved texture and geometry 输出: 具有改善纹理和几何形状的高质量3D网格 |
9.5 | [[9.5] 2503.08092 SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection](https://arxiv.org/abs/2503.08092) <br> [{'name': 'Hyeongseok Son, Jia He, Seung-In Park, Ying Min, Yunhao Zhang, ByungIn Yoo'}] | 3D Object Detection 三维物体检测 | v2<br>3D Object Detection<br>Sparse Voxel Features<br>Autonomous Driving | Input: Multi-modal data (LiDAR and camera) 多模态数据（LiDAR和相机）<br>Step1: Feature Extraction 特征提取<br>Step2: Sparse Voxel Representation 稀疏体素表示<br>Step3: Transformer-based Detection 基于变压器的检测<br>Output: Detected 3D objects 检测到的三维物体 |
9.5 | [[9.5] 2503.08093 MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface Reconstruction](https://arxiv.org/abs/2503.08093) <br> [{'name': 'Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, Gim Hee Lee'}] | Surface Reconstruction 表面重建 | v2<br>3D reconstruction<br>Gaussian splatting<br>surface reconstruction<br>multi-view consistency | Input: Multi-view images 多视角图像<br>Step1: Feature extraction 特征提取<br>Step2: Distractor masking distractor mask generation distractor 遮罩生成<br>Step3: Gaussian pruning 高斯剪枝<br>Step4: Surface reconstruction 表面重建<br>Output: Enhanced 3D models 改进的三维模型 |
9.5 | [[9.5] 2503.08135 ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2503.08135) <br> [{'name': 'Junfu Guo, Yu Xin, Gaoyi Liu, Kai Xu, Ligang Liu, Ruizhen Hu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>articulated objects<br>3D reconstruction<br>self-supervised learning<br>digital twins | Input: Multi-view imagery of articulated objects 多视角图像<br>Step 1: Concurrent part-level reconstruction 部件级同时重建<br>Step 2: Multi-step optimization of parameters 多步骤参数优化<br>Step 3: Model formation using 3D Gaussian representations 使用3D高斯模型形成<br>Output: Digital twins of articulated objects in 3D digital format 输出: 3D数字格式的物体数字双胞胎 |
9.5 | [[9.5] 2503.08140 HOTFormerLoc: Hierarchical Octree Transformer for Versatile Lidar Place Recognition Across Ground and Aerial Views](https://arxiv.org/abs/2503.08140) <br> [{'name': 'Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, Milad Ramezani'}] | 3D Place Recognition 3D位置识别 | v2<br>Lidar place recognition<br>3D reconstruction<br>autonomous systems | Input: Lidar point cloud data 激光雷达点云数据<br>Step1: Octree-based multi-scale attention mechanism 八叉树多尺度注意机制<br>Step2: Relay tokens for efficient communication 采用中继标记以提高通信效率<br>Step3: Pyramid attentional pooling for global descriptor synthesis 采用金字塔注意池化以合成全局描述符<br>Output: Robust global descriptors for place recognition 输出: 用于位置识别的鲁棒全局描述符 |
9.5 | [[9.5] 2503.08142 A Framework for Reducing the Complexity of Geometric Vision Problems and its Application to Two-View Triangulation with Approximation Bounds](https://arxiv.org/abs/2503.08142) <br> [{'name': 'Felix Rydell, Georg B\\"okman, Fredrik Kahl, Kathl\\\'en Kohn'}] | Structure from Motion (SfM) 运动结构估计 | v2<br>3D reconstruction<br>triangulation<br>Structure-from-Motion | Input: Noisy 2D projections from multiple images 多个图像的噪声2D投影<br>Step1: Cost function reweighting 代价函数重加权<br>Step2: Simplification of polynomial degree to improve efficiency 简化多项式的程度以提高效率<br>Step3: Derive optimal weighting strategies 推导最佳加权策略<br>Output: Closed-form solution for triangulation 闭式解的三角测量 |
9.5 | [[9.5] 2503.08208 Explaining Human Preferences via Metrics for Structured 3D Reconstruction](https://arxiv.org/abs/2503.08208) <br> [{'name': 'Jack Langerman, Denys Rozumnyi, Yuzhong Huang, Dmytro Mishkin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>metrics<br>human preferences | Input: Structured 3D reconstructions 结构三维重建<br>Step1: Evaluate automated metrics 评估自动化度量<br>Step2: Analyze human preferences 分析人类偏好<br>Step3: Propose metrics and recommendations 提出度量和建议<br>Output: Improved metric for 3D reconstructions 改进的三维重建度量 |
9.5 | [[9.5] 2503.08217 S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction](https://arxiv.org/abs/2503.08217) <br> [{'name': 'Guangting Zheng, Jiajun Deng, Xiaomeng Chu, Yu Yuan, Houqiang Li, Yanyong Zhang'}] | 3D Reconstruction and Modeling 3D重建与建模 | v2<br>3D reconstruction<br>street scene | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model evaluation 模型评估<br>Output: Streamlined reconstruction pipeline 精简的重建管线 |
9.5 | [[9.5] 2503.08218 MVD-HuGaS: Human Gaussians from a Single Image via 3D Human Multi-view Diffusion Prior](https://arxiv.org/abs/2503.08218) <br> [{'name': 'Kaiqiang Xiong, Ying Feng, Qi Zhang, Jianbo Jiao, Yang Zhao, Zhihao Liang, Huachen Gao, Ronggang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human reconstruction<br>multi-view diffusion model | Input: Single image 单张图像<br>Step1: Generate multi-view images from a single reference image 从单个参考图像生成多视角图像<br>Step2: Introduce an alignment module for camera poses 引入相机位姿对齐模块<br>Step3: Optimize 3D Gaussians and refine facial regions 优化3D高斯，并细化面部区域<br>Output: High-fidelity free-view 3D human rendering 输出：高保真自由视图3D人类渲染 |
9.5 | [[9.5] 2503.08219 CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning](https://arxiv.org/abs/2503.08219) <br> [{'name': 'Kaiqiang Xiong, Rui Peng, Zhe Zhang, Tianxing Feng, Jianbo Jiao, Feng Gao, Ronggang Wang'}] | Multi-view Stereo 多视角立体 | v2<br>3D reconstruction<br>Multi-view Stereo<br>contrastive learning<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Integrate dual-level contrastive learning 双层对比学习集成<br>Step2: Implement image-level contrastive loss 实现图像级对比损失<br>Step3: Implement scene-level contrastive loss 实现场景级对比损失<br>Step4: L0.5 photometric consistency loss implementation L0.5光度一致性损失实现<br>Output: Enhanced depth estimation 改进的深度估计 |
9.5 | [[9.5] 2503.08224 HRAvatar: High-Quality and Relightable Gaussian Head Avatar](https://arxiv.org/abs/2503.08224) <br> [{'name': 'Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>head avatars | Input: Monocular video input 单胞视频输入<br>Step1: Optimize facial tracking through end-to-end training 优化面部追踪，采用端到端训练<br>Step2: Utilize learnable blendshapes for deformation 使用可学习的混合形状进行变形<br>Step3: Model head appearance using physical properties and shading techniques 使用物理属性和阴影技术建模头部外观<br>Output: High-fidelity, relightable 3D head avatars 输出：高保真、可照明的三维头部头像 |
9.5 | [[9.5] 2503.08336 Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving](https://arxiv.org/abs/2503.08336) <br> [{'name': 'Runwei Guan, Jianan Liu, Ningwei Ouyang, Daizong Liu, Xiaolou Sun, Lianqing Zheng, Ming Xu, Yutao Yue, Hui Xiong'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D visual grounding<br>LiDAR<br>radar<br>autonomous driving | Input: Dual-sensor inputs (LiDAR and radar) 双传感器输入 (激光雷达和雷达)<br>Step 1: Feature extraction from LiDAR and radar sensor 数据提取: 从激光雷达和雷达传感器提取特征<br>Step 2: Dual-sensor feature fusion using Bidirectional Agent Cross Attention (BACA) 双传感器特征融合: 使用双向代理交叉注意力 (BACA)<br>Step 3: Region localization using Dynamic Gated Graph Fusion (DGGF) 区域定位: 使用动态门控图融合 (DGGF)<br>Output: 3D visual grounding prediction 3D视觉定位预测 |
9.5 | [[9.5] 2503.08352 Mitigating Ambiguities in 3D Classification with Gaussian Splatting](https://arxiv.org/abs/2503.08352) <br> [{'name': 'Ruiqi Zhang, Hao Zhu, Jingyi Zhao, Qi Zhang, Xun Cao, Zhan Ma'}] | 3D Classification 3D 分类 | v2<br>3D classification<br>Gaussian Splatting<br>point clouds<br>ambiguity | Input: GS point cloud as input 输入: GS 点云<br>Step1: Analyze ambiguities in traditional point cloud 分析传统点云中的歧义<br>Step2: Implement Gaussian Splatting classification 实施高斯点云分类<br>Step3: Evaluate performance using a new dataset 通过新数据集评估性能<br>Output: Enhanced classification of 3D objects 输出: 改进的 3D 对象分类 |
9.5 | [[9.5] 2503.08363 Parametric Point Cloud Completion for Polygonal Surface Reconstruction](https://arxiv.org/abs/2503.08363) <br> [{'name': 'Zhaiyu Chen, Yuqing Wang, Liangliang Nan, Xiao Xiang Zhu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>point cloud completion<br>polygonal surfaces | Input: Incomplete point cloud 数据集<br>Step1: Infer parametric primitives 推断参数化原始体<br>Step2: Recover high-level geometric structures 恢复高层次几何结构<br>Step3: Construct polygonal surfaces from primitives 根据原始体构建多边形表面<br>Output: High-quality polygonal surface reconstruction 高质量多边形表面重建 |
9.5 | [[9.5] 2503.08382 Twinner: Shining Light on Digital Twins in a Few Snaps](https://arxiv.org/abs/2503.08382) <br> [{'name': 'Jesus Zarzar, Tom Monnier, Roman Shapovalov, Andrea Vedaldi, David Novotny'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>PBR<br>digital twins<br>autonomous systems | Input: Posed images 设定图像<br>Step1: Voxel-grid transformer transformation 体素网格转换<br>Step2: Photometric error minimization 照明误差最小化<br>Step3: Model evaluation and comparison 模型评估与比较<br>Output: 3D geometry and materials 三维几何与材料 |
9.5 | [[9.5] 2503.08407 WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images](https://arxiv.org/abs/2503.08407) <br> [{'name': 'Yansong Guo, Jie Hu, Yansong Qu, Liujuan Cao'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D segmentation<br>2D images<br>real-time systems | Input: 2D images of arbitrary 3D objects 多视角图像<br>Step1: Pre-processing: 2D mask feature construction 预处理：2D掩模特征构建<br>Step2: Dynamic Global Aligning (DGA) for accuracy improvement 动态全局对齐(DGA)来提升精度<br>Step3: Multi-view Group Mapping (MGM) for real-time segmentation 多视角组映射(MGM)实现实时分割<br>Output: Aligned 3D segmentation results 对齐的3D分割结果 |
9.5 | [[9.5] 2503.08422 JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data](https://arxiv.org/abs/2503.08422) <br> [{'name': 'Runjian Chen, Wenqi Shao, Bo Zhang, Shaoshuai Shi, Li Jiang, Ping Luo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object detection<br>LiDAR<br>simulation-to-real<br>autonomous driving | Input: LiDAR point clouds from real and simulated environments<br>Step1: Jittering augmentation to enhance sample efficiency<br>Step2: Utilize a domain-aware backbone for better feature extraction<br>Step3: Implement memory-based sectorized alignment loss to bridge the simulation-to-real gap<br>Output: Effective 3D object detection with minimal real labels |
9.5 | [[9.5] 2503.08511 PCGS: Progressive Compression of 3D Gaussian Splatting](https://arxiv.org/abs/2503.08511) <br> [{'name': 'Yihang Chen, Mengyao Li, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>progressive compression<br>novel view synthesis | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Progressive masking strategy progressive masking strategy<br>Step2: Progressive quantization approach progressive quantization method<br>Step3: Entropy coding enhancement entropy coding优化<br>Output: Compressed bitstream with fidelity 改进的压缩比特流 |
9.5 | [[9.5] 2503.08516 High-Quality 3D Head Reconstruction from Any Single Portrait Image](https://arxiv.org/abs/2503.08516) <br> [{'name': 'Jianfu Zhang, yujie Gao, Jiahui Zhan, Wentao Wang, Yiyi Zhang, Haohua Zhao, Liqing Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>portrait images<br>facial expressions | Input: Single portrait image 单幅肖像图像<br>Step1: Data collection 数据收集<br>Step2: Multi-view video generation 多视角视频生成<br>Step3: Identity and expression integration 身份和表情整合<br>Step4: 3D head reconstruction 3D头部重建<br>Output: High-quality 3D head models 高质量3D头部模型 |
9.5 | [[9.5] 2503.08594 3D Point Cloud Generation via Autoregressive Up-sampling](https://arxiv.org/abs/2503.08594) <br> [{'name': 'Ziqiao Meng, Qichao Wang, Zhipeng Zhou, Irwin King, Peilin Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud generation<br>autoregressive modeling<br>up-sampling | Input: 3D point clouds 3D 点云<br>Step1: Learn multi-scale discrete representations 学习多尺度离散表示<br>Step2: Train autoregressive transformer 训练自回归变换器<br>Step3: Generate point clouds 生成点云<br>Output: Refined 3D point clouds 精炼的 3D 点云 |
9.5 | [[9.5] 2503.08601 LiSu: A Dataset and Method for LiDAR Surface Normal Estimation](https://arxiv.org/abs/2503.08601) <br> [{'name': "Du\\v{s}an Mali\\'c, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger"}] | 3D Reconstruction 三维重建 | v2<br>LiDAR<br>surface normal estimation<br>autonomous driving<br>3D reconstruction | Input: LiDAR point clouds LiDAR点云<br>Step1: Generate synthetic dataset 生成合成数据集<br>Step2: Develop surface normal estimation method 开发表面法线估计方法<br>Step3: Evaluate model performance 评估模型性能<br>Output: Accurate surface normals for 3D reconstruction 改进的三维重建表面法线 |
9.5 | [[9.5] 2503.08639 GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection](https://arxiv.org/abs/2503.08639) <br> [{'name': "Du\\v{s}an Mali\\'c, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger"}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>domain generalization<br>LiDAR<br>Gaussian blobs<br>local geometry | Input: LiDAR point cloud data 激光雷达点云数据<br>Step1: Encode local point cloud neighborhoods using Gaussian blobs 使用高斯点云对局部点云邻域进行编码<br>Step2: Integrate the Gaussian blobs into existing detection frameworks 将高斯点云集成到现有检测框架中<br>Step3: Evaluate model performance on cross-domain benchmarks 在跨域基准测试中评估模型性能<br>Output: Enhanced detection accuracy in domain generalization 在领域泛化中提高检测精度 |
9.5 | [[9.5] 2503.08664 MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention](https://arxiv.org/abs/2503.08664) <br> [{'name': 'Yuhan Wang, Fangzhou Hong, Shuai Yang, Liming Jiang, Wayne Wu, Chen Change Loy'}] | 3D Generation 三维生成 | v2<br>3D generation<br>multiview diffusion<br>human modeling | Input: Frontal image of a human figure 人物的正面图像<br>Step1: Establish correspondences using rasterization 和投影建立对应关系<br>Step2: Introduce mesh attention to handle high resolution 引入网格注意力以处理高分辨率<br>Step3: Generate multiview images using the trained model 使用训练好的模型生成多视角图像<br>Output: Dense, view-consistent human images at megapixel resolution 输出：百万像素分辨率下的稠密一致人像图像 |
9.5 | [[9.5] 2503.08676 Language-Depth Navigated Thermal and Visible Image Fusion](https://arxiv.org/abs/2503.08676) <br> [{'name': 'Jinchang Zhang, Zijun Li, Guoyu Lu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image fusion<br>depth estimation<br>autonomous driving | Input: Infrared and visible images, along with depth information 输入: 红外和可见图像，及深度信息<br>Step1: Multi-channel feature extraction using a diffusion model 步骤1: 使用扩散模型进行多通道特征提取<br>Step2: Language-guided fusion with depth information 步骤2: 结合深度信息的语言指导融合<br>Step3: Depth estimation and optimization of the fusion network 步骤3: 深度估计并优化融合网络<br>Output: Enhanced color-fused images 输出: 改进的彩色融合图像 |
9.2 | [[9.2] 2503.07946 7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting](https://arxiv.org/abs/2503.07946) <br> [{'name': 'Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu'}] | Neural Rendering 神经渲染 | v2<br>real-time rendering<br>Gaussian Splatting<br>dynamic scenes | Input: Scene elements represented as 7D Gaussians 场景元素以7D高斯表示<br>Step1: Conditional slicing mechanism 逐步：条件切片机制<br>Step2: Joint optimization integration 联合优化集成<br>Step3: Rendering of dynamic scenes 渲染动态场景<br>Output: Real-time rendering with view-dependent effects 输出：支持视图依赖的实时渲染 |
9.2 | [[9.2] 2503.08101 Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning](https://arxiv.org/abs/2503.08101) <br> [{'name': 'Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection 3D目标检测<br>zero-shot pruning零样本剪枝<br>transformer decoders 变换器解码器 | Input: 3D object detection models 3D目标检测模型<br>Step1: Classification score extraction 分类评分提取<br>Step2: Importance score computation 重要性评分计算<br>Step3: Key pruning based on importance 依据重要性进行关键字剪枝<br>Output: Accelerated inference speed 加速的推理速度 |
9.0 | [[9.0] 2503.08373 nnInteractive: Redefining 3D Promptable Segmentation](https://arxiv.org/abs/2503.08373) <br> [{'name': 'Fabian Isensee, Maximilian Rokuss, Lars Kr\\"amer, Stefan Dinkelacker, Ashis Ravindran, Florian Stritzke, Benjamin Hamm, Tassilo Wald, Moritz Langenberg, Constantin Ulrich, Jonathan Deissler, Ralf Floca, Klaus Maier-Hein'}] | 3D Segmentation 三维分割 | v2<br>3D segmentation<br>interactive segmentation<br>volumetric data | Input: User prompts (points, scribbles, bounding boxes, lasso) 用户提示（点、涂鸦、边界框、套索）<br>Step1: Data integration from volumetric datasets 从体积数据集中进行数据集成<br>Step2: 3D interactive segmentation algorithm development 开发 3D 交互式分割算法<br>Step3: Integration into imaging platforms (e.g., Napari, MITK) 集成到成像平台（例如，Napari，MITK）<br>Output: Full 3D segmentations from 2D interactions 从 2D 交互生成完整的 3D 分割 |
9.0 | [[9.0] 2503.08471 TrackOcc: Camera-based 4D Panoptic Occupancy Tracking](https://arxiv.org/abs/2503.08471) <br> [{'name': 'Zhuoguang Chen, Kenan Li, Xiuyu Yang, Tao Jiang, Yiming Li, Hang Zhao'}] | Autonomous Systems and Robotics 自动驾驶及机器人技术 | v2<br>4D occupancy tracking<br>autonomous systems<br>3D tracking<br>camera-based perception | Input: Camera images 相机图像<br>Step1: Image feature extraction 图像特征提取<br>Step2: 4D panoptic queries integration 4D全景查询集成<br>Step3: Result prediction 结果预测<br>Output: Panoptic occupancy labels 全景占用标签 |
8.5 | [[8.5] 2503.07813 AgriField3D: A Curated 3D Point Cloud and Procedural Model Dataset of Field-Grown Maize from a Diversity Panel](https://arxiv.org/abs/2503.07813) <br> [{'name': 'Elvis Kimara, Mozhgan Hadadi, Jackson Godbersen, Aditya Balu, Talukder Jubery, Yawei Li, Adarsh Krishnamurthy, Patrick S. Schnable, Baskar Ganapathysubramanian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point clouds<br>agricultural research<br>maize | Input: 3D point clouds of maize plants 玉米植物的三维点云<br>Step1: Data collection 数据收集<br>Step2: Procedural model generation 程序模型生成<br>Step3: Graph-based segmentation 图基于的分割<br>Output: Curated dataset for agricultural research 为农业研究提供的整理数据集 |
8.5 | [[8.5] 2503.07829 Fixing the RANSAC Stopping Criterion](https://arxiv.org/abs/2503.07829) <br> [{'name': 'Johannes Sch\\"onberger, Viktor Larsson, Marc Pollefeys'}] | Multi-view and Stereo Vision 多视角立体视觉 | v2<br>RANSAC<br>3D reconstruction<br>robust estimation | Input: Noisy measurements 噪声测量<br>Step1: Analyze RANSAC sampling probability 分析RANSAC采样概率<br>Step2: Derive exact stopping criterion 推导精确停止准则<br>Step3: Evaluate model performance 评估模型性能<br>Output: Improved model estimation 改进的模型估计 |
8.5 | [[8.5] 2503.07909 FunGraph: Functionality Aware 3D Scene Graphs for Language-Prompted Scene Interaction](https://arxiv.org/abs/2503.07909) <br> [{'name': 'Dennis Rotondi, Fabio Scaparro, Hermann Blum, Kai O. Arras'}] | 3D Scene Graphs 3D场景图 | v2<br>3D scene graphs<br>functional interactive elements<br>robot perception<br>affordance grounding | Input: Multi-view RGB-D images 多视角RGB-D图像<br>Step1: Detect functional elements 检测功能性元件<br>Step2: Augment 3D scene graph generation 扩展3D场景图生成<br>Step3: Evaluate functional segmentation 评估功能分割<br>Output: Enhanced 3D scene graphs 改进的3D场景图 |
8.5 | [[8.5] 2503.07933 From Slices to Sequences: Autoregressive Tracking Transformer for Cohesive and Consistent 3D Lymph Node Detection in CT Scans](https://arxiv.org/abs/2503.07933) <br> [{'name': 'Qinji Yu, Yirui Wang, Ke Yan, Dandan Zheng, Dashan Ai, Dazhou Guo, Zhanghexuan Ji, Yanzhou Su, Yun Bian, Na Shen, Xiaowei Ding, Le Lu, Xianghua Ye, Dakai Jin'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>autonomous driving | Input: 3D CT scans<br>Step1: Transform slice-based detection to a tracking task<br>Step2: Develop a transformer decoder for tracking and detection<br>Step3: Evaluate 3D instance association<br>Output: Enhanced lymph node detection in 3D CT scans |
8.5 | [[8.5] 2503.07939 STRMs: Spatial Temporal Reasoning Models for Vision-Based Localization Rivaling GPS Precision](https://arxiv.org/abs/2503.07939) <br> [{'name': 'Hin Wai Lui, Jeffrey L. Krichmar'}] | Localization and Navigation 本地化与导航 | v2<br>vision-based localization<br>3D reconstruction<br>autonomous navigation | Input: First-person perspective observations (FPP)  第一定义观察<br>Step 1: Data transformation to global map perspective (GMP)  数据转化为全景视图<br>Step 2: Model training using VAE-RNN and VAE-Transformer  使用VAE-RNN和VAE-Transformer进行模型训练<br>Step 3: Performance evaluation in real-world environments  在真实环境中评估性能<br>Output: Precise geographical coordinates and localization capabilities  精确的地理坐标和定位能力 |
8.5 | [[8.5] 2503.07942 STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications](https://arxiv.org/abs/2503.07942) <br> [{'name': 'Andrew Gao, Jun Liu'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>anomaly detection<br>autonomous driving | Input: Video data 视频数据<br>Step1: Anomaly detection algorithm development 异常检测算法开发<br>Step2: Feature extraction 特征提取<br>Step3: Model evaluation 模型评估<br>Output: Anomalies identified anomalies identified |
8.5 | [[8.5] 2503.08016 SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2503.08016) <br> [{'name': 'Akshat Ghiya, Ali K. AlShami, Jugal Kalita'}] | Autonomous Driving 自动驾驶 | v2<br>pedestrian trajectory prediction<br>autonomous driving<br>pose estimation<br>skeleton information | Input: Video data 视频数据<br>Step1: Extract skeleton information using ViTPose 从ViTPose提取骨骼信息<br>Step2: Compute joint angles based on skeleton data 根据骨骼数据计算关节角度<br>Step3: Integrate pose information with bounding box data 将姿态信息与边界框数据集成<br>Step4: Apply temporal data augmentation for improved performance 进行时间数据增强以提高性能<br>Output: Predicted pedestrian trajectories 预测行人轨迹 |
8.5 | [[8.5] 2503.08068 Simulating Automotive Radar with Lidar and Camera Inputs](https://arxiv.org/abs/2503.08068) <br> [{'name': 'Peili Song, Dezhen Song, Yifan Yang, Enfan Lan, Jingtai Liu'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Automotive radar<br>Autonomous driving<br>Data simulation<br>Neural networks<br>Lidar and camera integration | Input: Camera images and lidar point clouds 摄像头图像和激光雷达点云<br>Step1: Estimate radar signal distribution 估计雷达信号分布<br>Step2: Generate 4D radar signals 生成4D雷达信号<br>Step3: Predict radar signal strength (RSS) 预测雷达信号强度 (RSS)<br>Output: Simulated radar datagram 输出: 模拟雷达数据包 |
8.5 | [[8.5] 2503.08165 Multimodal Generation of Animatable 3D Human Models with AvatarForge](https://arxiv.org/abs/2503.08165) <br> [{'name': 'Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang'}] | 3D Generation 三维生成 | v2<br>3D human modeling 3D人类建模<br>animatable avatars 可动画头像<br>LLM integration LLM集成 | Input: Text or image inputs 文本或图像输入<br>Step1: Capture detailed specifications 捕捉详细规范<br>Step2: Integrate LLM for commonsense reasoning 集成LLM进行常识推理<br>Step3: Utilize 3D human generators 利用3D人类生成器<br>Step4: Iterative refinement through auto-verification 通过自动验证进行迭代完善<br>Output: Customizable, animatable 3D human avatars 输出: 可定制的可动画3D人类头像 |
8.5 | [[8.5] 2503.08377 Layton: Latent Consistency Tokenizer for 1024-pixel Image Reconstruction and Generation by 256 Tokens](https://arxiv.org/abs/2503.08377) <br> [{'name': 'Qingsong Xie, Zhao Zhang, Zhe Huang, Yanhao Zhang, Haonan Lu, Zhenyu Yang'}] | Image Generation 图像生成 | v2<br>image reconstruction<br>latent diffusion models<br>tokenization | Input: High-resolution images 高分辨率图像<br>Step1: Image tokenization 图像令牌化<br>Step2: Latent consistency decoding 潜在一致性解码<br>Step3: Token compression 令牌压缩<br>Output: Efficient 1024x1024 image representation 高效的1024x1024图像表示 |
8.5 | [[8.5] 2503.08421 Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual Labels](https://arxiv.org/abs/2503.08421) <br> [{'name': 'Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen Dong, Cheng Wang, Chenglu Wen'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>LiDAR scans<br>unsupervised learning | Input: Multi-agent LiDAR scans 多代理LiDAR扫描<br>Step1: Initialization with shared ego-pose and ego-shape 使用共享的自我姿态和自我形状初始化<br>Step2: Preliminary label generation 生成初步标签<br>Step3: Multi-scale encoding for label refinement 对标签进行多尺度编码以进行精炼<br>Step4: Contrastive learning with refined labels 使用精炼标签进行对比学习<br>Output: High-quality detection results 高质量检测结果 |
8.5 | [[8.5] 2503.08483 GAS-NeRF: Geometry-Aware Stylization of Dynamic Radiance Fields](https://arxiv.org/abs/2503.08483) <br> [{'name': 'Nhat Phuong Anh Vu, Abhishek Saroha, Or Litany, Daniel Cremers'}] | Neural Rendering 神经渲染 | v2<br>3D stylization<br>dynamic radiance fields | Input: Dynamic scenes 动态场景<br>Step1: Extract depth maps 提取深度图<br>Step2: Geometry and appearance stylization 几何和外观风格化<br>Step3: Temporal coherence maintenance 时间一致性维护<br>Output: Stylized dynamic radiance fields 风格化动态辐射场 |
8.5 | [[8.5] 2503.08485 TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction via Spatio-Temporal Gaussian Splatting](https://arxiv.org/abs/2503.08485) <br> [{'name': 'Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>occupancy prediction<br>3D Gaussians<br>autonomous driving | Input: Raw sensor streams 原始传感器流<br>Step1: Lift surrounding-view semantics to instantiate Gaussians 提升周围视图语义以实例化高斯<br>Step2: Move dynamic Gaussians along estimated scene flow 移动动态高斯以沿估计场景流进行<br>Step3: Smooth neighboring Gaussians during optimization 平滑优化过程中相邻高斯<br>Output: Voxelized occupancy prediction 体素化占用预测 |
8.5 | [[8.5] 2503.08512 SAS: Segment Any 3D Scene with Integrated 2D Priors](https://arxiv.org/abs/2503.08512) <br> [{'name': 'Zhuoyuan Li, Jiahao Lu, Jiacheng Deng, Hanzhi Chang, Lifan Wu, Yanzhe Liang, Tianzhu Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene understanding<br>open vocabulary<br>point cloud<br>2D to 3D correspondence | Input: Point cloud features and 2D model capabilities 2D模型能力和点云特征<br>Step1: Model Alignment via Text 模型对齐<br>Step2: Annotation-Free Model Capability Construction 免标注模型能力构建<br>Step3: Feature distillation to 3D domain 特征蒸馏到3D域<br>Output: Integrated 3D scene representations 集成的3D场景表示 |
8.5 | [[8.5] 2503.08596 X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction](https://arxiv.org/abs/2503.08596) <br> [{'name': 'Feiran Wang, Jiachen Tao, Junyi Wu, Haoxuan Wang, Bin Duan, Kai Wang, Zongxin Yang, Yan Yan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>X-ray imaging<br>medical diagnostics | Input: X-ray projections X射线投影<br>Step1: Material modeling 材料建模<br>Step2: Path partitioning algorithm 路径分区算法<br>Step3: Energy absorption estimation 能量吸收估算<br>Output: 3D representations of internal structures 内部结构的三维表示 |
8.5 | [[8.5] 2503.08673 Keypoint Detection and Description for Raw Bayer Images](https://arxiv.org/abs/2503.08673) <br> [{'name': 'Jiakai Lin, Jinchang Zhang, Guoyu Lu'}] | Robotic Perception 机器人感知 | v2<br>keypoint detection<br>SLAM<br>raw images | Input: Raw Bayer images 原始拜尔图像<br>Step1: Develop convolutional kernels 开发卷积核<br>Step2: Direct keypoint detection 直接关键点检测<br>Step3: Feature description 特征描述<br>Output: Accurate keypoints and descriptors 准确的关键点和描述符 |
8.5 | [[8.5] 2503.08683 CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving](https://arxiv.org/abs/2503.08683) <br> [{'name': 'Changxing Liu, Genjia Liu, Zijun Wang, Jinchang Yang, Siheng Chen'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>cooperative autonomous driving<br>vehicle-to-vehicle communication<br>LLM-based negotiation<br>real-time control | Input: Vehicle-to-vehicle data 车辆间数据<br>Step1: LLM-based negotiation module 建立互动的LLM协商模块<br>Step2: Intention-guided waypoint generation 道路意图引导的路径生成<br>Step3: Real-time driving control 实时驾驶控制<br>Output: Improved cooperative driving performance 改进的合作驾驶性能 |
7.5 | [[7.5] 2503.08368 Debiased Prompt Tuning in Vision-Language Model without Annotations](https://arxiv.org/abs/2503.08368) <br> [{'name': 'Chaoquan Jiang, Yunfan Yang, Rui Hu, Jitao Sang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Robustness<br>Debiased Prompt Tuning | Input: Vision-Language Models (VLMs) 视觉语言模型<br>Step1: Analyze spurious correlations 分析虚假相关性<br>Step2: Utilize zero-shot recognition capabilities 利用零样本识别能力<br>Step3: Propose a debiased prompt tuning method 提出去偏置的提示调整方法<br>Output: Improved group robustness 提高的群体稳健性 |


## Arxiv 2025-03-11

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.06117 NeuraLoc: Visual Localization in Neural Implicit Map with Dual Complementary Features](https://arxiv.org/abs/2503.06117) <br> [{'name': 'Hongjia Zhai, Boming Zhao, Hai Li, Xiaokun Pan, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>visual localization<br>neural implicit maps<br>3D modeling | Input: 2D images with 3D context 提供2D图像与3D上下文<br>Step1: Extract 2D feature maps 提取2D特征图<br>Step2: Learn a 3D keypoint descriptor field 学习3D关键点描述符场<br>Step3: Align feature distributions 对齐特征分布<br>Step4: Establish matching graph 建立匹配图<br>Output: 6-DoF pose estimation 输出6自由度位姿估计 |
9.5 | [[9.5] 2503.06154 SRM-Hair: Single Image Head Mesh Reconstruction via 3D Morphable Hair](https://arxiv.org/abs/2503.06154) <br> [{'name': 'Zidu Wang, Jiankuo Zhao, Miao Xu, Xiangyu Zhu, Zhen Lei'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>3DMM<br>hair modeling | Input: Single image 单张图像<br>Step1: Data collection 数据收集<br>Step2: Semantic-consistent ray modeling 语义一致的光线建模<br>Step3: Hair mesh reconstruction 头发网格重建<br>Output: 3D hair mesh 3D头发网格 |
9.5 | [[9.5] 2503.06219 VLScene: Vision-Language Guidance Distillation for Camera-Based 3D Semantic Scene Completion](https://arxiv.org/abs/2503.06219) <br> [{'name': 'Meng Wang, Huilong Pi, Ruihui Li, Yunchuan Qin, Zhuo Tang, Kenli Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic scene completion<br>autonomous driving<br>vision-language models | Input: Camera-based images 相机采集图像<br>Step1: Vision-language guidance distillation 视觉语言指导蒸馏<br>Step2: Geometric-semantic awareness mechanism 几何-语义感知机制<br>Step3: Model evaluation 模型评估<br>Output: Enhanced 3D semantic representations 改进的三维语义表示 |
9.5 | [[9.5] 2503.06222 Vision-based 3D Semantic Scene Completion via Capture Dynamic Representations](https://arxiv.org/abs/2503.06222) <br> [{'name': 'Meng Wang, Fan Wu, Yunchuan Qin, Ruihui Li, Zhuo Tang, Kenli Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene completion<br>autonomous driving<br>semantic scene completion | Input: 2D images<br>Step1: Extract 2D explicit semantics and align into 3D space<br>Step2: Decouple scene information into dynamic and static features<br>Step3: Design dynamic-static adaptive fusion module<br>Output: Robust and accurate semantic scene representations |
9.5 | [[9.5] 2503.06235 StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams](https://arxiv.org/abs/2503.06235) <br> [{'name': 'Yang LI, Jinglu Wang, Lei Chu, Xiao Li, Shiu-hong Kao, Ying-Cong Chen, Yan Lu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting | Input: Unposed image streams 未标定图像流<br>Step1: Predict per-frame Gaussians 逐帧预测高斯<br>Step2: Establish pixel correspondences 建立像素对应关系<br>Step3: Merge redundant Gaussians 合并冗余高斯<br>Output: Online 3D reconstruction 在线三维重建 |
9.5 | [[9.5] 2503.06237 Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection](https://arxiv.org/abs/2503.06237) <br> [{'name': 'Yifan Chang, Junjie Huang, Xiaofeng Wang, Yun Ye, Zhujin Liang, Yi Shan, Dalong Du, Xingang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D lane detection<br>autonomous driving<br>geometric structures | Input: Monocular images 单目图像<br>Step1: Theoretical analysis 理论分析<br>Step2: Patching strategy development 修补策略开发<br>Step3: Model enhancement 模型增强<br>Output: Improved lane representations 改进的车道表示 |
9.5 | [[9.5] 2503.06462 StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting](https://arxiv.org/abs/2503.06462) <br> [{'name': 'Zexu Huang, Min Xu, Stuart Perry'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Gaussian Splatting<br>3D reconstruction<br>neural rendering | Input: Data from multiple views of a scene 多视角场景数据<br>Step1: Utilization of 3D Gaussian Splatting 采用3D高斯涂抹<br>Step2: Dynamic adjustment of spherical harmonics 动态调整球谐<br>Step3: Incorporation of Multi-scale Residual Network (MSRN) 引入多尺度残差网络<br>Step4: Rendering of high-quality images from low-resolution inputs 从低分辨率输入生成高质量图像<br>Output: Enhanced novel views of 3D models 改进的3D模型新视图 |
9.5 | [[9.5] 2503.06485 A Mesh Is Worth 512 Numbers: Spectral-domain Diffusion Modeling for High-dimension Shape Generation](https://arxiv.org/abs/2503.06485) <br> [{'name': 'Jiajie Fan, Amal Trigui, Andrea Bonfanti, Felix Dietrich, Thomas B\\"ack, Hao Wang'}] | 3D Generation 三维生成 | v2<br>3D generation<br>spectral-domain diffusion<br>mesh processing | Input: High-dimensional shapes 高维形状<br>Step1: Shape encoding using SVD 采用SVD进行形状编码<br>Step2: Generative modeling on eigenfeatures 在特征向量上进行生成建模<br>Step3: Mesh generation based on spectral features 基于谱特征生成网格<br>Output: High-quality 3D shapes 生成高质量的三维形状 |
9.5 | [[9.5] 2503.06565 Future-Aware Interaction Network For Motion Forecasting](https://arxiv.org/abs/2503.06565) <br> [{'name': 'Shijie Li, Xun Xu, Si Yong Yeo, Xulei Yang'}] | Autonomous Driving 自动驾驶 | v2<br>motion forecasting<br>autonomous driving<br>spatiotemporal modeling | Input: Scene encoding with historical trajectories 输入: 包含历史轨迹的场景编码<br>Step 1: Integrate future trajectories into encoding 步骤1: 将未来轨迹整合到编码中<br>Step 2: Use Mamba for spatiotemporal modeling 步骤2: 使用Mamba进行时空建模<br>Step 3: Refine and predict future trajectories 步骤3: 精炼并预测未来轨迹<br>Output: Accurate future trajectory predictions 输出: 准确的未来轨迹预测 |
9.5 | [[9.5] 2503.06569 Global-Aware Monocular Semantic Scene Completion with State Space Models](https://arxiv.org/abs/2503.06569) <br> [{'name': 'Shijie Li, Zhongyao Cheng, Rong Li, Shuai Li, Juergen Gall, Xun Xu, Xulei Yang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Semantic Scene Completion 语义场景补全<br>3D Reconstruction 三维重建<br>Monocular Vision 单目视觉 | Input: Single image 单幅图像<br>Step1: 2D feature extraction 2D特征提取<br>Step2: Long-range dependency modeling 长程依赖建模<br>Step3: 3D information completion 3D信息补全<br>Output: Complete 3D representation 完整的3D表现 |
9.5 | [[9.5] 2503.06587 Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction](https://arxiv.org/abs/2503.06587) <br> [{'name': 'Xiaoming Peng, Yixin Yang, Yang Zhou, Hui Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>surface reconstruction | Input: 2D Gaussian Splatting data 2D高斯涂抹数据<br>Step1: Analyze reflection discontinuity 分析反射不连续性<br>Step2: Introduce depth convergence loss 引入深度收敛损失<br>Step3: Rectify depth criterion 修正深度标准<br>Output: Enhanced surface reconstruction 改进的表面重建 |
9.5 | [[9.5] 2503.06660 AxisPose: Model-Free Matching-Free Single-Shot 6D Object Pose Estimation via Axis Generation](https://arxiv.org/abs/2503.06660) <br> [{'name': 'Yang Zou, Zhaoshuai Qi, Yating Liu, Zihao Xu, Weipeng Sun, Weiyi Liu, Xingyuan Li, Jiaqi Yang, Yanning Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>6D pose estimation 6D姿态估计<br>robotics 机器人<br>autonomous driving 自动驾驶<br>computer vision 计算机视觉 | Input: Single view image 单视图图像<br>Step1: Axis Generation Module (AGM) construction 轴生成模块（AGM）构建<br>Step2: Geometric consistency loss injection 几何一致性损失注入<br>Step3: Triaxial Back-projection Module (TBM) application 三轴反投影模块（TBM）应用<br>Output: Estimated 6D object pose 估计的6D物体姿态 |
9.5 | [[9.5] 2503.06677 REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints](https://arxiv.org/abs/2503.06677) <br> [{'name': 'Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, Cewu Lu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>articulated objects<br>Gaussian Splatting | Input: Multi-view RGB images of articulated objects 多视角RGB图像<br>Step1: Introduce Signed Distance Field (SDF) guidance to regularize Gaussian opacity fields 引入签名距离场（SDF）引导以规范化高斯不透明度场<br>Step2: Establish deformable fields for 3D Gaussians constrained by kinematic structures 建立受运动结构约束的3D高斯可变形场<br>Step3: Achieve unsupervised generation of surface meshes in unseen states 实现对未见状态表面网格的无监督生成<br>Output: High-quality textured surface reconstruction and generation 输出：高质量纹理表面重建与生成 |
9.5 | [[9.5] 2503.06744 CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving](https://arxiv.org/abs/2503.06744) <br> [{'name': 'Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, Alois Knoll'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>4D Gaussian Splatting<br>dynamic scene rendering<br>autonomous driving | Input: Dynamic scenes 动态场景<br>Step1: Use 2D segmentation for Gaussian features 使用2D分割获取高斯特征<br>Step2: Track temporally deformed features 跟踪时间变形特征<br>Step3: Aggregate context and deformation features 组合上下文和变形特征<br>Output: Enhanced dynamic scene representations 改进的动态场景表示 |
9.5 | [[9.5] 2503.06762 Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields](https://arxiv.org/abs/2503.06762) <br> [{'name': 'Abdelaziz Bouzidi, Hamid Laga, Hazem Wannous'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neural fields<br>Gaussian RBF | Input: Neural fields and images 神经场和图像<br>Step1: Replace MLP neurons with RBF kernels 用RBF核替换MLP神经元<br>Step2: Train for 3D geometry representation 训练3D几何表示<br>Step3: Optimize for novel view synthesis 优化新视图合成<br>Output: Fast and accurate neural representation 快速准确的神经表示 |
9.5 | [[9.5] 2503.06818 Sub-Image Recapture for Multi-View 3D Reconstruction](https://arxiv.org/abs/2503.06818) <br> [{'name': 'Yanwei Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-view geometry | Input: Original high-resolution images 原始高分辨率图像<br>Step1: Split images into sub-images 将图像分割成子图像<br>Step2: Process sub-images individually 分别处理子图像<br>Step3: Apply existing 3D reconstruction algorithms using sub-images 使用子图像处理现有三维重建算法<br>Output: Enhanced 3D reconstruction results 改进的三维重建结果 |
9.5 | [[9.5] 2503.06821 HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors](https://arxiv.org/abs/2503.06821) <br> [{'name': 'Siyu Li, Yihong Cao, Hao Shi, Yongsheng Zang, Xuan He, Kailun Yang, Zhiyong Li'}] | Autonomous Driving 自动驾驶 | v2<br>Bird's-Eye View (BEV) mapping<br>domain adaptation<br>3D mapping | Input: Multi-view images 多视角图像<br>Step1: Hierarchical perspective prior-guided domain adaptation 分层视角先验引导的领域适应<br>Step2: Component integration 组件集成 (SGPS, DACL, CDFM)<br>Step3: Performance evaluation 性能评估<br>Output: Enhanced BEV mapping results 改进的鸟瞰映射结果 |
9.5 | [[9.5] 2503.06900 DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation](https://arxiv.org/abs/2503.06900) <br> [{'name': 'Xiaoliang Ju, Hongsheng Li'}] | 3D Generation 三维生成 | v2<br>3D generation<br>Gaussian Splatting<br>triplane representation<br>neural rendering | Input: Multi-view images 多视角图像<br>Step1: Encode 3D objects into triplanes 将3D对象编码为三平面<br>Step2: Train a Variational Autoencoder (VAE) to compress triplanes 训练变分自编码器以压缩三平面<br>Step3: Train a diffusion model on triplane latent code 在三平面潜在空间上训练扩散模型<br>Output: Generate high-quality 3D objects 生成高质量的3D对象 |
9.5 | [[9.5] 2503.06947 Aligning Instance-Semantic Sparse Representation towards Unsupervised Object Segmentation and Shape Abstraction with Repeatable Primitives](https://arxiv.org/abs/2503.06947) <br> [{'name': 'Jiaxin Li, Hongxing Wang, Jiawei Tan, Zhilong Ou, Junsong Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic representation 三维语义表示<br>unsupervised learning 无监督学习<br>shape abstraction 形状抽象<br>instance segmentation 实例分割 | Input: Point cloud data 点云数据<br>Step1: Instance and semantic segmentation 实例和语义分割<br>Step2: Sparse representation model construction 稀疏表示模型构建<br>Step3: Feature alignment between instance-level and semantic-level features 实例级和语义级特征对齐<br>Output: Semantic-aware shape representation 语义感知形状表示 |
9.5 | [[9.5] 2503.06983 Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark](https://arxiv.org/abs/2503.06983) <br> [{'name': 'Jiahao Wang, Xiangyu Cao, Jiaru Zhong, Yuner Zhang, Haibao Yu, Lei He, Shaobing Xu'}] | 3D Perception and Modeling 三维感知与建模 | v2<br>3D perception<br>autonomous driving<br>dataset<br>benchmark<br>object detection | Input: Aerial and ground images aerial和地面图像<br>Step1: Dataset creation 数据集创建<br>Step2: Benchmarking framework development 基准框架开发<br>Step3: AGILE integration AGILE 集成<br>Output: Comprehensive evaluation metrics 综合评估指标 |
9.5 | [[9.5] 2503.07019 HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions](https://arxiv.org/abs/2503.07019) <br> [{'name': 'Keyu Du, Hao Xu, Haipeng Li, Hong Qu, Chi-Wing Fu, Shuaicheng Liu'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>point cloud registration<br>hybrid motions | Input: Scene-level point clouds场景级点云<br>Step1: Dataset construction 数据集构建<br>Step2: Mask-learning module formulation 掩模学习模块构建<br>Step3: Feature extraction and correlation computation 特征提取与相关性计算<br>Output: Robustly registered point clouds 稳健配准的点云 |
9.5 | [[9.5] 2503.07029 Availability-aware Sensor Fusion via Unified Canonical Space for 4D Radar, LiDAR, and Camera](https://arxiv.org/abs/2503.07029) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | Autonomous Driving 自动驾驶 | v2<br>sensor fusion<br>autonomous driving<br>object detection | Input: Camera, LiDAR, and 4D Radar sensor data 传感器数据<br>Step1: Unified Canonical Projection (UCP) for feature alignment 特征对齐<br>Step2: Cross-attention across sensors for robustness 增强鲁棒性的跨传感器注意机制<br>Step3: Object Detection using fused features 采用融合特征的目标检测<br>Output: Improved object detection performance 提高的目标检测性能 |
9.5 | [[9.5] 2503.07133 A Light Perspective for 3D Object Detection](https://arxiv.org/abs/2503.07133) <br> [{'name': "Marcelo Eduardo Pederiva, Jos\\'e Mario De Martino, Alessandro Zimmer"}] | 3D Object Detection 三维物体检测 | v2<br>3D Object Detection<br>Sensor Fusion<br>Autonomous Driving | Input: Camera and LIDAR data 摄像头和激光雷达数据<br>Step1: Feature extraction 特征提取<br>Step2: BEV conversion 鸟瞰图转换<br>Step3: Sensor fusion 传感器融合<br>Output: Lightweight 3D Object Detection model 轻量级3D物体检测模型 |
9.5 | [[9.5] 2503.07152 Controllable 3D Outdoor Scene Generation via Scene Graphs](https://arxiv.org/abs/2503.07152) <br> [{'name': 'Yuheng Liu, Xinke Li, Yuning Zhang, Lu Qi, Xin Li, Wenping Wang, Chongshou Li, Xueting Li, Ming-Hsuan Yang'}] | 3D Generation 三维生成 | v2<br>3D generation<br>scene graphs<br>autonomous driving | Input: Scene graphs 场景图<br>Step1: GNN processing 图神经网络处理<br>Step2: Spatial mapping 空间映射<br>Step3: Diffusion model conditioning 扩散模型条件化<br>Output: 3D scene generation 3D场景生成 |
9.5 | [[9.5] 2503.07190 Multi-Modal 3D Mesh Reconstruction from Images and Text](https://arxiv.org/abs/2503.07190) <br> [{'name': 'Melvin Reka, Tessa Pulli, Markus Vincze'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>few-shot learning<br>language-guided segmentation | Input: Set of input images and language query 输入：输入图像集和语言查询<br>Step1: Mask generation using GroundingDINO and Segment Anything Model 步骤1：使用GroundingDINO和Segment Anything Model生成掩模<br>Step2: Sparse point cloud reconstruction using VGGSfM 步骤2：使用VGGSfM重构稀疏点云<br>Step3: Mesh reconstruction with Gaussian Splatting method (SuGAR) 步骤3：使用高斯点阵方法（SuGAR）重建网格<br>Step4: Artifact removal from the reconstructed mesh 步骤4：从重建的网格中去除伪影<br>Output: Final 3D mesh of the queried object 输出：查询对象的最终3D网格 |
9.5 | [[9.5] 2503.07476 SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting](https://arxiv.org/abs/2503.07476) <br> [{'name': 'Jiahui Zhang, Fangneng Zhan, Ling Shao, Shijian Lu'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>novel view synthesis<br>texture optimization<br>scene representation | Input: Multi-view images 多视角图像<br>Step1: Incorporate second-order statistics 引入二阶统计量<br>Step2: Feature augmentation 特征增强<br>Step3: Selective gradient loss 选择性梯度损失<br>Output: Improved rendering quality 改进的渲染质量 |
9.5 | [[9.5] 2503.07507 PE3R: Perception-Efficient 3D Reconstruction](https://arxiv.org/abs/2503.07507) <br> [{'name': 'Jie Hu, Shizun Wang, Xinchao Wang'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction 3D重建<br>semantic reconstruction 语义重建<br>autonomous systems 自动化系统 | Input: 2D images 2D图像<br>Step1: Pixel embedding disambiguation 像素嵌入消歧义<br>Step2: Semantic field reconstruction 语义场重建<br>Step3: Global view perception 全局视图感知<br>Output: 3D semantic fields 3D语义场 |
9.5 | [[9.5] 2503.07561 Alligat0R: Pre-Training Through Co-Visibility Segmentation for Relative Camera Pose Regression](https://arxiv.org/abs/2503.07561) <br> [{'name': 'Thibaut Loiseau, Guillaume Bourmaud, Vincent Lepetit'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>pose regression<br>autonomous driving | Input: Image pairs 图像对<br>Step1: Co-visibility segmentation co-可见性分割<br>Step2: Model training 模型训练<br>Output: Enhanced pose regression model 改进的姿态回归模型 |
9.5 | [[9.5] 2503.07593 Hierarchical Cross-Modal Alignment for Open-Vocabulary 3D Object Detection](https://arxiv.org/abs/2503.07593) <br> [{'name': 'Youjun Zhao, Jiaying Lin, Rynson W. H. Lau'}] | 3D Object Detection 3D对象检测 | v2<br>3D object detection<br>open-vocabulary<br>vision-language models | Input: 3D point clouds and corresponding images 3D点云和相应的图像<br>Step1: Hierarchical Data Integration (HDI) to create multi-level data 层次数据集成 (HDI)以创建多层数据<br>Step2: Apply Interactive Cross-Modal Alignment (ICMA) to establish feature connections 应用交互式跨模态对齐 (ICMA) 建立特征连接<br>Step3: Use Object-Focusing Context Adjustment (OFCA) to refine object-centric features 使用对象聚焦上下文调整 (OFCA) 来细化以对象为中心的特征<br>Output: Enhanced 3D object detection results 改进的3D对象检测结果 |
9.5 | [[9.5] 2503.07608 AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning](https://arxiv.org/abs/2503.07608) <br> [{'name': 'Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, Xinggang Wang'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>reinforcement learning<br>vision-language models | Input: Vision-language models (VLMs) in autonomous driving 视觉语言模型 (VLMs) 在自动驾驶中的应用<br>Step1: Reinforcement Learning reward design 强化学习奖励设计<br>Step2: Two-stage planning reasoning training strategy 双阶段规划推理训练策略<br>Step3: Model evaluation and performance enhancement 模型评估与性能提升<br>Output: Improved planning performance and training efficiency 改进的规划性能和训练效率 |
9.2 | [[9.2] 2503.06282 From Dataset to Real-world: General 3D Object Detection via Generalized Cross-domain Few-shot Learning](https://arxiv.org/abs/2503.06282) <br> [{'name': 'Shuangzhi Li, Junlong Shen, Lei Ma, Xingyu Li'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>LiDAR<br>few-shot learning<br>domain adaptation | Input: LiDAR data and few-shot samples<br>Step1: Define the generalized cross-domain few-shot (GCFS) task<br>Step2: Integrate multi-modal fusion for semantic insights<br>Step3: Implement physically-aware box searching strategy<br>Step4: Employ contrastive-enhanced prototype learning<br>Output: Robust 3D object detection across domains |
9.2 | [[9.2] 2503.06435 OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection](https://arxiv.org/abs/2503.06435) <br> [{'name': 'Adrian Chow, Evelien Riddell, Yimu Wang, Sean Sedwards, Krzysztof Czarnecki'}] | 3D Object Detection 三维物体检测 | v2<br>open-vocabulary<br>3D object detection<br>cross-modal alignment | Input: 3D point cloud data 3D点云数据<br>Step1: Core strategy development 核心策略开发<br>Step2: Annotations discovery 注释发现<br>Step3: Alignment pair filtering 对齐对过滤<br>Output: Enhanced 3D object detector 改进的3D对象检测器 |
9.2 | [[9.2] 2503.06986 ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration](https://arxiv.org/abs/2503.06986) <br> [{'name': 'Youngseok Kim, Sunwook Hwang, Hyung-Sin Kim, Saewoong Bahk'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud<br>model inversion attack<br>autonomous driving<br>restoration | Input: 3D point cloud data 3D 点云数据<br>Step1: Identify empty and non-empty voxels 识别空和非空体素<br>Step2: Implement Voxel Occupancy Classification 实现体素占用分类<br>Step3: Apply Dispersion-Controlled Supervision 应用分散控制监督<br>Output: Restored 3D point cloud scene 恢复的 3D 点云场景 |
9.2 | [[9.2] 2503.07125 Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation](https://arxiv.org/abs/2503.07125) <br> [{'name': 'Sihao Lin, Daqi Liu, Ruochong Fu, Dongrui Liu, Andy Song, Hongwei Xie, Zhihui Li, Bing Wang, Xiaojun Chang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>occupancy prediction<br>autonomous driving | Input: Monocular images 单目图像<br>Step1: Decoupling 3D supervision 3D监督解耦<br>Step2: Optimizing relative depth 优化相对深度<br>Step3: Projecting semantics into 3D space 语义投影到3D空间<br>Output: 3D occupancy prediction 3D占用预测 |
9.2 | [[9.2] 2503.07168 HisTrackMap: Global Vectorized High-Definition Map Construction via History Map Tracking](https://arxiv.org/abs/2503.07168) <br> [{'name': 'Jing Yang, Sen Yang, Xiao Tan, Hanli Wang'}] | Autonomous Driving 自动驾驶 | v2<br>high-definition maps 高清地图<br>autonomous driving 自动驾驶<br>temporal tracking 时间跟踪 | Input: High-definition map data 高精度地图数据<br>Step1: Instance-level historical rasterization map representation 实例级历史栅格地图表示<br>Step2: Map-Trajectory Prior Fusion module introduction 引入地图轨迹先验融合模块<br>Step3: Global perspective metric for evaluation 评估全球视角度量<br>Output: Enhanced HD maps 改进的高清地图 |
9.0 | [[9.0] 2503.06467 SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts](https://arxiv.org/abs/2503.06467) <br> [{'name': 'Shijia Zhao, Qiming Xia, Xusheng Guo, Pufan Zou, Maoji Zheng, Hai Wu, Chenglu Wen, Cheng Wang'}] | 3D Object Detection 3D对象检测 | v2<br>3D object detection 3D对象检测<br>sparsely-supervised 3D detection 稀疏监督3D检测<br>cross-modal prompts 跨模态提示 | Input: 3D object detection with sparse annotations 3D对象检测，数据稀缺<br>Step1: Generate cross-modal semantic prompts from LMMs 使用大规模多模态模型生成跨模态语义提示<br>Step2: Apply CPST for accurate prompt selection 应用自信点语义转移进行准确提示选择<br>Step3: Generate high-quality pseudo-labels using DCPG and DS score 基于DCPG和DS分数生成高质量伪标签<br>Output: Enhanced sparsely-supervised 3D object detection 改进的稀疏监督3D对象检测 |
9.0 | [[9.0] 2503.06469 Vector Quantized Feature Fields for Fast 3D Semantic Lifting](https://arxiv.org/abs/2503.06469) <br> [{'name': 'George Tang, Aditya Agarwal, Weiqiao Han, Trevor Darrell, Yutong Bai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D lifting<br>Vector-Quantized Feature Field<br>semantic lifting<br>scene representation | Input: Multi-view images 多视角图像<br>Step1: Lifting to per-image masks 提升到每图像掩模<br>Step2: Introducing Vector-Quantized Feature Field 引入向量量化特征场<br>Step3: Instant querying for relevance masks 即时查询相关掩模<br>Output: Enhanced scene representation 改进的场景表现 |
9.0 | [[9.0] 2503.06740 D3DR: Lighting-Aware Object Insertion in Gaussian Splatting](https://arxiv.org/abs/2503.06740) <br> [{'name': 'Vsevolod Skorokhodov, Nikita Durasov, Pascal Fua'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>object insertion<br>lighting consistency | Input: 3DGS scenes and objects 3DGS场景和物体<br>Step1: Object insertion 物体插入<br>Step2: Lighting adjustment 光照调整<br>Step3: Optimization of Gaussian parameters 高斯参数优化<br>Output: Realistically inserted 3D models 现实插入的三维模型 |
9.0 | [[9.0] 2503.07597 HumanMM: Global Human Motion Recovery from Multi-shot Videos](https://arxiv.org/abs/2503.07597) <br> [{'name': 'Yuhong Zhang, Guanlin Wu, Ling-Hao Chen, Zhuokai Zhao, Jing Lin, Xiaoke Jiang, Jiamin Wu, Zhuoheng Li, Hao Frank Yang, Haoqian Wang, Lei Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D motion recovery<br>human pose estimation<br>multi-shot videos | Input: Multi-shot videos 多视角视频<br>Step1: Shot transition detection 镜头过渡检测<br>Step2: Enhanced camera pose estimation 增强型相机位姿估计<br>Step3: Human Motion Recovery (HMR) integration 人体运动恢复集成<br>Step4: Motion integration and evaluation 运动集成与评估<br>Output: Accurate 3D human motion reconstruction 精确的三维人体动作重建 |
8.5 | [[8.5] 2503.05949 Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting](https://arxiv.org/abs/2503.05949) <br> [{'name': 'Dominic Maggio, Luca Carlone'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic mapping<br>Gaussian Splatting | Input: Posed RGB images 置标RGB图像<br>Step1: Task granularity determination 任务粒度确定<br>Step2: Bayesian updating to aggregate observations 贝叶斯更新以聚合观察<br>Step3: Clustering 3D Gaussians into task-relevant objects 将3D高斯聚类为任务相关对象<br>Output: Semantic Gaussian Splat representation 语义高斯点云表示 |
8.5 | [[8.5] 2503.06003 Integrating Frequency-Domain Representations with Low-Rank Adaptation in Vision-Language Models](https://arxiv.org/abs/2503.06003) <br> [{'name': 'Md Azim Khan, Aryya Gangopadhyay, Jianwu Wang, Robert F. Erbacher'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>frequency-domain transformations<br>low-rank adaptation<br>Unmanned Ground Vehicle (UGV)<br>Visual Question Answering (VQA) | Input: Visual and textual data 视觉和文本数据<br>Step1: Frequency domain transformation 频域变换<br>Step2: Low-rank adaptation (LoRA) 低秩适应<br>Step3: Model evaluation on benchmark datasets 在基准数据集上进行模型评估<br>Output: Enhanced multimodal model 改进的多模态模型 |
8.5 | [[8.5] 2503.06012 End-to-End HOI Reconstruction Transformer with Graph-based Encoding](https://arxiv.org/abs/2503.06012) <br> [{'name': 'Zhenrong Wang, Qi Zheng, Sihan Ma, Maosheng Ye, Yibing Zhan, Dongjiang Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human-object interaction<br>transformer | Input: Single image from which human and object meshes are reconstructed 从单张图像重建人类和物体网格<br>Step1: Generate initial mesh vertices 生成初始网格顶点<br>Step2: Concatenate grid-sampled features and input to transformer 连接网格采样特征并输入到变压器<br>Step3: Predict object pose and reconstruct human vertices 预测物体姿势并重建人类顶点<br>Output: Final reconstructed human and object meshes 输出：最终重建的人类和物体网格 |
8.5 | [[8.5] 2503.06014 Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity](https://arxiv.org/abs/2503.06014) <br> [{'name': 'Xiaohao Xu, Feng Xue, Xiang Li, Haowei Li, Shusheng Yang, Tianyi Zhang, Matthew Johnson-Roberson, Xiaonan Huang'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>3D spatial understanding | Input: Monocular image inputs 单目图像输入<br>Step 1: Introduce benchmark MD-3k to expose depth biases 介绍基准MD-3k以揭示深度偏差<br>Step 2: Apply Laplacian Visual Prompting for depth extraction 应用拉普拉斯视觉提示进行深度提取<br>Step 3: Integrate depth estimations from LVP and RGB inputs 集成来自LVP和RGB输入的深度估计<br>Output: Multi-layer depth estimates and enhanced spatial understanding 输出：多层深度估计和增强的空间理解 |
8.5 | [[8.5] 2503.06089 Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision](https://arxiv.org/abs/2503.06089) <br> [{'name': 'David C. Jeong, Aditya Puranik, James Vong, Vrushabh Abhijit Deogirikar, Ryan Fell, Julianna Dietrich, Maria Kyrarini, Christopher Kitts'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human mesh recovery<br>egocentric vision<br>transformer model | Input: Fisheye images 鱼眼图像<br>Step1: Egocentric position embedding generation 自我中心位置嵌入生成<br>Step2: Multi-task model training 多任务模型训练<br>Step3: 3D human mesh reconstruction 3D人类网格重建<br>Output: Accurate 3D human mesh models 准确的3D人类网格模型 |
8.5 | [[8.5] 2503.06094 PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point Cloud Semantic Segmentation](https://arxiv.org/abs/2503.06094) <br> [{'name': 'Yong He, Hongshan Yu, Mingtao Feng, Tongjia Chen, Zechuan Li, Anwaar Ulhaq, Saeed Anwar, Ajmal Saeed Mian'}] | Point Cloud Processing 点云处理 | v2<br>point cloud<br>semantic segmentation<br>diffusion models<br>3D reconstruction<br>autonomous driving | Input: Point cloud data 点云数据<br>Step1: Introduce noisy label embedding mechanism 引入噪声标签嵌入机制<br>Step2: Implement point frequency transformer 实现点频率变换器<br>Step3: Design denoising PointNet 设计去噪PointNet<br>Output: Large-scale point cloud semantic segmentation 大规模点云语义分割 |
8.5 | [[8.5] 2503.06118 SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography](https://arxiv.org/abs/2503.06118) <br> [{'name': 'Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>3D reconstruction<br>security<br>steganography | Input: 3D Gaussian points  三维高斯点<br>Step1: Introduce hybrid decoupled Gaussian encryption  引入混合解耦高斯加密<br>Step2: Embed offsets and RGB attributes 嵌入偏移和RGB属性<br>Step3: Develop density region-aware anchor strategy 开发密度区域感知的锚点策略<br>Output: Secure and efficient steganography output 安全高效的隐写输出 |
8.5 | [[8.5] 2503.06136 GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation](https://arxiv.org/abs/2503.06136) <br> [{'name': 'Ye Tao, Jiawei Zhang, Yahao Shi, Dongqing Zou, Bin Zhou'}] | 3D Generation 三维生成 | v2<br>3D generation<br>Gaussian Splatting<br>geometry consistency<br>diffusion models | Input: Single image 单幅图像<br>Step1: Utilize 2D diffusion models 使用2D扩散模型<br>Step2: Gaussian-splatting-based geometric distillation 高斯点云的几何蒸馏<br>Step3: Generate explicit 3D representations 生成明确的3D表示<br>Output: Multi-view consistent 3D models 多视角一致的3D模型 |
8.5 | [[8.5] 2503.06161 Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction](https://arxiv.org/abs/2503.06161) <br> [{'name': 'Kai Li, Junhao Wang, William Han, Ding Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>surgical scene understanding<br>Gaussian Splatting | Input: Endoscopic stereo videos 内窥镜立体视频<br>Step1: Integrate segmentation cues 2D分割线索集成<br>Step2: Extend Gaussian Splatting 扩展高斯点云<br>Step3: Real-time rendering and segmentation 实时渲染和分割<br>Output: Enhanced surgical scene understanding 改进的外科场景理解 |
8.5 | [[8.5] 2503.06169 Treble Counterfactual VLMs: A Causal Approach to Hallucination](https://arxiv.org/abs/2503.06169) <br> [{'name': 'Li Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, Yue Zhao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>hallucination<br>autonomous driving | Step 1: Design structural causal graphs to model relationships between vision, text, and outputs.<br>Step 2: Estimate Natural Direct Effect (NDE) of vision and text modalities on outputs.<br>Step 3: Implement a dynamic test-time intervention module to adjust model's reliance on modalities. |
8.5 | [[8.5] 2503.06179 ForestSplats: Deformable transient field for Gaussian Splatting in the Wild](https://arxiv.org/abs/2503.06179) <br> [{'name': 'Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>transient elements<br>real-time rendering | Input: Unconstrained image collections 不受限制的图像集<br>Step1: Deformable transient field design 变形瞬态场设计<br>Step2: Superpixel-aware masking 超像素感知掩膜<br>Step3: Multi-stage training scheme 多阶段训练方案<br>Output: Decomposed static and transient fields 分解的静态和瞬态场 |
8.5 | [[8.5] 2503.06261 Segment Anything, Even Occluded](https://arxiv.org/abs/2503.06261) <br> [{'name': 'Wei-En Tai, Yu-Lin Shih, Cheng Sun, Yu-Chiang Frank Wang, Hwann-Tzong Chen'}] | Amodal Instance Segmentation 非模态实例分割 | v2<br>amodal instance segmentation<br>autonomous driving<br>mask prediction | Input: Images containing occluded objects 图像包含遮挡物体<br>Step1: Use existing object detectors to generate modal and amodal boxes 使用现有物体检测器生成模态和非模态框<br>Step2: Process detections through SAMEO to produce amodal masks 通过SAMEO处理检测生成非模态掩码<br>Step3: Evaluate on benchmarks 在基准上评估<br>Output: Segmentation masks for visible and occluded parts 分割掩码用于可见和遮挡部分 |
8.5 | [[8.5] 2503.06271 SplatTalk: 3D VQA with Gaussian Splatting](https://arxiv.org/abs/2503.06271) <br> [{'name': 'Anh Thai, Songyou Peng, Kyle Genova, Leonidas Guibas, Thomas Funkhouser'}] | 3D Visualization and Modeling 三维可视化与建模 | v2<br>3D visual question answering 3D视觉问答<br>Gaussian Splatting 高斯点云<br>language features 语言特征 | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Extract language features from images 从图像中提取语言特征<br>Step2: Build a 3D Gaussian Splatting representation 构建3D高斯点云表示<br>Step3: Utilize a pretrained LLM for 3D VQA 利用预训练的LLM进行3D视觉问答<br>Output: 3D tokens for visual question answering 3D视觉问答的tokens |
8.5 | [[8.5] 2503.06287 Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding](https://arxiv.org/abs/2503.06287) <br> [{'name': 'Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Grounding 视觉定位<br>Large Vision-Language Models 大型视觉语言模型 | Input: Large Vision-Language Models (LVLMs) 用户化语言视觉模型<br>Step1: Identify localization heads 确定定位头<br>Step2: Create training-free visual grounding framework 创建无需训练的视觉定位框架<br>Step3: Utilize attention maps to localize objects 使用注意力图定位对象<br>Output: Bounding boxes or masks for referred objects 输出: 被称述对象的边界框或掩膜 |
8.5 | [[8.5] 2503.06313 Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection](https://arxiv.org/abs/2503.06313) <br> [{'name': 'Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, Diange Yang'}] | Autonomous Driving 自动驾驶 | v2<br>Traffic Sign Recognition<br>Lane Detection<br>Autonomous Vehicles<br>Deep Learning | Input: Traffic sign images and lane data 交通标志图像和车道数据<br>Step1: Model evaluation for traffic sign recognition 模型评估以识别交通标志<br>Step2: Deep learning segmentation for lane detection 深度学习分割以进行车道检测<br>Step3: Integration of Multimodal LLM framework 多模态LLM框架的集成<br>Output: Robust lane and sign detection outputs 稳健的车道和标志检测输出 |
8.5 | [[8.5] 2503.06380 TI-JEPA: An Innovative Energy-based Joint Embedding Strategy for Text-Image Multimodal Systems](https://arxiv.org/abs/2503.06380) <br> [{'name': 'Khang H. N. Vo, Duc P. T. Nguyen, Thong Nguyen, Tho T. Quan'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>Text-image alignment 文本图像对齐<br>Multimodal systems 多模态系统<br>Energy-based model 能量基础模型 | Input: Text and image modalities 文本与图像模态<br>Step1: Joint embedding strategy 共同嵌入策略<br>Step2: Energy-based model framework 能量基础模型框架<br>Step3: Performance evaluation on multimodal benchmarks 多模态基准的性能评估<br>Output: Enhanced multimodal alignment 改进的多模态对齐 |
8.5 | [[8.5] 2503.06458 Reconstructing Depth Images of Moving Objects from Wi-Fi CSI Data](https://arxiv.org/abs/2503.06458) <br> [{'name': 'Guanyu Cao, Takuya Maekawa, Kazuya Ohara, Yasue Kishino'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Depth imaging 深度成像<br>Wi-Fi CSI Wi-Fi信道状态信息<br>3D Reconstruction 三维重建 | Input: Wi-Fi channel state information (CSI) Wi-Fi信道状态信息<br>Step1: Estimate shape, depth, and position 估计形状、深度和位置<br>Step2: Implement VAE-based teacher-student architecture 实现基于VAE的教师-学生架构<br>Step3: Output reconstructed depth image 输出重建的深度图像 |
8.5 | [[8.5] 2503.06477 PDB: Not All Drivers Are the Same -- A Personalized Dataset for Understanding Driving Behavior](https://arxiv.org/abs/2503.06477) <br> [{'name': 'Chuheng Wei, Ziye Qin, Siyan Li, Ziyan Zhang, Xuanpeng Zhao, Amr Abdelraouf, Rohit Gupta, Kyungtae Han, Matthew J. Barth, Guoyuan Wu'}] | Autonomous Driving 自动驾驶 | v2<br>Personalized Driving Behavior<br>autonomous driving<br>trajectory prediction | Input: Multi-modal dataset with various sensor data 多模态数据集，包括各种传感器数据<br>Step1: Data collection 数据收集<br>Step2: Data processing 数据处理<br>Step3: Analysis of driving behavior 驾驶行为分析<br>Output: Structured trajectory prediction dataset 结构化轨迹预测数据集 |
8.5 | [[8.5] 2503.06497 Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2503.06497) <br> [{'name': 'Enming Zhang, Peizhe Gong, Xingyuan Dai, Yisheng Lv, Qinghai Miao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>safety cognition<br>vision-language models<br>autonomous driving | Input: Vision-language models in autonomous driving 视觉语言模型在自动驾驶中的应用<br>Step1: Development of Safety Cognitive Driving Benchmark (SCD-Bench) 安全认知驾驶基准(SCD-Bench)的构建<br>Step2: Large-scale annotation for evaluation 大规模标注以进行评估<br>Step3: Automated evaluation method using LLMs 基于大型语言模型的自动评估方法<br>Output: Assessment of VLM safety cognition 在驾驶中评估VLM的安全认知 |
8.5 | [[8.5] 2503.06601 StructVPR++: Distill Structural and Semantic Knowledge with Weighting Samples for Visual Place Recognition](https://arxiv.org/abs/2503.06601) <br> [{'name': 'Yanqing Shen, Sanping Zhou, Jingwen Fu, Ruotong Wang, Shitao Chen, Nanning Zheng'}] | Visual Place Recognition 视觉地点识别 | v2<br>Visual Place Recognition<br>Knowledge Distillation<br>Autonomous Driving<br>Robotics | Input: RGB images and segmentation images RGB图像和分割图像<br>Step1: Global feature retrieval using RGB input 使用RGB输入进行全局特征检索<br>Step2: Reranking using structural and semantic information 使用结构和语义信息进行重新排序<br>Step3: Knowledge distillation from segmentation to RGB 特征从分割图到RGB的知识蒸馏<br>Output: Improved visual place recognition model 改进的视觉地点识别模型 |
8.5 | [[8.5] 2503.06670 Attention, Please! PixelSHAP Reveals What Vision-Language Models Actually Focus On](https://arxiv.org/abs/2503.06670) <br> [{'name': 'Roni Goldshmidt'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>interpretability<br>autonomous driving | Input: Vision-Language Models' input-output pairs 视觉语言模型的输入输出对<br>Step 1: Segmentation of image objects 图像对象的分割<br>Step 2: Systematic perturbation of objects 对对象的系统性扰动<br>Step 3: Quantification of object importance 物体重要性的量化<br>Output: Enhanced interpretability of VLMs 提高视觉语言模型的可解释性 |
8.5 | [[8.5] 2503.06773 Investigating Image Manifolds of 3D Objects: Learning, Shape Analysis, and Comparisons](https://arxiv.org/abs/2503.06773) <br> [{'name': 'Benjamin Beaudett, Shenyuan Liang, Anuj Srivastava'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>manifold learning<br>shape analysis<br>image manifolds<br>pose manifolds | Input: Sets of images of 3D objects 3D对象的图像集<br>Step1: Geometry-preserving transformations 几何保留变换<br>Step2: Mapping to low-dimensional latent spaces 映射到低维潜在空间<br>Step3: Shape analysis using Kendall's shape analysis shape analysis 使用Kendall形状分析<br>Output: Characterization of pose manifolds and clustering of objects 输出: 主要形状流形的特征化及物体聚类 |
8.5 | [[8.5] 2503.06859 ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting](https://arxiv.org/abs/2503.06859) <br> [{'name': 'Konstantinos D. Polyzos, Athanasios Bacharis, Saketh Madhuvarasu, Nikos Papanikolopoulos, Tara Javidi'}] | Neural Rendering 神经渲染 | v2<br>Gaussian Splatting<br>3D representation<br>rendering performance<br>active image selection | Input: Selected 2D images 选定的2D图像<br>Step1: Active image selection 活跃图像选择<br>Step2: Gaussian function initialization 高斯函数初始化<br>Step3: Performance evaluation 性能评估<br>Output: Improved GS rendering performance 改进的GS渲染性能 |
8.5 | [[8.5] 2503.06960 A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning](https://arxiv.org/abs/2503.06960) <br> [{'name': 'Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>robot learning 机器人学习<br>pre-trained vision models 预训练视觉模型 | Input: Pre-trained vision models (PVMs) 预训练视觉模型<br>Step1: Systematic evaluation 系统评估<br>Step2: Introduce SlotMIM 方法设计<br>Step3: Pre-training on diverse datasets 使用多样数据集进行预训练<br>Output: Improved object-centric representations 改进的物体中心表示 |
8.5 | [[8.5] 2503.07000 Frequency-Aware Density Control via Reparameterization for High-Quality Rendering of 3D Gaussian Splatting](https://arxiv.org/abs/2503.07000) <br> [{'name': 'Zhaojie Zeng, Yuesong Wang, Lili Ju, Tao Guan'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>high-quality rendering<br>density control | Input: 3D Gaussian representations 3D高斯表示<br>Step1: Establish density and scale relation 建立密度和尺度的关系<br>Step2: Implement frequency-aware density control 实施频率感知密度控制<br>Step3: Validate with experimental datasets 使用实验数据集进行验证<br>Output: Enhanced rendering quality 改进的渲染质量 |
8.5 | [[8.5] 2503.07167 Temporal Overlapping Prediction: A Self-supervised Pre-training Method for LiDAR Moving Object Segmentation](https://arxiv.org/abs/2503.07167) <br> [{'name': 'Ziliang Miao, Runjian Chen, Yixi Cai, Buwei He, Wenquan Zhao, Wenqi Shao, Bo Zhang, Fu Zhang'}] | Moving Object Segmentation 移动物体分割 | v2<br>LiDAR<br>self-supervised learning<br>autonomous driving<br>moving object segmentation | Input: LiDAR point clouds LiDAR点云<br>Step1: Explore temporal overlapping points 探索时间重叠点<br>Step2: Learn spatiotemporal representations 学习时空表征<br>Step3: Pre-train the model with occupancy states using TOP 使用TOP预训练模型<br>Output: Enhanced moving object segmentation model 改进的运动物体分割模型 |
8.5 | [[8.5] 2503.07204 Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from motion](https://arxiv.org/abs/2503.07204) <br> [{'name': 'Mona Sheikh Zeinoddin, Mobarakol Islam, Zafer Tandogdu, Greg Shaw, Mathew J. Clarkson, Evangelos Mazomenos, Danail Stoyanov'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>robotic-assisted surgery<br>pose estimation<br>depth estimation | Input: Endoscopic images 内窥镜图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm modification 算法修改<br>Step3: Depth and pose estimation 深度和姿态估计<br>Output: Improved pose and depth estimates 改进的姿态和深度估计 |
8.5 | [[8.5] 2503.07234 CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting](https://arxiv.org/abs/2503.07234) <br> [{'name': 'Haicheng Liao, Hanlin Kong, Bonan Wang, Chengyue Wang, Wang Ye, Zhengbing He, Chengzhong Xu, Zhenning Li'}] | Autonomous Driving 自动驾驶 | v2<br>motion forecasting<br>autonomous driving<br>large language models<br>chain-of-thought prompting | Input: Textual data from traffic scenes 交通场景文本数据<br>Step1: Knowledge distillation 知识提取<br>Step2: Chain-of-thought prompting 逐步推理提示<br>Step3: Motion forecasting model evaluation 运动预测模型评估<br>Output: Accurate motion forecasts 准确的运动预测 |
8.5 | [[8.5] 2503.07347 DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection](https://arxiv.org/abs/2503.07347) <br> [{'name': 'Johan Edstedt, Georg B\\"okman, M{\\aa}rten Wadenb\\"ack, Michael Felsberg'}] | Keypoint Detection 关键点检测 | v2<br>keypoint detection<br>Structure-from-Motion (SfM)<br>reinforcement learning<br>multi-view geometry | Input: Images with keypoints 图像及关键点<br>Step1: Self-supervised reinforcement learning for keypoint detection 自监督强化学习进行关键点检测<br>Step2: Balanced top-K sampling strategy 平衡的Top-K采样策略<br>Step3: Point-wise maximum knowledge distillation point-wise maximum知识蒸馏<br>Output: Diverse keypoint detector 多样化的关键点检测器 |
8.5 | [[8.5] 2503.07353 Certifiably Optimal Anisotropic Rotation Averaging](https://arxiv.org/abs/2503.07353) <br> [{'name': 'Carl Olsson, Yaroslava Lochman, Johan Malmport, Christopher Zach'}] | Rotation Averaging 旋转平均 | v2<br>rotation averaging<br>SLAM | Input: Relative rotation measurements 相对旋转测量<br>Step1: Incorporation of anisotropic costs 引入各向异性成本<br>Step2: Strong convex relaxation 强的凸松弛<br>Step3: Global optimization 全球优化<br>Output: Certifiably optimal rotation estimations 可证明最优的旋转估计 |
8.5 | [[8.5] 2503.07367 LEGO-Motion: Learning-Enhanced Grids with Occupancy Instance Modeling for Class-Agnostic Motion Prediction](https://arxiv.org/abs/2503.07367) <br> [{'name': 'Kangan Qian, Jinyu Miao, Ziang Luo, Zheng Fu, and Jinchen Li, Yining Shi, Yunlong Wang, Kun Jiang, Mengmeng Yang, Diange Yang'}] | Autonomous Driving 自动驾驶 | v2<br>motion prediction<br>occupancy modeling<br>autonomous driving<br>3D modeling | Input: Multi-view perceptions from LiDAR and camera data LiDAR与相机数据的多视角感知<br>Step 1: Instance feature extraction 实例特征提取<br>Step 2: Integration into Bird's Eye View (BEV) space 引入鸟瞰视图空间<br>Step 3: Motion prediction estimation 运动预测估计<br>Output: Accurate motion trajectories and interaction models 准确的运动轨迹和交互模型 |
8.5 | [[8.5] 2503.07375 Probabilistic Segmentation for Robust Field of View Estimation](https://arxiv.org/abs/2503.07375) <br> [{'name': 'R. Spencer Hallyburton, David Hunt, Yiwei He, Judy He, Miroslav Pajic'}] | Autonomous Systems and Robotics 自动驾驶与机器人 | v2<br>FOV estimation<br>security-aware sensor fusion<br>autonomous vehicles<br>deep learning | Input: Sensor data from autonomous vehicles 自动驾驶车辆的传感器数据<br>Step1: Ray tracing and concave hull algorithms for FOV estimation 使用光线追踪和凹壳算法进行视野估计<br>Step2: Dataset creation with ground truth FOV labels 创建包含真实视野标签的数据集<br>Step3: Learning-based segmentation model for FOV feature extraction 学习激励的分割模型提取视野特征<br>Output: Robust FOV estimations for autonomous systems 为自主系统提供稳健的视野估计 |
8.5 | [[8.5] 2503.07413 REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding](https://arxiv.org/abs/2503.07413) <br> [{'name': 'Yan Tai, Luhao Zhu, Zhiqiang Chen, Ynan Ding, Yiying Dong, Xiaohong Liu, Guodong Guo'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Visual Decoding Tasks<br>Multimodal Learning | Input: Large-scale multimodal dataset 大规模多模态数据集<br>Step1: Model architecture design 模型架构设计<br>Step2: Triplet-Based Referring Paradigm implementation 三元组引用范式实现<br>Step3: Performance evaluation and comparison 性能评估与比较<br>Output: Enhanced visual task performance 改进的视觉任务性能 |
8.5 | [[8.5] 2503.07417 GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts](https://arxiv.org/abs/2503.07417) <br> [{'name': 'Minwen Liao, Hao Bo Dong, Xinyi Wang, Ziyang Yan, Yihua Shao'}] | Low-Light Image Enhancement 低光图像增强 | v2<br>Low-Light Image Enhancement<br>Mixture-of-Experts<br>Autonomous Driving<br>3D Reconstruction | Input: Low-light images 低光图像<br>Step1: Dynamic gated weight adjustment 动态门控权重调整<br>Step2: Sub-expert network integration 子专家网络集成<br>Step3: Multi-scale feature fusion 多尺度特征融合<br>Output: Enhanced images 改进的图像 |
8.5 | [[8.5] 2503.07472 A Review on Geometry and Surface Inspection in 3D Concrete Printing](https://arxiv.org/abs/2503.07472) <br> [{'name': 'K. Mawas, M. Maboudi, M. Gerke'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D concrete printing<br>geometry inspection<br>quality control<br>additive manufacturing | Input: 3D concrete printing techniques 3D混凝土打印技术<br>Step1: Overview of quality control methods 质量控制方法综述<br>Step2: Data capture technology investigation 数据采集技术研究<br>Step3: Automated data capture planning 自动化数据采集规划<br>Output: Strategies for quality control and inspection 质量控制和检验策略 |
8.5 | [[8.5] 2503.07485 Chameleon: Fast-slow Neuro-symbolic Lane Topology Extraction](https://arxiv.org/abs/2503.07485) <br> [{'name': 'Zongzheng Zhang, Xinrun Li, Sizhe Zou, Guoxuan Chi, Siqi Li, Xuchong Qiu, Guoliang Wang, Guantian Zheng, Leichen Wang, Hang Zhao, Hao Zhao'}] | Autonomous Driving 自动驾驶 | v2<br>lane topology extraction<br>autonomous driving<br>vision-language models | Input: Multi-view images 多视角图像<br>Step1: Traffic element detection 交通元素检测<br>Step2: Lane topology extraction 道路拓扑提取<br>Step3: Program synthesis 程序合成<br>Output: Lane topology models 道路拓扑模型 |
8.5 | [[8.5] 2503.07499 AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements](https://arxiv.org/abs/2503.07499) <br> [{'name': 'Calvin Yeung, Tomohiro Suzuki, Ryota Tanaka, Zhuoer Yin, Keisuke Fujii'}] | 3D Human Pose Estimation 三维人体姿态估计 | v2<br>3D pose estimation<br>athletic movements<br>dataset | Input: Monocular 3D pose estimation models 单目三维姿态估计模型<br>Step 1: Dataset introduction 数据集引入<br>Step 2: Model evaluation 模型评估<br>Step 3: Fine-tuning on AthletePose3D 在AthletePose3D上微调模型<br>Output: Reduced MPJPE and kinematic validation 减少MPJPE和运动学验证 |
8.5 | [[8.5] 2503.07587 Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru](https://arxiv.org/abs/2503.07587) <br> [{'name': 'Dunant Cusipuma, David Ortega, Victor Flores-Benites, Arturo Deza'}] | Autonomous Systems and Robotics 自主系统与机器人技术 | v2<br>Visual Question Answering 视觉问答<br>Autonomous Driving 自动驾驶<br>Foundational Visual Language Models 基础视觉语言模型 | Input: Dashcam video data from Peru 佩鲁的行车记录仪视频数据<br>Step1: Dataset creation 数据集创建<br>Step2: Multi-modal Visual Question Answering framework 多模态视觉问答框架<br>Step3: Cognitive alignment analysis 认知对齐分析<br>Output: Insights on VLM and human response alignment VLM与人类响应对齐的见解 |
8.5 | [[8.5] 2503.07598 VACE: All-in-One Video Creation and Editing](https://arxiv.org/abs/2503.07598) <br> [{'name': 'Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, Yu Liu'}] | Image and Video Generation 图像与视频生成 | v2<br>video generation<br>video editing<br>unified framework | Input: Video inputs for various tasks 视频输入用于不同的任务<br>Step 1: Organize inputs into a unified interface 组织输入到一个统一界面<br>Step 2: Utilize a Context Adapter for task differentiation 使用上下文适配器进行任务区分<br>Step 3: Perform video generation and editing tasks 进行视频生成和编辑任务<br>Output: Comprehensive video content creation outputs 综合视频内容生成输出 |
8.0 | [[8.0] 2503.06071 TransParking: A Dual-Decoder Transformer Framework with Soft Localization for End-to-End Automatic Parking](https://arxiv.org/abs/2503.06071) <br> [{'name': 'Hangyu Du, Chee-Meng Chew'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>trajectory prediction | Input: Camera-captured data 摄像头捕获的数据<br>Step1: Sample trajectory points from expert trajectories 从专家轨迹中提取轨迹点<br>Step2: Dual-decoder structure for future trajectory prediction 使用双解码器结构进行未来轨迹预测<br>Step3: Output future trajectory coordinates 输出未来轨迹坐标 |
8.0 | [[8.0] 2503.06794 Silent Hazards of Token Reduction in Vision-Language Models: The Hidden Impact on Consistency](https://arxiv.org/abs/2503.06794) <br> [{'name': 'Yizheng Sun, Hao Li, Chang Xu, Chenghua Lin, Riza Batista-Navarro, Jingyuan Sun'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Token Reduction<br>Computational Cost | Step 1: Analyze token reduction methods 分析令牌减少方法<br>Step 2: Apply Singular Value Decomposition (SVD) 施加奇异值分解<br>Step 3: Propose a training-free token reduction method 提出一种无训练的令牌减少方法<br>Output: Improved output consistency and reduced costs 输出：改善的输出一致性和降低的成本 |
7.5 | [[7.5] 2503.05977 Is Your Video Language Model a Reliable Judge?](https://arxiv.org/abs/2503.05977) <br> [{'name': 'Ming Liu, Wensheng Zhang'}] | VLMs & VLA 视觉语言模型与视觉语言对齐 | v2<br>Video Language Models<br>Evaluation Methods<br>Collective Intelligence | Input: Video inputs 视频输入<br>Step1: Evaluate VLMs using traditional methods 传统方法评估VLM<br>Step2: Analyze reliability issues of VLM evaluators 分析VLM评估者的可靠性问题<br>Step3: Implement collective judgments from multiple VLMs 实施多个VLM的集体判断<br>Output: Improved evaluation framework 改进的评估框架 |
7.5 | [[7.5] 2503.06100 Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained Patch Strategy and Depth Integrity-Prior](https://arxiv.org/abs/2503.06100) <br> [{'name': 'Xianjie Liu, Keren Fu, Qijun Zhao'}] | 3D Understanding and Depth Estimation 三维理解与深度估计 | v2<br>3D understanding<br>depth maps<br>image segmentation | Input: High-resolution natural images 高分辨率自然图像<br>Step1: Fusion of multi-modal inputs 多模态输入融合<br>Step2: Enhancement through patch selection and fine-grained strategy 通过补丁选择和细粒度策略增强<br>Step3: Application of integrity-prior loss 应用完整性优先损失<br>Output: High-precision segmentation 高精度分割 |
7.5 | [[7.5] 2503.06223 Reinforced Diffuser for Red Teaming Large Vision-Language Models](https://arxiv.org/abs/2503.06223) <br> [{'name': 'Ruofan Wang, Xiang Zheng, Xiaosen Wang, Cong Wang, Xingjun Ma'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>toxic text continuation<br>reinforcement learning<br>Black-box attack | Input: Large Vision-Language Models (VLMs) 大型视觉语言模型<br>Step1: Greedy search for high-quality image prompts 对高质量图像提示进行贪婪搜索<br>Step2: Fine-tuning of a diffusion model through reinforcement learning 通过强化学习对扩散模型进行微调<br>Output: Toxicity-amplified outputs 增强毒性的输出 |
7.5 | [[7.5] 2503.06621 Dynamic Updates for Language Adaptation in Visual-Language Tracking](https://arxiv.org/abs/2503.06621) <br> [{'name': 'Xiaohai Li, Bineng Zhong, Qihua Liang, Zhiyi Mo, Jian Nong, Shuxiang Song'}] | Vision-Language Models 视觉语言模型 | v2<br>vision-language tracking<br>dynamic updates<br>multi-modal references | Input: Multi-modal references 多模态参考<br>Step1: Dynamic Language Update Module 动态语言更新模块<br>Step2: Dynamic Template Capture Module 动态模板捕获模块<br>Step3: Update strategy assessment 更新策略评估<br>Output: Updated multi-modal references 更新的多模态参考 |
7.5 | [[7.5] 2503.06626 DiffCLIP: Differential Attention Meets CLIP](https://arxiv.org/abs/2503.06626) <br> [{'name': 'Hasan Abed Al Kader Hammoud, Bernard Ghanem'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>vision-language model 视觉语言模型<br>differential attention 差异注意力 | Input: Vision-language model 视觉语言模型<br>Step1: Integrate differential attention into CLIP 将差异注意力集成到CLIP中<br>Step2: Model performance evaluation 模型性能评估<br>Output: Improved multi-modal representation 改进的多模态表示 |
7.5 | [[7.5] 2503.06887 Accessing the Effect of Phyllotaxy and Planting Density on Light Use Efficiency in Field-Grown Maize using 3D Reconstructions](https://arxiv.org/abs/2503.06887) <br> [{'name': 'Nasla Saleem, Talukder Zaki Jubery, Aditya Balu, Yan Zhou, Yawei Li, Patrick S. Schnable, Adarsh Krishnamurthy, Baskar Ganapathysubramanian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>light interception<br>agriculture | Input: 3D reconstructions of maize plants 玉米植物的三维重建<br>Step1: Data integration 数据集成<br>Step2: Light interception modeling 光拦截建模<br>Step3: Virtual field construction 虚拟田地构建<br>Output: Analyzed planting density effects on light interception 分析种植密度对光拦截的影响 |
7.5 | [[7.5] 2503.06974 Asymmetric Visual Semantic Embedding Framework for Efficient Vision-Language Alignment](https://arxiv.org/abs/2503.06974) <br> [{'name': 'Yang Liu, Mengyuan Liu, Shudong Huang, Jiancheng Lv'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language alignment 视觉语言对齐<br>semantic similarity 语义相似性 | Input: Images and text inputs 图像与文本输入<br>Step1: Radial bias sampling for image patches 径向偏差采样图像补丁<br>Step2: Feature extraction and embedding 特征提取与嵌入<br>Step3: Dynamic similarity calculation between embeddings 嵌入之间动态相似性计算<br>Output: Visual semantic similarity scores 输出: 视觉语义相似性分数 |
7.5 | [[7.5] 2503.07419 Analysis of 3D Urticaceae Pollen Classification Using Deep Learning Models](https://arxiv.org/abs/2503.07419) <br> [{'name': 'Tijs Konijn, Imaan Bijl, Lu Cao, Fons Verbeek'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D classification<br>pollen classification<br>deep learning | Input: Whole stack of 3D images 整个三维图像堆栈<br>Step1: Data collection 数据收集<br>Step2: Model training 模型训练<br>Step3: Performance evaluation 性能评估<br>Output: Classification results 分类结果 |
7.5 | [[7.5] 2503.07575 VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/abs/2503.07575) <br> [{'name': 'Jen-tse Huang, Jiantong Qin, Jianping Zhang, Youliang Yuan, Wenxuan Wang, Jieyu Zhao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>bias<br>Vision-Language Models<br>VLMs<br>social biases | Input: Vision-Language Models (VLMs) 视觉语言模型<br>Step1: Task design 任务设计<br>Step2: Bias assessment 偏见评估<br>Step3: Model evaluation 模型评估<br>Output: Analysis of biases 偏见分析 |
7.5 | [[7.5] 2503.07588 When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning](https://arxiv.org/abs/2503.07588) <br> [{'name': 'Junwei Luo, Yingying Zhang, Xue Yang, Kang Wu, Qi Zhu, Lei Liang, Jingdong Chen, Yansheng Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Remote Sensing<br>Token Pruning | Input: Large Remote Sensing Images (RSIs) 大型遥感图像<br>Step1: Implement Region Focus Module (RFM) 实现区域聚焦模块<br>Step2: Apply Dynamic Image Pyramid (DIP) to select image tiles 应用动态图像金字塔（DIP）选择图像瓦片<br>Step3: Perform token pruning based on RFM outputs 根据RFM输出执行token修剪<br>Output: Efficiently processed visual tokens for LVLM 高效处理的视觉token用于LVLM |
7.0 | [[7.0] 2503.07416 TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for Versatile and Efficient Diffusion Models in Vision](https://arxiv.org/abs/2503.07416) <br> [{'name': 'Shaobin Zhuang, Yiwei Guo, Yanbo Ding, Kunchang Li, Xinyuan Chen, Yaohui Wang, Fangyikang Wang, Ying Zhang, Chen Li, Yali Wang'}] | Image Generation 图像生成 | v2<br>Diffusion Models<br>Visual Generation | Input: Diffusion model parameters 和扩散模型参数<br>Step1: Fostering stage with diverse LoRAs 丰厚阶段采用多种LoRA<br>Step2: Core-context collaboration of experts 核心-上下文专家协作<br>Output: Enhanced generative capabilities 改进的生成能力 |


## Arxiv 2025-03-10

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.04919 FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement](https://arxiv.org/abs/2503.04919) <br> [{'name': 'Ian Huang, Yanan Bao, Karen Truong, Howard Zhou, Cordelia Schmid, Leonidas Guibas, Alireza Fathi'}] | 3D Generation 三维生成 | v2<br>3D scene generation<br>object placement<br>large language models<br>geometric reasoning | Input: 3D scene, 3D object, language prompt (3D场景、3D对象、语言提示)<br>Step1: Extract geometric details (提取几何细节)<br>Step2: Construct geometric constraints (构建几何约束)<br>Step3: Prune placements based on common sense (基于常识修剪位置)<br>Output: Edited USD file representing the scene with the object placed (表示带有放置对象的场景的编辑过的USD文件) |
9.5 | [[9.5] 2503.04927 Metadata-free Georegistration of Ground and Airborne Imagery](https://arxiv.org/abs/2503.04927) <br> [{'name': 'Adam Bredvik, Scott Richardson, Daniel Crispell'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>georegistration<br>neural radiance fields | Input: Ground and airborne imagery 地面和空中影像<br>Step1: Generate novel views using NeRF 通过NeRF生成新视图<br>Step2: Establish correspondences for georegistration 建立对应关系进行地理注册<br>Step3: Apply transformation for alignment 应用变换进行对齐<br>Output: Georegistered 3D models 输出: 地理注册的三维模型 |
9.5 | [[9.5] 2503.05086 Fake It To Make It: Virtual Multiviews to Enhance Monocular Indoor Semantic Scene Completion](https://arxiv.org/abs/2503.05086) <br> [{'name': 'Anith Selvakumar, Manasa Bharadwaj'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic completion<br>novel view synthesis<br>multiview fusion | Input: Single RGB image 单个 RGB 图像<br>Step1: Virtual camera placement 虚拟相机放置<br>Step2: Novel view synthesis 新视图合成<br>Step3: Multiview fusion 多视角融合<br>Output: 3D semantic occupancy map 3D 语义占用地图 |
9.5 | [[9.5] 2503.05127 HexPlane Representation for 3D Semantic Scene Understanding](https://arxiv.org/abs/2503.05127) <br> [{'name': 'Zeren Chen, Yuenan Hou, Yulin Chen, Li Liu, Xiao Sun, Lu Sheng'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic scene understanding 3D语义场景理解<br>HexPlane representation HexPlane表示<br>point cloud processing 点云处理 | Input: 3D point cloud 3D点云<br>Step1: Project the point cloud into six planes 将点云投影到六个平面<br>Step2: Extract features using a 2D encoder 使用2D编码器提取特征<br>Step3: Adaptive fusion of features using the HexPlane Association Module (HAM) 使用HexPlane关联模块（HAM）自适应融合特征<br>Output: Classification and segmentation results 分类和分割结果 |
9.5 | [[9.5] 2503.05161 GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three Orthographic Views Using 3D Gaussian Splatting](https://arxiv.org/abs/2503.05161) <br> [{'name': 'Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>CAD reconstruction<br>self-supervised learning<br>sparse-view reconstruction | Input: Raster CAD sketches 像素CAD草图<br>Step1: Transforming CAD sketches into natural image representations 将CAD草图转换为自然图像表示<br>Step2: Manual calculation of camera poses for orthographic views 对正投影视角的相机姿态进行手动计算<br>Step3: Employing sparse-view 3D reconstruction method 采用稀疏视图3D重建方法<br>Output: High-quality 3D CAD models 高质量的3D CAD模型 |
9.5 | [[9.5] 2503.05162 EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D Gaussian Representation](https://arxiv.org/abs/2503.05162) <br> [{'name': 'Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>dynamic scenes<br>compression | Input: Dynamic scene data 动态场景数据<br>Step1: Align Gaussian model using optical flow 引导高斯模型对齐<br>Step2: Refine Gaussian model with point updates 对高斯模型进行点更新重建<br>Output: Streamable volumetric video 可流式传输的体积视频 |
9.5 | [[9.5] 2503.05182 MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions](https://arxiv.org/abs/2503.05182) <br> [{'name': 'Qingyuan Zhou, Yuehu Gong, Weidong Yang, Jiaze Li, Yeqi Luo, Baixin Xu, Shuhao Li, Ben Fei, Ying He'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>surface reconstruction<br>Gaussian splatting | Input: Multi-view images 多视角图像<br>Step1: Mutual optimization of 2D-GS and 3D-GS 2D-GS和3D-GS的相互优化<br>Step2: Geometry-guided illumination decomposition 几何引导的光照分解<br>Step3: Surface reconstruction and rendering surfaces 重建表面和可视化<br>Output: High-fidelity 3D surfaces and realistic renderings 高保真3D表面和真实渲染 |
9.5 | [[9.5] 2503.05217 Separability Membrane: 3D Active Contour for Point Cloud Surface Reconstruction](https://arxiv.org/abs/2503.05217) <br> [{'name': 'Gulpi Qorik Oktagalu Pratamasunu, Guoqing Hao, Kazuhiro Fukui'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>surface reconstruction<br>point cloud | Input: 3D point cloud data 3D 点云数据<br>Step1: Define surface boundary 定义表面边界<br>Step2: Maximize class separability 最大化类别可分性<br>Step3: Adjust membrane properties 动态调整膜属性<br>Output: Reconstructed surface model 重建的表面模型 |
9.5 | [[9.5] 2503.05332 CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images](https://arxiv.org/abs/2503.05332) <br> [{'name': 'Jungho Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting | Input: Motion-blurred images 运动模糊图像<br>Step1: Predict continuous camera trajectories 预测连续相机轨迹<br>Step2: Model rigid body transformations 模型刚体变换<br>Step3: Introduce continuous motion refinement transformation 引入连续运动优化变换<br>Output: Accurate 3D scene reconstruction 准确的3D场景重建 |
9.5 | [[9.5] 2503.05492 FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework](https://arxiv.org/abs/2503.05492) <br> [{'name': 'Haotian Hu, Jingwei Xu, Fanyi Wang, Toyota Li, Yaonong Wang, Laifeng Hu, Zhiwang Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving | Input: High-definition images 高分辨率图像<br>Step1: Query generation 查询生成<br>Step2: Decoder optimization 解码器优化<br>Step3: Performance validation 性能验证<br>Output: Vectorized HD maps 矢量化的高清地图 |
9.5 | [[9.5] 2503.05549 Stereo Any Video: Temporally Consistent Stereo Matching](https://arxiv.org/abs/2503.05549) <br> [{'name': 'Junpeng Jing, Weixun Luo, Ye Mao, Krystian Mikolajczyk'}] | Multi-view Stereo 多视角立体 | v2<br>video stereo matching<br>temporal coherence<br>3D reconstruction | Input: Stereo video sequences 立体视频序列<br>Step1: Feature extraction 特征提取<br>Step2: All-to-all-pair correlation construction 全对全对比构建<br>Step3: Temporal convex upsampling mechanism 时间凸上采样机制<br>Output: Accurate and temporally consistent disparities 准确且时间一致的视差 |
9.2 | [[9.2] 2503.05082 Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs](https://arxiv.org/abs/2503.05082) <br> [{'name': 'Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Gaussian Splatting<br>video diffusion models<br>scene modeling<br>extrapolation<br>occlusion | Input: Sparse input data from multiple views 多视角的稀疏输入数据<br>Step1: Generate sequences using video diffusion models 利用视频扩散模型生成序列<br>Step2: Introduce scene-grounding guidance scenes for consistency 引入场景引导指导以确保一致性<br>Step3: Optimize 3D Gaussian Splatting based on generated sequences 基于生成序列优化3D高斯点淋<br>Output: Enhanced 3D scene representations 改进的3D场景表示 |
9.0 | [[9.0] 2503.04953 Spectral Informed Mamba for Robust Point Cloud Processing](https://arxiv.org/abs/2503.04953) <br> [{'name': 'Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers'}] | Point Cloud Processing 点云处理 | v2<br>3D reconstruction<br>point cloud processing<br>autonomous driving | Input: Point cloud data 点云数据<br>Step1: Graph Laplacian spectrum exploitation 利用图拉普拉斯谱<br>Step2: Recursive patch partitioning strategy 递归补丁划分策略<br>Step3: Token placement restoration 令牌位置恢复<br>Output: Enhanced point cloud analysis 改进的点云分析 |
9.0 | [[9.0] 2503.05174 SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting](https://arxiv.org/abs/2503.05174) <br> [{'name': 'Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>6-DoF pose estimation<br>3D Gaussian Splatting<br>RGB image | Input: Single RGB image 单个RGB图像<br>Step1: Dual-Attention Ray Scoring Network (DARS-Net) 强调几何评分<br>Step2: Coarse-to-fine optimization process 粗到细优化流程<br>Output: High-precision 6-DoF pose estimates 高精度6-DoF姿态估计 |
8.5 | [[8.5] 2503.04863 Manboformer: Learning Gaussian Representations via Spatial-temporal Attention Mechanism](https://arxiv.org/abs/2503.04863) <br> [{'name': 'Ziyue Zhao, Qining Qi, Jianfa Ma'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D semantic occupancy prediction<br>Gaussian representation<br>autonomous driving | Input: 3D Gaussian representations and temporal data 3D高斯表示与时间数据<br>Step1: Integrate sensor data 集成传感器数据<br>Step2: Optimize Gaussian representations with self-attention mechanisms 使用自注意力机制优化高斯表示<br>Step3: Evaluate model performance on the NuScenes dataset 在NuScenes数据集上评估模型性能<br>Output: Enhanced 3D semantic occupancy predictions 改进的3D语义占用预测 |
8.5 | [[8.5] 2503.04877 Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning](https://arxiv.org/abs/2503.04877) <br> [{'name': 'Albert Wilcox, Mohamed Ghanem, Masoud Moghani, Pierre Barroso, Benjamin Joffe, Animesh Garg'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D representation<br>imitation learning<br>zero-shot transfer | Input: RGBD data from calibrated cameras 经过校准的RGBD相机数据<br>Step1: Extract semantic features from a pretrained 2D model 从预训练的2D模型中提取语义特征<br>Step2: Localize semantic information in 3D 对3D中的语义信息进行定位<br>Step3: Reduce point cloud to a single vector 将点云简化为单一向量<br>Output: Conditioning vector for imitation learning algorithms 模拟学习算法的条件向量 |
8.5 | [[8.5] 2503.05283 Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces](https://arxiv.org/abs/2503.05283) <br> [{'name': 'Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodol\\`a, Simone Melzi, Maks Ovsjanikov'}] | Vision-Language Alignment (VLA) 视觉语言对齐 | v2<br>3D representation<br>text features<br>alignment<br>multi-modal learning | Input: Uni-modal 3D encoders and text encoders 3D编码器和文本编码器<br>Step1: Feature extraction 特征提取<br>Step2: Subspace projection 子空间投影<br>Step3: Alignment of subspaces 子空间对齐<br>Output: Improved latent space alignment 改进的潜在空间对齐 |
8.5 | [[8.5] 2503.05578 Novel Object 6D Pose Estimation with a Single Reference View](https://arxiv.org/abs/2503.05578) <br> [{'name': 'Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian'}] | Pose Estimation 位置估计 | v2<br>6D pose estimation<br>single reference view<br>robotics | Input: Single reference view 单一参考视图<br>Step1: Establish point-wise alignment 建立逐点对齐<br>Step2: Iterative refinement process 迭代优化过程<br>Step3: Pose estimation 位置估计<br>Output: 6D pose of the novel object 新颖物体的6D位置 |
8.5 | [[8.5] 2503.05638 TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models](https://arxiv.org/abs/2503.05638) <br> [{'name': 'Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>monocular videos<br>camera trajectory<br>3D reconstruction<br>video generation | Input: Monocular videos 单目视频<br>Step1: Disentangling view transformations 视图变换解耦<br>Step2: Dual-stream conditional video diffusion model 双流条件视频扩散模型<br>Step3: Dataset curation 数据集整理<br>Output: Videos with redirected camera trajectories 重定向相机轨迹的视频 |
8.5 | [[8.5] 2503.05689 GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving](https://arxiv.org/abs/2503.05689) <br> [{'name': 'Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He, Qian Zhang, Xiaoxiao Long, Wei Yin'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>multimodal trajectories<br>goal-driven generation | Input: Sensor data (images and LiDAR) 传感器数据（图像和激光雷达）<br>Step1: Data fusion 数据融合<br>Step2: Goal point selection 目标点选择<br>Step3: Trajectory generation using flow matching 采用流匹配生成轨迹<br>Output: High-quality multimodal trajectories 高质量的多模态轨迹 |
8.0 | [[8.0] 2503.04918 Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed Environments: Vision-Language Model Approach](https://arxiv.org/abs/2503.04918) <br> [{'name': 'Soumyadeep Ro, Sanapala Satwika, Pamarthi Yasoda Gayathri, Mohmmad Ghaith Balsha, Aysegul Ucar'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>object detection 对象检测<br>unstructured environments 无结构环境 | Input: Florence 2 model and unstructured environments (Florence 2 模型和无结构环境)<br>Step 1: Fine-tuning model parameters (调整模型参数)<br>Step 2: Experimentation with GPUs and optimizers (使用GPU和优化器进行实验)<br>Step 3: Performance analysis using Mean Average Precision (使用平均精度均值(mAP)分析性能)<br>Output: Enhanced object detection capabilities (增强的目标检测能力) |
7.5 | [[7.5] 2503.04816 Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks](https://arxiv.org/abs/2503.04816) <br> [{'name': 'Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Graph Neural Networks<br>3D Pose Estimation<br>Dance<br>Reconstruction | Input: Dance videos 舞蹈视频<br>Step1: Video-to-3D pose extraction 视频到3D姿态提取<br>Step2: Data preprocessing 数据预处理<br>Step3: Apply Graph Neural Networks (GNNs) 应用图神经网络<br>Output: Weight connections between dancers 输出：舞者之间的权重连接 |
7.5 | [[7.5] 2503.04839 Advancing Multimodal In-Context Learning in Large Vision-Language Models with Task-aware Demonstrations](https://arxiv.org/abs/2503.04839) <br> [{'name': 'Yanshu Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal in-context learning<br>Vision-Language Models<br>task-aware attention | Input: Images and text samples 图像与文本样本<br>Step1: Task-aware attention mechanism 任务感知注意力机制<br>Step2: In-context demonstration selection 上下文演示选择<br>Step3: Empirical testing and evaluation 实证测试与评估<br>Output: Enhanced in-context learning performance 改进的上下文学习表现 |
7.5 | [[7.5] 2503.04871 Toward Lightweight and Fast Decoders for Diffusion Models in Image and Video Generation](https://arxiv.org/abs/2503.04871) <br> [{'name': 'Alexey Buzovkin, Evgeny Shilov'}] | Image and Video Generation 图像生成与视频生成 | v2<br>image generation<br>video generation<br>diffusion models<br>lightweight decoders | Input: Custom-trained decoders for image and video synthesis 自定义训练的解码器用于图像和视频合成<br>Step1: Design lightweight decoders 设计轻量级解码器<br>Step2: Implement dual masking strategies 实施双掩码策略<br>Step3: Evaluate performance and efficiency 评估性能和效率<br>Output: Optimized image and video generation 优化的图像和视频生成 |


## Arxiv 2025-03-07

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.03907 Neural Descriptors: Self-Supervised Learning of Robust Local Surface Descriptors Using Polynomial Patches](https://arxiv.org/abs/2503.03907) <br> [{'name': 'Gal Yona, Roy Velich, Ron Kimmel, Ehud Rivlin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D surfaces 3D表面<br>shape descriptors 形状描述符 | Input: 3D surfaces 3D表面<br>Step 1: Synthetic data generation 合成数据生成<br>Step 2: Neural architecture design 神经网络架构设计<br>Step 3: Feature extraction 特征提取<br>Output: Robust geometric features 可靠的几何特征 |
9.5 | [[9.5] 2503.04030 Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration](https://arxiv.org/abs/2503.04030) <br> [{'name': 'Aocheng Li, James R. Zimmer-Dauphinee, Rajesh Kalyanam, Ian Lindsay, Parker VanValkenburgh, Steven Wernke, Daniel Aliaga'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud completion<br>3D reconstruction<br>archaeological restoration | Input: Original point clouds 原始点云<br>Step1: Project point clouds into multiple-center-of-projection (MCOP) images 将点云投影到多中心投影（MCOP）图像中<br>Step2: Perform self-supervised inpainting on MCOP images to fill in missing pixels 对MCOP图像进行自监督修复以填充缺失的像素<br>Step3: Map completed MCOP images back to 3D space 将完成的MCOP图像映射回3D空间<br>Output: Final restored 3D point clouds 输出: 最终恢复的3D点云 |
9.5 | [[9.5] 2503.04034 GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world Scene Understanding](https://arxiv.org/abs/2503.04034) <br> [{'name': 'Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu'}] | 3D Scene Graph Generation 3D场景图生成 | v2<br>3D Gaussian Splatting<br>scene graph generation<br>semantic segmentation<br>spatial reasoning | Input: 3D scene data 3D场景数据<br>Step1: 2D feature extraction 2D特征提取<br>Step2: Gaussian clustering 高斯聚类<br>Step3: 3D correction module implementation 3D校正模块实现<br>Output: 3D scene graph 3D场景图 |
9.5 | [[9.5] 2503.04079 Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene Rendering](https://arxiv.org/abs/2503.04079) <br> [{'name': 'Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction 3D重建<br>Gaussian Surfels 高斯涂抹<br>surgical scene rendering 手术场景渲染 | Input: Monocular endoscopic video 单目内窥镜视频<br>Step1: Gaussian Surfel initialization 高斯涂抹初始化<br>Step2: Surface alignment surfaces 进行表面对齐<br>Step3: Motion field prediction 运动场预测<br>Step4: Depth data integration 深度数据集成<br>Output: Accurate 3D reconstruction of surgical scenes 准确的手术场景3D重建 |
9.5 | [[9.5] 2503.04082 Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting](https://arxiv.org/abs/2503.04082) <br> [{'name': 'Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>photorealistic rendering<br>surgical instruments | Input: Monocular surgical videos 单目外科视频<br>Step1: Geometry pre-training 几何预训练<br>Step2: Gaussian point cloud binding 高斯点云绑定<br>Step3: Novel instrument pose tracking method 新的器械姿态跟踪方法<br>Output: Controllable 3D reconstruction 可控的三维重建 |
9.5 | [[9.5] 2503.04235 Geometry-Constrained Monocular Scale Estimation Using Semantic Segmentation for Dynamic Scenes](https://arxiv.org/abs/2503.04235) <br> [{'name': 'Hui Zhang, Zhiyang Wu, Qianqian Shangguan, Kang An'}] | Autonomous Driving 自动驾驶 | v2<br>monocular visual odometry<br>ego-motion estimation | Input: Monocular images 单目图像<br>Step1: Dynamic object segmentation 动态物体分割<br>Step2: Feature point selection 特征点选择<br>Step3: Triangulation and scale recovery 三角测量与尺度恢复<br>Output: Accurate ego-motion estimation 准确的自我运动估计 |
9.5 | [[9.5] 2503.04314 S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting](https://arxiv.org/abs/2503.04314) <br> [{'name': 'Yecong Wan, Mingwen Shao, Yuanshuo Cheng, Wangmeng Zuo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>sparse views<br>super-resolution | Input: Sparse low-resolution views 稀疏低分辨率视图<br>Step1: Optimize low-resolution Gaussian representation 优化低分辨率高斯表示<br>Step2: Initialize high-resolution Gaussians 初始化高分辨率高斯<br>Step3: Refine high-resolution Gaussians with super-resolved images 使用超分辨率图像细化高分辨率高斯<br>Output: High-quality 3D scene reconstruction 高质量3D场景重建 |
9.5 | [[9.5] 2503.04351 PLMP -- Point-Line Minimal Problems for Projective SfM](https://arxiv.org/abs/2503.04351) <br> [{'name': 'Kim Kiehn, Albin Ahlb\\"ack, Kathl\\\'en Kohn'}] | Structure from Motion (SfM) 运动结构估计 | v2<br>Structure-from-Motion<br>SfM<br>minimal problems | Input: Uncalibrated images from multiple pinhole cameras 未校准的多个针孔相机图像<br>Step1: Classify minimal problems for SfM 根据SfM分类最小问题<br>Step2: Compute number of solutions 计算每个最小问题的解的数量<br>Step3: Develop systematic factorization strategy 发展系统化的因式分解策略<br>Output: Catalog of minimal problems for SfM 关于SfM的最小问题目录 |
9.5 | [[9.5] 2503.04376 MIDAS: Modeling Ground-Truth Distributions with Dark Knowledge for Domain Generalized Stereo Matching](https://arxiv.org/abs/2503.04376) <br> [{'name': 'Peng Xu, Zhiyu Xiang, Jingyun Fu, Tianyu Pu, Hanzhi Zhong, Eryun Liu'}] | Multi-view Stereo 多视角立体 | v2<br>stereo matching<br>domain generalization<br>depth estimation | Input: Synthetic and real datasets 合成与真实数据集<br>Step1: Extract dark knowledge from pre-trained network 从预训练网络中提取暗知识<br>Step2: Model multi-modal distributions for stereo matching 为立体匹配建模多模态分布<br>Step3: Train stereo networks with fine-grained supervision 使用细粒度监督训练立体网络<br>Output: Enhanced generalization performance 改进的泛化性能 |
9.5 | [[9.5] 2503.04513 A Novel Solution for Drone Photogrammetry with Low-overlap Aerial Images using Monocular Depth Estimation](https://arxiv.org/abs/2503.04513) <br> [{'name': 'Jiageng Zhong, Qi Zhou, Ming Li, Armin Gruen, Xuan Liao'}] | 3D Reconstruction 三维重建 | v2<br>Aerial Photogrammetry<br>Monocular Depth Estimation<br>3D Reconstruction | Input: Aerial images with POS information 航空图像与位置信息<br>Step1: Perform aerial triangulation 进行航空三角测量<br>Step2: Generate depth maps using monocular depth estimation 生成单目深度图<br>Step3: Recover metric depth using tie points 利用特征点恢复度量深度<br>Step4: Convert depth maps to point clouds 转换深度图为点云<br>Step5: Generate Digital Surface Models (DSMs) 生成数字表面模型 |
9.2 | [[9.2] 2503.04059 H3O: Hyper-Efficient 3D Occupancy Prediction with Heterogeneous Supervision](https://arxiv.org/abs/2503.04059) <br> [{'name': 'Yunxiao Shi, Hong Cai, Amin Ansari, Fatih Porikli'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D occupancy prediction<br>autonomous driving | Input: Multi-camera images 多摄像头图像<br>Step1: Generate 3D reference points 生成三维参考点<br>Step2: Average features from different views 融合不同视图的特征<br>Step3: Differentiable volume rendering 可微体积渲染<br>Output: 3D occupancy predictions 三维占用预测 |
9.2 | [[9.2] 2503.04127 Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image Matching and 3D Registration](https://arxiv.org/abs/2503.04127) <br> [{'name': 'Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang'}] | 3D Registration 立体配准 | v2<br>3D registration<br>image matching<br>diffusion model | Input: 2D images and 3D point clouds 2D图像和3D点云<br>Step1: Establish correspondence 设定对应关系<br>Step2: Apply diffusion model 应用扩散模型<br>Step3: Matrix optimization 矩阵优化<br>Output: Enhanced registration results 改进的配准结果 |
9.0 | [[9.0] 2503.04718 Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation](https://arxiv.org/abs/2503.04718) <br> [{'name': 'David T. Hoffmann, Syed Haseeb Raza, Hanqiu Jiang, Denis Tananaev, Steffen Klingenhoefer, Martin Meinke'}] | 3D Reconstruction 三维重建 | v2<br>scene flow estimation<br>3D reconstruction<br>autonomous robotics | Input: Point cloud sequences 点云序列<br>Step1: Analyze existing methods 分析现有方法<br>Step2: Develop a voxel grid-based model 开发基于体素网格的模型<br>Step3: Introduce a new multiframe loss formulation 引入新的多帧损失公式<br>Output: Fast scene flow estimation method 快速场景流估计方法 |
8.5 | [[8.5] 2503.03882 IC-Mapper: Instance-Centric Spatio-Temporal Modeling for Online Vectorized Map Construction](https://arxiv.org/abs/2503.03882) <br> [{'name': 'Jiangtong Zhu, Zhao Yang, Yinan Shi, Jianwu Fang, Jianru Xue'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>online mapping<br>autonomous driving<br>3D reconstruction | Input: Visual data from multiple frames 多帧视觉数据<br>Step1: Instance-centric temporal association for matching instances across frames 实例中心时间关联以匹配跨帧实例<br>Step2: Spatial fusion module to integrate historical map data with current detections 空间融合模块将历史地图数据与当前检测结果集成<br>Output: Globally updated vectorized map 全球更新的矢量地图 |
8.5 | [[8.5] 2503.03947 COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for Off-Road Semantic Segmentation](https://arxiv.org/abs/2503.03947) <br> [{'name': 'Aurelio Noca, Xianmei Lei, Jonathan Becktor, Jeffrey Edlund, Anna Sabel, Patrick Spieler, Curtis Padgett, Alexandre Alahi, Deegan Atha'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>off-road semantic segmentation 越野语义分割<br>domain adaptation 领域适应<br>autonomous driving 自动驾驶 | Input: Sparse coarse in-domain labels and densely labeled out-of-domain data 稀疏的粗略域内标签和密集的域外数据<br>Step1: Data integration 数据集成<br>Step2: Framework development 框架开发<br>Step3: Model evaluation 模型评估<br>Output: Enhanced semantic segmentation model 改进的语义分割模型 |
8.5 | [[8.5] 2503.04078 Spatial-Temporal Perception with Causal Inference for Naturalistic Driving Action Recognition](https://arxiv.org/abs/2503.04078) <br> [{'name': 'Qing Chang, Wei Dai, Zhihao Shuai, Limin Yu, Yutao Yue'}] | Autonomous Driving 自动驾驶 | v2<br>naturalistic driving action recognition<br>spatial-temporal features<br>causal inference | Input: RGB video clips RGB视频片段<br>Step1: Extract temporal and spatial distance features 提取时间和空间距离特征<br>Step2: Joint encoding of features 特征的联合编码<br>Step3: Behavior recognition and temporal action localization 行为识别和时间动作定位<br>Output: Enhanced driver action recognition framework 改进的驾驶行为识别框架 |
8.5 | [[8.5] 2503.04154 CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection](https://arxiv.org/abs/2503.04154) <br> [{'name': 'Chupeng Liu, Runkai Zhao, Weidong Cai'}] | Monocular 3D Detection 单目3D检测 | v2<br>3D detection 3D检测<br>weak supervision 弱监督<br>monocular vision 单目视觉 | Input: Monocular images 单目图像<br>Step1: Pre-training stage with ROCM 预训练阶段引入区域级对象对比匹配<br>Step2: Pseudo-label training with D2OD mechanism 伪标签训练阶段引入双重到单一蒸馏机制<br>Output: Context-aware 3D object detection 具有上下文感知的三维对象检测 |
8.5 | [[8.5] 2503.04171 DuCos: Duality Constrained Depth Super-Resolution via Foundation Model](https://arxiv.org/abs/2503.04171) <br> [{'name': 'Zhiqiang Yan, Zhengxue Wang, Haoye Dong, Jun Li, Jian Yang, Gim Hee Lee'}] | Depth Estimation 深度估计 | v2<br>Depth Super-Resolution 深度超分辨率<br>3D Reconstruction 三维重建 | Input: Low-resolution depth inputs 低分辨率深度输入<br>Step1: Data reformulation 数据重构<br>Step2: Integration of prompt learning 结合提示学习<br>Step3: Application of Lagrangian duality theory 应用拉格朗日对偶理论<br>Output: High-resolution depth maps 高分辨率深度图 |
8.5 | [[8.5] 2503.04199 MASTER: Multimodal Segmentation with Text Prompts](https://arxiv.org/abs/2503.04199) <br> [{'name': 'Fuyang Liu, Shun Lu, Jilin Mei, Yu Hu'}] | Multimodal Learning 多模态学习 | v2<br>Multimodal Fusion<br>Autonomous Driving<br>Large Language Model | Input: RGB and thermal images with text prompts<br>Step1: Dual-path processing for RGB and thermal data<br>Step2: Integration of large language models for semantic guidance<br>Step3: Generation of learnable codebook tokens<br>Step4: Image decoding for semantic segmentation results<br>Output: Semantic segmentation masks for autonomous driving scenarios |
8.5 | [[8.5] 2503.04322 A Modular Pipeline for 3D Object Tracking Using RGB Cameras](https://arxiv.org/abs/2503.04322) <br> [{'name': 'Lars Bredereke, Yale Hartmann, Tanja Schultz'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object tracking<br>multiple cameras<br>RGB cameras<br>computer vision<br>object detection | Input: Multi-view RGB camera data 多视角RGB摄像头数据<br>Step1: Object detection using YOLO 检测对象使用YOLO<br>Step2: Camera pose optimization 相机姿态优化<br>Step3: 3D trajectory calculation 计算3D轨迹<br>Output: 3D trajectories of objects 对象的3D轨迹 |
8.5 | [[8.5] 2503.04420 PointsToWood: A deep learning framework for complete canopy leaf-wood segmentation of TLS data across diverse European forests](https://arxiv.org/abs/2503.04420) <br> [{'name': 'Harry J. F. Owen, Matthew J. A. Allen, Stuart W. D. Grieve, Phill Wilkes, Emily R. Lines'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>semantic segmentation<br>Terrestrial Laser Scanning<br>TLS<br>deep learning | Input: 3D point clouds from Terrestrial Laser Scanning (TLS) 三维点云<br>Step1: Data processing 数据处理<br>Step2: Semantic segmentation of wood and leaves 语义分割木材和叶子<br>Step3: Model evaluation 模型评估<br>Output: Reliable segmentation results 可信的分割结果 |
8.5 | [[8.5] 2503.04501 IMFine: 3D Inpainting via Geometry-guided Multi-view Refinement](https://arxiv.org/abs/2503.04501) <br> [{'name': 'Zhihao Shi, Dong Huo, Yuhongze Zhou, Kejia Yin, Yan Min, Juwei Lu, Xinxin Zuo'}] | 3D Inpainting 三维修复 | v2<br>3D reconstruction<br>inpainting 修复<br>autonomous driving 自动驾驶 | Input: Multi-view images 多视角图像<br>Step1: Inpainting with image models 基于图像模型进行修复<br>Step2: Coarse geometry restoration 粗略几何恢复<br>Step3: Warping inpainted content 变形修复内容<br>Step4: Multi-view refinement model refinement 多视角细化模型调整<br>Output: Consistent multi-view inpainted scenes 一致的多视角修复场景 |
8.5 | [[8.5] 2503.04641 Simulating the Real World: A Unified Survey of Multimodal Generative Models](https://arxiv.org/abs/2503.04641) <br> [{'name': 'Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong'}] | 3D Generation 三维生成 | v2<br>multimodal generative models<br>3D generation<br>real-world simulation<br>unified survey | Input: Multimodal data dimensions 多模态数据维度<br>Step1: Review 2D generation methods 复习2D生成方法<br>Step2: Review video generation methods 复习视频生成方法<br>Step3: Review 3D generation methods 复习3D生成方法<br>Step4: Review 4D generation methods 复习4D生成方法<br>Output: Unified framework for multimodal generative models 输出：多模态生成模型的统一框架 |
8.5 | [[8.5] 2503.04665 Implicit Neural Representation for Video and Image Super-Resolution](https://arxiv.org/abs/2503.04665) <br> [{'name': 'Mary Aiyetigbo, Wanqi Yuan, Feng Luo, Nianyi Li'}] | Image Generation 图像生成 | v2<br>super-resolution<br>implicit neural representation<br>3D reconstruction | Input: Low-resolution images and videos 低分辨率图像和视频<br>Step1: Hierarchical grid-based encoding 层次网格编码<br>Step2: Implicit neural representation construction 隐式神经表示构建<br>Step3: High-resolution reconstruction 高分辨率重建<br>Output: Enhanced high-resolution images and videos 改进的高分辨率图像和视频 |
8.5 | [[8.5] 2503.04720 FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video](https://arxiv.org/abs/2503.04720) <br> [{'name': 'Yue Gao, Hong-Xing Yu, Bo Zhu, Jiajun Wu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Fluid Reconstruction<br>Video Prediction<br>Physics Simulation | Input: Single video 单一视频<br>Step1: Video-to-video synthesis 视频到视频合成<br>Step2: 4D fluid reconstruction 四维流体重建<br>Step3: Interaction simulation 交互模拟<br>Output: Predictive fluid motion 预测流体运动 |
7.5 | [[7.5] 2503.03848 Nexar Dashcam Collision Prediction Dataset and Challenge](https://arxiv.org/abs/2503.03848) <br> [{'name': 'Daniel C. Moura, Shizhan Zhu, Orly Zvitia'}] | Autonomous Driving 自动驾驶 | v2<br>collision prediction<br>autonomous vehicle<br>video analysis | Input: Annotated video clips 经过注释的视频片段<br>Step1: Dataset construction 数据集构建<br>Step2: Labeling event types 事件类型标注<br>Step3: Machine learning model training 机器学习模型训练<br>Output: Collision prediction 模型预测碰撞 |
7.5 | [[7.5] 2503.04151 Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation](https://arxiv.org/abs/2503.04151) <br> [{'name': 'Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu'}] | Multi-view Stereo 多视角立体 | v2<br>multi-view learning<br>contrastive learning<br>representation fusion | Input: Multi-view data 多视角数据<br>Step1: Transform heterogeneous data to homogeneous embeddings 将异构数据转换为同质嵌入<br>Step2: Apply sample-level attention for fusion 应用样本级注意力进行融合<br>Step3: Generate perturbed versions for contrastive learning 生成扰动版本以进行对比学习<br>Output: Discriminative multi-view representations 输出：区分性多视角表示 |
7.5 | [[7.5] 2503.04606 The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606) <br> [{'name': 'Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang'}] | Image and Video Generation 图像生成与视频生成 | v2<br>text-to-video generation<br>Hybrid framework<br>Diffusion models | Input: Textual descriptions 文本描述<br>Step1: Semantic tokenization 语义标记化<br>Step2: Coarse video generation 粗略视频生成<br>Step3: Detail refinement 细节完善<br>Output: High-fidelity video 高保真视频 |


## Arxiv 2025-03-06

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.03115 NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics](https://arxiv.org/abs/2503.03115) <br> [{'name': 'Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, Qing Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>thermal imaging<br>dynamic modeling<br>NTR-Gaussian | Input: Multi-view TIR images and synthetic VIS images 多视角热成像图像与合成可见光图像<br>Step1: Predict thermodynamic parameters 预测热力学参数<br>Step2: Learn a 4D spatiotemporal representation 学习4D时空表示<br>Step3: Forecast temperatures at various time intervals 预测不同时间间隔的温度<br>Output: Dynamic 3D thermal models 动态三维热模型 |
9.5 | [[9.5] 2503.03190 DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering](https://arxiv.org/abs/2503.03190) <br> [{'name': 'Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin'}] | 3D Question Answering 三维问答 | v2<br>3D Question Answering<br>Multi-view images<br>Point clouds | Input: Multi-view images and 3D point clouds 多视角图像和三维点云<br>Step1: Text-guided Multi-view Fusion (TGMF) module for feature integration TGMF模块进行特征集成<br>Step2: Adaptive Dual-vision Perception (ADVP) module for fusing features ADVP模块进行特征融合<br>Step3: Multimodal Context-guided Reasoning (MCGR) for robust reasoning MCGR进行稳健推理<br>Output: Enhanced scene comprehension 改进的场景理解 |
9.5 | [[9.5] 2503.03280 BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation](https://arxiv.org/abs/2503.03280) <br> [{'name': 'Hiep Truong Cong, Ajay Kumar Sigatapu, Arindam Das, Yashwanth Sharma, Venkatesh Satagopan, Ganesh Sistu, Ciaran Eising'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人技术 | v2<br>autonomous driving<br>sensor fusion<br>moving object segmentation | Input: Multimodal sensor data including cameras, LiDAR, and radar 多模态传感器数据，包括相机、激光雷达和雷达<br>Step1: Sensor data integration 传感器数据整合<br>Step2: Deformable cross-attention guided sensor fusion 可变形交叉注意力引导的传感器融合<br>Step3: Output moving object segmentation in BEV 生成鸟瞩图中的移动物体分割结果 |
9.5 | [[9.5] 2503.03299 Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters](https://arxiv.org/abs/2503.03299) <br> [{'name': 'Julia Hindel, Rohit Mohan, Jelena Bratuli\\`c, Daniele Cattaneo, Thomas Brox, Abhinav Valada'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR semantic segmentation<br>3D vision<br>autonomous driving | Input: LiDAR data and pre-trained vision models LiDAR 数据和预训练视觉模型<br>Step1: Encoding range-view and bird’s-eye-view features 编码范围视图和鸟瞰视图特征<br>Step2: Adapting pre-trained models using a 2D-3D adapter 使用二维-三维适配器适配预训练模型<br>Step3: Refining segmentation outputs with complementary branches 通过补充分支细化分割输出<br>Output: Label-efficient 3D semantic segmentation results 标签高效的三维语义分割结果 |
9.5 | [[9.5] 2503.03543 A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction](https://arxiv.org/abs/2503.03543) <br> [{'name': 'Dragos Costea, Alina Marcu, Marius Leordeanu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>novel view synthesis<br>neural rendering | Input: Recorded videos 记录的视频<br>Step1: Self-supervised learning 自监督学习<br>Step2: Neural rendering and geometric analysis 神经渲染与几何分析<br>Step3: Image restoration and reconstruction 图像修复与重建<br>Output: Enhanced RGB and mesh reconstructions 改进的RGB和网格重建 |
9.5 | [[9.5] 2503.03548 Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case](https://arxiv.org/abs/2503.03548) <br> [{'name': 'Milin Patel, Rolf Jung'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object detection<br>LiDAR<br>SOTIF<br>autonomous driving | Input: LiDAR point cloud dataset from SOTIF simulations<br>Step1: Define and model SOTIF-related Use Case<br>Step2: Generate LiDAR point cloud dataset incorporating diverse weather conditions<br>Step3: Evaluate 3D object detection methods using MMDetection3D and OpenPCDet<br>Output: Performance metrics (Average Precision and Recall) for 3D object detection |
9.5 | [[9.5] 2503.03664 A Generative Approach to High Fidelity 3D Reconstruction from Text Data](https://arxiv.org/abs/2503.03664) <br> [{'name': 'Venkat Kumar R, Deepak Saravanan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Text-to-3D Reconstruction<br>Generative AI<br>Computer Vision | Input: Textual descriptions 文本描述<br>Step1: Text-to-image generation 文本到图像生成<br>Step2: Image enhancement using reinforcement learning 图像增强<br>Step3: Reflection removal using Stable Delight 反射去除<br>Step4: 2D to volumetric 3D model transformation 2D到体积3D模型转化<br>Output: Detailed 3D models 详细的3D模型 |
9.5 | [[9.5] 2503.03751 GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control](https://arxiv.org/abs/2503.03751) <br> [{'name': 'Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas M\\"uller, Alexander Keller, Sanja Fidler, Jun Gao'}] | 3D Generation 三维生成 | v2<br>3D consistency<br>novel view synthesis<br>video generation | Input: Seed images and previously generated frames 种子图像和之前生成的帧<br>Step1: Predict pixel-wise depth 预测像素级深度<br>Step2: Construct 3D cache 构建3D缓存<br>Step3: Condition video generation on 2D renderings 使用2D渲染进行视频生成<br>Output: High-fidelity video output 高保真视频输出 |
9.2 | [[9.2] 2503.03726 Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames](https://arxiv.org/abs/2503.03726) <br> [{'name': 'Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander'}] | Multi-view and Stereo Vision 多视角和立体视觉 | v2<br>6D Object Pose 6D物体姿态<br>Multi-View Optimization 多视角优化<br>Active Vision 主动视觉 | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Estimate 3D translation 估计3D平移<br>Step2: Estimate 3D orientation 估计3D朝向<br>Step3: Predict next best camera viewpoint 预测下一个最佳相机视角<br>Output: Enhanced object pose estimation 改进的物体姿态估计 |
9.0 | [[9.0] 2503.03430 CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization](https://arxiv.org/abs/2503.03430) <br> [{'name': 'Junhao Xu, Yanan Zhang, Zhi Cai, Di Huang'}] | Autonomous Driving 自动驾驶 | v2<br>collaborative perception<br>3D object detection<br>autonomous driving | Input: Multi-agent perception data 多智能体感知数据<br>Step1: Model supply-demand relationship 建模供需关系<br>Step2: Refine collaboration region selection 精炼协作区域选择<br>Step3: Implement intermediate-late hybrid collaboration 实施中后期混合协作<br>Output: Optimized detection results 优化的检测结果 |
8.5 | [[8.5] 2503.02916 Monocular Person Localization under Camera Ego-motion](https://arxiv.org/abs/2503.02916) <br> [{'name': 'Yu Zhan, Hanjing Ye, Hong Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>monocular localization<br>3D position estimation<br>human-robot interaction | Input: Monocular images 单目图像<br>Step1: 2D-3D correspondence estimation 2D-3D对应估计<br>Step2: Camera attitude optimization 相机姿态优化<br>Step3: 3D location estimation 3D位置估计<br>Output: 3D human localization 3D人类定位 |
8.5 | [[8.5] 2503.03132 Dynamic Neural Surfaces for Elastic 4D Shape Representation and Analysis](https://arxiv.org/abs/2503.03132) <br> [{'name': 'Awais Nizamani, Hamid Laga, Guanjin Wang, Farid Boussaid, Mohammed Bennamoun, Anuj Srivastava'}] | 3D Shape Analysis 三维形状分析 | v2<br>4D shape analysis<br>D-SNS<br>neural representations | Input: Discrete 4D surfaces 离散的4D表面<br>Step1: Develop continuous representations 开发连续表示<br>Step2: Perform spatiotemporal registration 执行时空配准<br>Step3: Compute geodesics and mean shapes 计算测地线和均值形状<br>Output: Shape analysis results 形状分析结果 |
8.5 | [[8.5] 2503.03222 Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture](https://arxiv.org/abs/2503.03222) <br> [{'name': 'Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D motion reconstruction<br>2D data<br>absolute positioning | Input: Monocular 2D data from various sources 输入: 各种来源的单目2D数据<br>Step1: Pretraining a single-view diffusion model using 2D data 第一步: 使用2D数据预训练单视图扩散模型<br>Step2: Fine-tuning a multi-view diffusion model for view consistency 第二步: 针对视图一致性进行多视角扩散模型的微调<br>Step3: Gradual recovery of global movements from 2D poses 第三步: 从2D姿态逐步恢复全局运动<br>Output: Accurate 3D motion reconstruction with absolute positioning 输出: 具有绝对定位的精确3D运动重建 |
8.5 | [[8.5] 2503.03259 BANet: Bilateral Aggregation Network for Mobile Stereo Matching](https://arxiv.org/abs/2503.03259) <br> [{'name': 'Gangwei Xu, Jiaxin Liu, Xianqi Wang, Junda Cheng, Yong Deng, Jinliang Zang, Yurui Chen, Xin Yang'}] | Stereo Matching 立体匹配 | v2<br>stereo matching<br>depth estimation<br>mobile deployment | Input: Stereo image pairs 立体图像对<br>Step1: Separate full cost volume into detailed and smooth volumes 将完整的成本体积分为详细和光滑体积<br>Step2: Perform detailed and smooth aggregations 分别执行详细和光滑聚合<br>Step3: Fuse both to obtain the final disparity map 结合两者以获得最终的视差图<br>Output: High-quality disparity map with preserved edges and details 输出：高质量的视差图，保留边缘和细节 |
8.5 | [[8.5] 2503.03367 Top-K Maximum Intensity Projection Priors for 3D Liver Vessel Segmentation](https://arxiv.org/abs/2503.03367) <br> [{'name': 'Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>liver vessel segmentation<br>maximum intensity projection<br>latent diffusion model | Input: CT images of liver vessels 肝脏血管的 CT 图像<br>Step1: Compute top-k maximum intensity projections 计算 top-k 最大强度投影<br>Step2: Condition a latent diffusion model 训练潜在扩散模型<br>Step3: Generate 3D liver vessel trees 生成 3D 肝脏血管树<br>Output: Enhanced liver vessel segmentation results 提高的肝脏血管分割结果 |
8.5 | [[8.5] 2503.03599 REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation](https://arxiv.org/abs/2503.03599) <br> [{'name': "D\\'ebora N. P. Oliveira, Joshua Knights, Sebasti\\'an Barbas Laina, Simon Boche, Wolfram Burgard, Stefan Leutenegger"}] | Robotic Perception 机器人感知 | v2<br>3D reconstruction<br>re-localization<br>LiDAR<br>graph-based methods<br>autonomous systems | Input: LiDAR-based submaps 基于LiDAR的子地图<br>Step1: Semantic classification of LiDAR scans LiDAR扫描的语义分类<br>Step2: Generation of global descriptors from object features 从对象特征生成全局描述符<br>Step3: Identification of revisits using geometric consistency 通过几何一致性识别重访<br>Output: Efficient place recognition and registration 高效的地点识别和注册 |
8.5 | [[8.5] 2503.03689 DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance](https://arxiv.org/abs/2503.03689) <br> [{'name': 'Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu'}] | Image and Video Generation 图像生成与视频生成 | v2<br>3D reconstruction<br>video generation<br>autonomous driving<br>conditional generation | Input: Multi-view images and occupancy grid map 多视角图像和占据网格图<br>Step1: Data integration 数据集成<br>Step2: Dual-branch model development 双分支模型开发<br>Step3: Semantic fusion and attention mechanism 语义融合和关注机制<br>Step4: Reward-guided video generation 奖励引导的视频生成<br>Output: High-fidelity video and scene generation 高保真视频和场景生成 |


## Arxiv 2025-03-05

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.01899 FASTer: Focal Token Acquiring-and-Scaling Transformer for Long-term 3D Object Detection](https://arxiv.org/abs/2503.01899) <br> [{'name': 'Chenxu Dang, Zaipeng Duan, Pei An, Xinmin Zhang, Xuzhong Hu, Jie Ma'}] | 3D Object Detection 三维物体检测 | v2<br>3D object detection<br>Lidar<br>Adaptive Scaling | Input: Lidar point cloud sequences 激光雷达点云序列<br>Step1: Dynamic selection of focal tokens 动态选择焦点令牌<br>Step2: Apply Adaptive Scaling mechanism 应用自适应缩放机制<br>Step3: Hierarchical fusion of spatial and temporal information 空间与时间信息的分层融合<br>Output: Efficient 3D object detection results 高效的三维物体检测结果 |
9.5 | [[9.5] 2503.02195 HyperGCT: A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration](https://arxiv.org/abs/2503.02195) <br> [{'name': 'Xiyu Zhang, Jiayi Ma, Jianwei Guo, Wei Hu, Zhaoshuai Qi, Fei Hui, Jiaqi Yang, Yanning Zhang'}] | 3D Registration 三维配准 | v2<br>3D registration<br>geometric constraints<br>hypergraph | Input: Point clouds and correspondences 点云和对应关系<br>Step1: Model relationships using hypergraphs 使用超图建模关系<br>Step2: Dynamic feature aggregation 动态特征聚合<br>Step3: Generate hypotheses based on optimized constraints 基于优化约束生成假设<br>Output: Enhanced 3D registration performance 改进的三维配准性能 |
9.5 | [[9.5] 2503.02223 DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting](https://arxiv.org/abs/2503.02223) <br> [{'name': 'Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>object perception<br>object-SLAM<br>3D reconstruction<br>Gaussian Splatting | Input: RGB-D and instance frames RGB-D 和实例帧<br>Step1: Object pose estimation 物体姿态估计<br>Step2: Object reconstruction 物体重建<br>Step3: Decouple objects from the scene 从场景中解耦物体<br>Output: Enhanced object-level map 改进的物体级地图 |
9.5 | [[9.5] 2503.02375 mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction](https://arxiv.org/abs/2503.02375) <br> [{'name': 'Jiarui Yang, Songpengcheng Xia, Zengyuan Lai, Lan Sun, Qi Wu, Wenxian Yu, Ling Pei'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>mmWave radar<br>point cloud enhancement<br>human body reconstruction | Input: Sparse mmWave point clouds<br>Step1: Enhance point clouds using temporal features and 2D masks<br>Step2: Apply a multi-stage completion network<br>Step3: 2D-3D fusion of motion features<br>Output: Accurate human body reconstruction |
9.5 | [[9.5] 2503.02388 PIDLoc: Cross-View Pose Optimization Network Inspired by PID Controllers](https://arxiv.org/abs/2503.02388) <br> [{'name': 'Wooju Lee, Juhye Park, Dasol Hong, Changki Sung, Youngwoo Seo, Dongwan Kang, Hyun Myung'}] | Autonomous Driving 自动驾驶 | v2<br>cross-view pose optimization<br>PID controller<br>autonomous driving | Input: RGB images and LiDAR RGB图像和LiDAR<br>Step1: Model cross-view feature relationships 建模跨视图特征关系<br>Step2: Estimate vehicle pose 估计车辆姿态<br>Step3: Adjust pose using PID branches 使用PID分支调整姿态<br>Output: Optimized pose estimation 优化的姿态估计 |
9.5 | [[9.5] 2503.02414 InfoGNN: End-to-end deep learning on mesh via graph neural networks](https://arxiv.org/abs/2503.02414) <br> [{'name': 'Ling Gao, Zhenyu Shu, Shiqing Xin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>mesh data<br>graph neural networks<br>3D modeling | Input: Mesh data 网格数据<br>Step1: Treat mesh as a graph 将网格视为图<br>Step2: Implement InfoConv and InfoMP modules 实现InfoConv和InfoMP模块<br>Step3: Classify and segment meshes 对网格进行分类和分割<br>Output: Enhanced mesh classifications and segmentations 改进的网格分类和分割 |
9.5 | [[9.5] 2503.02606 ARC-Flow : Articulated, Resolution-Agnostic, Correspondence-Free Matching and Interpolation of 3D Shapes Under Flow Fields](https://arxiv.org/abs/2503.02606) <br> [{'name': 'Adam Hartshorne, Allen Paul, Tony Shardlow, Neill D. F. Campbell'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Shapes<br>Interpolation<br>Shape Correspondence | Input: Two 3D articulated shapes 两个3D关节形状<br>Step1: Predict interpolations using flow fields 使用流场预测插值<br>Step2: Automatically estimate dense correspondence 自动估计密集对应关系<br>Step3: Ensure topological consistency 确保拓扑一致性<br>Output: Physically plausible deformation trajectories 物理上合理的变形轨迹 |
9.2 | [[9.2] 2503.02230 Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views](https://arxiv.org/abs/2503.02230) <br> [{'name': 'Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields (NeRF)<br>3D reconstruction<br>sparse-input rendering<br>semantic guidance | Input: Sparse input views 稀疏输入视角<br>Step1: Train teacher NeRF with rendered semantics 训练教师NeRF并渲染语义<br>Step2: Bi-Directional Verification module implementation 实现双向验证模块<br>Step3: Student NeRF training with semantic guidance 训练学生NeRF并传递语义指导<br>Output: Enhanced novel view synthesis results 改进的视图合成结果 |
9.2 | [[9.2] 2503.02558 Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries](https://arxiv.org/abs/2503.02558) <br> [{'name': 'Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>deformation estimation<br>robotic surgery | Input: Video sequences of soft tissue deformations 软组织变形的视频序列<br>Step1: Track key points of soft tissue 跟踪软组织的关键点<br>Step2: Incorporate 2D deformation field into neural implicit reconstruction network 将二维变形场融入神经隐式重建网络<br>Step3: Estimate 3D tissue deformation 估计三维组织变形<br>Output: Accurate 3D mesh and deformation estimations 精确的三维网格和变形估计 |
8.5 | [[8.5] 2503.01930 Road Boundary Detection Using 4D mmWave Radar for Autonomous Driving](https://arxiv.org/abs/2503.01930) <br> [{'name': 'Yuyan Wu, Hae Young Noh'}] | Autonomous Driving 自动驾驶 | v2<br>Road Boundary Detection<br>4D mmWave Radar<br>Autonomous Driving | Input: 4D mmWave radar data 4D毫米波雷达数据<br>Step1: Preprocess point cloud data 点云数据预处理<br>Step2: Segment road boundary points from noisy data 从噪声数据中分割路边界点<br>Step3: Fit boundary curves based on segmented points 基于分割点拟合边界曲线<br>Output: Road boundary detection results 路边界检测结果 |
8.5 | [[8.5] 2503.02009 Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization](https://arxiv.org/abs/2503.02009) <br> [{'name': 'Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>novel-view synthesis | Input: Text prompts and RGBD images 输入: 文本提示和RGBD图像<br>Step1: Stylization control over appearance and shape styles 步骤1: 控制外观和形状样式的风格化<br>Step2: Use of autoregressive 3D Gaussian Splatting pipeline 步骤2: 使用自回归3D高斯点云管道<br>Step3: Depth-informed feature sharing 步骤3: 深度信息共享<br>Output: Stylized 3D models 输出: 风格化的3D模型 |
8.5 | [[8.5] 2503.02092 Data Augmentation for NeRFs in the Low Data Limit](https://arxiv.org/abs/2503.02092) <br> [{'name': 'Ayush Gaggar, Todd D. Murphey'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D reconstruction<br>data augmentation | Input: Sparse training views 稀疏训练视图<br>Step1: Generate posterior uncertainty distribution 生成后验不确定性分布<br>Step2: Sample additional views from the posterior  从后验分布中抽样添加视图<br>Step3: Train the model with augmented data 使用增强的数据训练模型<br>Output: Enhanced scene reconstruction 改进的场景重建 |
8.5 | [[8.5] 2503.02201 MonoLite3D: Lightweight 3D Object Properties Estimation](https://arxiv.org/abs/2503.02201) <br> [{'name': 'Ahmed El-Dawy, Amr El-Zawawi, Mohamed El-Habrouk'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving<br>deep learning<br>object detection | Input: Monocular images 单目图像<br>Step1: 2D object detection 2D目标检测<br>Step2: 3D property estimation 3D属性估计<br>Step3: Model evaluation 模型评估<br>Output: Estimated 3D object properties 估计的3D对象属性 |
8.5 | [[8.5] 2503.02247 WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation](https://arxiv.org/abs/2503.02247) <br> [{'name': 'Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen'}] | Autonomous Systems and Robotics 自动驾驶与机器人系统 | v2<br>Vision-Language Models<br>World Model<br>Object Goal Navigation | Input: Vision-Language Models (VLMs) and environmental observations 视觉语言模型（VLMs）和环境观察<br>Step1: Predict possible outcomes of navigation decisions 预测导航决策的可能结果<br>Step2: Maintain an online Curiosity Value Map for dynamic state retention 在线维护好奇值图以保持动态状态<br>Step3: Implement two-stage action proposer strategy for exploration and localization 实施两阶段行动提议策略以进行探索和定位<br>Output: Navigation decisions and actions 导航决策和行动 |
8.5 | [[8.5] 2503.02372 Label-Efficient LiDAR Panoptic Segmentation](https://arxiv.org/abs/2503.02372) <br> [{'name': 'Ahmet Selim \\c{C}anak\\c{c}{\\i}, Niclas V\\"odisch, K\\"ursat Petek, Wolfram Burgard, Abhinav Valada'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR panoptic segmentation<br>3D segmentation<br>point cloud<br>autonomous systems | Input: Annotated images from which to generate pseudo-labels 从标注图像生成伪标签<br>Step1: Generate panoptic pseudo-labels using a 2D network 使用2D网络生成全景伪标签<br>Step2: Project pseudo-labels onto point clouds 将伪标签投影到点云上<br>Step3: Refine pseudo-labels with a 3D module 使用3D模块优化伪标签<br>Output: High-quality 3D panoptic annotations 高质量的3D全景标注 |
8.5 | [[8.5] 2503.02452 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting](https://arxiv.org/abs/2503.02452) <br> [{'name': 'Qipeng Yan, Mingyang Sun, Lihua Zhang'}] | Neural Rendering 神经渲染 | v2<br>animatable avatars<br>2D Gaussian Splatting<br>real-time rendering | Input: Monocular RGB videos 单目RGB视频<br>Step1: Represent avatar with 2D Gaussian primitives 使用2D高斯原语表示头像<br>Step2: Apply Linear Blend Skinning (LBS) 应用线性混合蒙皮 (LBS)<br>Step3: Render RGB images and normal maps 渲染RGB图像和法线图<br>Output: High-fidelity animatable avatars 输出: 高保真可动画头像 |
8.5 | [[8.5] 2503.02578 TS-CGNet: Temporal-Spatial Fusion Meets Centerline-Guided Diffusion for BEV Mapping](https://arxiv.org/abs/2503.02578) <br> [{'name': 'Xinying Hong, Siyu Li, Kang Zeng, Hao Shi, Bomin Peng, Kailun Yang, Zhiyong Li'}] | Autonomous Driving 自动驾驶 | v2<br>BEV mapping<br>semantic segmentation<br>autonomous driving | Input: Initial generation of semantic maps using visual information<br>Step1: Incorporate historical information using transformation matrices<br>Step2: Apply centerline information to enhance segmentation reconstruction<br>Output: Generate robust BEV semantic segmentation maps |
8.5 | [[8.5] 2503.02593 CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework](https://arxiv.org/abs/2503.02593) <br> [{'name': 'Yanlong Xu, Haoxuan Qu, Jun Liu, Wenxiao Zhang, Xun Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Point Cloud Localization 点云定位<br>Textual Descriptions 文本描述<br>Autonomous Driving 自动驾驶 | Input: Textual descriptions of locations 文本描述位置<br>Step1: Coarse submap retrieval 粗略子地图检索<br>Step2: Fine localization process 精细定位过程<br>Step3: Cauchy-Mixture-Model integration Cauchy混合模型集成<br>Output: Precise 3D localization 精确三维定位 |
8.5 | [[8.5] 2503.02660 A dataset-free approach for self-supervised learning of 3D reflectional symmetries](https://arxiv.org/abs/2503.02660) <br> [{'name': 'Issac Aguirre, Ivan Sipiran, Gabriel Monta\\~nana'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D symmetry detection 3D对称性检测<br>self-supervised learning 自监督学习 | Input: Single object input 单个物体输入<br>Step1: Feature extraction 特征提取<br>Step2: Symmetry detection 对称性检测<br>Step3: Visual descriptor computation 视觉描述符计算<br>Output: Detected symmetries 检测到的对称性 |
8.5 | [[8.5] 2503.02687 Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?](https://arxiv.org/abs/2503.02687) <br> [{'name': 'Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang'}] | 3D Object Detection 3D目标检测 | v2<br>3D perception<br>data augmentation<br>radar point clouds<br>MSDA | Input: Radar point clouds 雷达点云<br>Step1: Identify challenges in MSDA for radar point clouds 确定雷达点云中混合样本数据增强的挑战<br>Step2: Develop Class-Aware PillarMix for data augmentation 开发类感知PillarMix用于数据增强<br>Step3: Assign independent mix ratios at pillar level 在柱级别分配独立混合比<br>Output: Diverse training samples 生成多样化的训练样本 |
7.5 | [[7.5] 2503.02393 Vision-Language Model IP Protection via Prompt-based Learning](https://arxiv.org/abs/2503.02393) <br> [{'name': 'Lianyu Wang, Meng Wang, Huazhu Fu, Daoqiang Zhang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>IP Protection<br>Prompt-based Learning | Input: CLIP model and associated visual and textual data<br>Step1: Develop IP-Prompt utilizing domain tokens and image tokens<br>Step2: Implement style enhancement branch with feature banks<br>Step3: Evaluate model performance using newly designed metrics<br>Output: Models with improved IP protection capabilities |
7.0 | [[7.0] 2503.02063 V$^2$Dial: Unification of Video and Visual Dialog via Multimodal Experts](https://arxiv.org/abs/2503.02063) <br> [{'name': 'Adnen Abdessaied, Anna Rohrbach, Marcus Rohrbach, Andreas Bulling'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal<br>video dialog<br>visual dialog | Input: Image and video input data 图像和视频输入数据<br>Step 1: Learn spatial and temporal features 学习空间和时间特征<br>Step 2: Route data through multimodal experts 将数据通过多模态专家路由<br>Step 3: Align features using matching and contrastive learning techniques 使用匹配和对比学习技术对齐特征<br>Output: Unified model for visual and video dialog 统一的视觉与视频对话模型 |
7.0 | [[7.0] 2503.02199 Words or Vision: Do Vision-Language Models Have Blind Faith in Text?](https://arxiv.org/abs/2503.02199) <br> [{'name': 'Ailin Deng, Tri Cao, Zhirui Chen, Bryan Hooi'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>text bias<br>modality preference | Input: Vision-Language Models for vision-centric tasks 视觉-语言模型用于视觉中心任务<br>Step1: Introduce textual variations to benchmark textual input 在基准测试中引入文本变体<br>Step2: Evaluate VLMs under different conditions 在不同条件下评估视觉-语言模型<br>Step3: Analyze modality preferences and biases 分析模态偏好和偏差<br>Output: Insights on text bias and modality interaction 对文本偏见和模态交互的见解 |
6.2 | [[6.2] 2503.01980 Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval](https://arxiv.org/abs/2503.01980) <br> [{'name': 'Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal retrieval<br>Transformer<br>cross-modal retrieval | Input: Multimodal queries, composed of both an image and text 多模态查询，由图像和文本组成<br>Step1: Feature extraction from visual and textual backbones 特征提取来自视觉和文本主干<br>Step2: Integration of features using a recurrent Transformer cell 使用递归Transformer单元整合特征<br>Step3: Evaluation of retrieval performance on multimodal datasets 在多模态数据集上评估检索性能<br>Output: Enhanced multimodal retrieval performance 增强的多模态检索性能 |


## Arxiv 2025-03-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2503.00167 EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration](https://arxiv.org/abs/2503.00167) <br> [{'name': 'Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>event cameras<br>LiDAR localization<br>pose estimation | Input: LiDAR maps and event camera data 利用激光雷达地图和事件相机数据<br>Step1: Initial pose projection 初始位姿投影<br>Step2: Depth map generation 生成深度图<br>Step3: Events alignment with LiDAR points 事件与激光雷达点的对齐<br>Step4: Camera pose estimation using PnP solver 使用PnP求解器进行相机位姿估计<br>Output: Accurate localization results 精确的定位结果 |
9.5 | [[9.5] 2503.00260 Seeing A 3D World in A Grain of Sand](https://arxiv.org/abs/2503.00260) <br> [{'name': 'Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>miniature scenes<br>3D Gaussian Splatting<br>visual hull | Input: Miniature scenes with 3D objects 微型场景中的三维物体<br>Step1: Design catadioptric imaging system 设计复合成像系统<br>Step2: Capture surrounding multi-view images 捕获周围多视角图像<br>Step3: Implement 3D Gaussian Splatting for reconstruction 实施3D高斯点云重建<br>Step4: Integrate visual hull constraints整合视觉外壳约束<br>Output: 3D reconstructed miniatures and novel views 输出：重建的微型三维场景和新视图 |
9.5 | [[9.5] 2503.00308 Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes](https://arxiv.org/abs/2503.00308) <br> [{'name': 'Yangge Li, Chenxi Ji, Xiangru Zhong, Huan Zhang, Sayan Mitra'}] | Neural Rendering 神经渲染 | v2<br>abstract rendering<br>Gaussian splat<br>autonomous systems<br>uncertainty propagation | Input: Scenes represented by Gaussian splats 场景表示为高斯点云<br>Step1: Transform 3D Gaussians to camera coordinates 将3D高斯变换为相机坐标<br>Step2: Transform to 2D pixel coordinates 转换为2D像素坐标<br>Step3: Compute effective opacity and color 计算有效的不透明度和颜色<br>Step4: Blend colors and sort by distance to image plane 混合颜色并按距离图像平面排序<br>Output: Abstract images representing possible renderings 输出: 表示可能渲染的抽象图像 |
9.5 | [[9.5] 2503.00513 Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning](https://arxiv.org/abs/2503.00513) <br> [{'name': 'Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D scene understanding 3D场景理解<br>large multi-modal model 大型多模态模型<br>instance-aware 实例感知 | Input: 3D scene data and language instructions 3D场景数据和语言指令<br>Step1: Multi-view Cross-Modal Fusion to integrate 2D and 3D features 多视角跨模态融合以集成2D和3D特征<br>Step2: 3D Instance Spatial Relation module to capture spatial relationships 3D实例空间关系模块以捕获空间关系<br>Step3: Multi-task instruction tuning for various 3D tasks 多任务指令调优以应对各种3D任务<br>Output: Instance-aware representations for 3D scene understanding 输出: 适应实例的3D场景理解表示 |
9.5 | [[9.5] 2503.00531 GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation Model](https://arxiv.org/abs/2503.00531) <br> [{'name': 'Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang'}] | 3D Generation 三维生成 | v2<br>3D Gaussian Splatting<br>watermarking<br>copyright protection<br>3D generation | Input: 3D Gaussian Splatting generative model 3D Gaussian Splatting 生成模型<br>Step1: Integrate adaptive bit modulation modules 集成自适应比特调制模块<br>Step2: Embed watermark into model network 在模型网络中嵌入水印<br>Step3: Ensure fidelity of 3D generation 确保三维生成的保真度<br>Output: Watermarked 3D models 带水印的三维模型 |
9.5 | [[9.5] 2503.00675 Dur360BEV: A Real-world Single 360-degree Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving](https://arxiv.org/abs/2503.00675) <br> [{'name': 'Wenke E, Chao Yuan, Li Li, Yixin Sun, Yona Falinie A. Gaus, Amir Atapour-Abarghouei, Toby P. Breckon'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving | Input: Single 360-degree camera and 3D LiDAR 单个360度相机和3D LiDAR<br>Step1: Data collection 数据收集<br>Step2: Spherical-image-to-BEV (SI2BEV) module development 球面图像到鸟瞰图模块开发<br>Step3: Application of Focal Loss for segmentation loss 采用Focal Loss进行分割损失<br>Output: Bird-Eye-View maps from single spherical camera 从单个球面相机生成鸟瞰图 |
9.5 | [[9.5] 2503.00737 Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration](https://arxiv.org/abs/2503.00737) <br> [{'name': 'Jinjiang You, Hewei Wang, Yijie Li, Mingxiao Huo, Long Van Tran Ha, Mingyuan Ma, Jinfeng Xu, Puzhen Wu, Shubham Garg, Wei Pu'}] | Structure from Motion (SfM) 运动结构估计 | v2<br>Structure-from-Motion<br>camera calibration<br>3D reconstruction<br>dense features | Input: Multi-frame images 多帧图像<br>Step1: Density feature extraction 密集特征提取<br>Step2: Extrinsics regularization 外部参数正则化<br>Step3: Dense feature reprojection 密集特征重投影<br>Step4: Joint optimization across multiple frames 多帧联合优化<br>Output: Refined camera intrinsics and 3D reconstructions 精炼的相机内参和三维重建 |
9.5 | [[9.5] 2503.00803 HiMo: High-Speed Objects Motion Compensation in Point Clouds](https://arxiv.org/abs/2503.00803) <br> [{'name': 'Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Processing 点云处理<br>Motion Compensation 运动补偿<br>Autonomous Driving 自动驾驶 | Input: LiDAR point clouds 激光雷达点云<br>Step1: Characterization of point cloud distortions 点云失真特征化<br>Step2: Development of the HiMo undistortion pipeline HiMo去失真流程开发<br>Step3: Scene flow estimation scene flow估计<br>Step4: Evaluation using new metrics 采用新度量进行评估<br>Output: Compensated point clouds 补偿后的点云 |
9.5 | [[9.5] 2503.00848 PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery](https://arxiv.org/abs/2503.00848) <br> [{'name': 'BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>high-frequency recovery | Input: Multi-view images 多视角图像<br>Step1: Create spectral residual significance map 创建光谱残差显著图<br>Step2: Apply depth-aware and depth-smooth losses 应用深度感知和深度平滑损失<br>Step3: Split and clone ellipsoids in high-frequency region 在高频区域内分裂和克隆椭圆体<br>Output: Enhanced scene geometry and textures 改进的场景几何和纹理 |
9.5 | [[9.5] 2503.00853 MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain](https://arxiv.org/abs/2503.00853) <br> [{'name': 'Rui Yi Yong, Samuel Picosson, Arnold Wiliem'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dataset<br>maritime<br>perception metrics<br>SfM | Input: Fly-over videos of maritime scenes 海洋场景的航拍视频<br>Step1: Dataset collection from the internet 数据集收集<br>Step2: Algorithm for 3D reconstruction 3D重建算法<br>Step3: Evaluation using reprojection and perception-based metrics 使用重投影和感知基础度量进行评估<br>Output: 3D models tailored for maritime environments 针对海洋环境的3D模型 |
9.5 | [[9.5] 2503.00881 Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization](https://arxiv.org/abs/2503.00881) <br> [{'name': 'You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>surface reconstruction<br>multi-view images<br>high-quality rendering | Input: Multi-view images 多视角图像<br>Step1: Contribution-adaptive regularization 适应性贡献正则化<br>Step2: Geometry-guided densification strategy 几何引导稠密化策略<br>Step3: Unified model training and evaluation 统一模型训练与评估<br>Output: High-quality rendering and surface reconstruction 高质量渲染与表面重建 |
9.5 | [[9.5] 2503.01109 FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion](https://arxiv.org/abs/2503.01109) <br> [{'name': 'Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>3D reconstruction<br>SLAM<br>gaussian splatting<br>autonomous systems | Input: Multi-view images 多视角图像<br>Step1: Frequency domain analysis 频率域分析<br>Step2: Gaussian initialization 高斯初始化<br>Step3: Sparse and dense map construction 稀疏与密集地图构建<br>Output: Real-time SLAM system 实时SLAM系统 |
9.5 | [[9.5] 2503.01202 A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping](https://arxiv.org/abs/2503.01202) <br> [{'name': 'Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-sensor fusion<br>UAV mapping | Input: Multi-sensor data inputs from GPS, IMU, radar, and camera 输入: 来自 GPS、IMU、雷达和摄像头的多传感器数据<br>Step1: Sensor data calibration 传感器数据校准<br>Step2: Prior-pose-optimized feature matching prior-pose-optimized 特征匹配步骤<br>Step3: SfM process integration 运动结构估计过程集成<br>Output: Accurate orthoimages and 3D spatial reconstruction 输出: 准确的正射影像和三维空间重建 |
9.5 | [[9.5] 2503.01254 Convex Hull-based Algebraic Constraint for Visual Quadric SLAM](https://arxiv.org/abs/2503.01254) <br> [{'name': 'Xiaolong Yu, Junqiao Zhao, Shuangfu Song, Zhongyang Zhu, Zihan Yuan, Chen Ye, Tiantian Feng'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>SLAM<br>object reconstruction<br>convex hull | Input: Image data 图像数据<br>Step1: Identify object landmarks 确定对象地标<br>Step2: Apply convex hull-based constraint 应用凸包代数约束<br>Step3: Integrate into SLAM system 集成到SLAM系统<br>Output: Enhanced mapping and localization 改进的映射和定位 |
9.5 | [[9.5] 2503.01309 OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging](https://arxiv.org/abs/2503.01309) <br> [{'name': 'Yijie Tang, Jiazhao Zhang, Yuqing Lan, Yulan Guo, Dezun Dong, Chenyang Zhu, Kai Xu'}] | 3D Segmentation 3D 分割 | v2<br>3D segmentation 3D 分割<br>online segmentation 在线分割<br>visual foundation models 视觉基础模型 | Input: Sequentially captured frames 逐帧捕获的图像<br>Step1: Generate 2D masks from VFMs 从视觉基础模型产生 2D 掩码<br>Step2: Hash-based spatial organization 基于哈希的空间组织<br>Step3: Merge masks using spatial similarity 基于空间相似性合并掩码<br>Output: Unified 3D instance segmentation 统一的 3D 实例分割 |
9.5 | [[9.5] 2503.01582 Category-level Meta-learned NeRF Priors for Efficient Object Mapping](https://arxiv.org/abs/2503.01582) <br> [{'name': 'Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D mapping<br>NeRF<br>category-level priors<br>object reconstruction<br>autonomous navigation | Input: Category-level shape priors 分类级形状先验<br>Step1: Meta-learning on synthetic datasets 使用合成数据集进行元学习<br>Step2: Integration of lightweight object-level NeRFs 与轻量级对象级NeRF的集成<br>Step3: Optimization using multi-objective genetic algorithm 使用多目标遗传算法进行优化<br>Output: Efficient object mapping framework 高效对象映射框架 |
9.5 | [[9.5] 2503.01646 OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding](https://arxiv.org/abs/2503.01646) <br> [{'name': 'Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Gaussian Splatting<br>Dense Semantic SLAM<br>3D Reconstruction<br>Autonomous Driving | Input: Multi-view images 多视角图像<br>Step1: Semantic label integration 语义标签集成<br>Step2: Gaussian Voting Splatting 高斯投票溅射<br>Step3: Label consensus and segmentation consensus 标签一致性和分割一致性<br>Output: Enhanced scene understanding and mapping 改进的场景理解与地图构建 |
9.5 | [[9.5] 2503.01661 MUSt3R: Multi-view Network for Stereo 3D Reconstruction](https://arxiv.org/abs/2503.01661) <br> [{'name': 'Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, Vincent Leroy'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-view images<br>geometry<br>visual odometry<br>SLAM | Input: Multi-view images 多视角图像<br>Step1: Architecture modification to ensure symmetry 结构修改以确保对称性<br>Step2: Implement memory mechanism 实现内存机制<br>Step3: Prediction of 3D structures in a common coordinate frame 在公共坐标框架中预测3D结构<br>Output: Scalable and efficient 3D reconstruction 可扩展的高效3D重建 |
9.5 | [[9.5] 2503.01774 Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models](https://arxiv.org/abs/2503.01774) <br> [{'name': 'Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>diffusion models<br>neural rendering | Input: Novel views rendered from 3D representations 渲染的三维表示的视图<br>Step1: Use DIFIX to enhance novel views 通过DIFIX增强新视图<br>Step2: Distill enhanced views back into 3D representation 将增强的视图反向提炼到3D表示中<br>Output: Improved 3D model with reduced artifacts 输出：改进的三维模型，减少伪影 |
9.5 | [[9.5] 2503.01845 Denoising Functional Maps: Diffusion Models for Shape Correspondence](https://arxiv.org/abs/2503.01845) <br> [{'name': 'Aleksei Zhuravlev, Zorah L\\"ahner, Vladislav Golyanik'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>shape correspondence<br>denoising diffusion models<br>functional maps<br>3D reconstruction<br>3D modeling | Input: Deformable shapes 可变形形状<br>Step1: Data collection from synthetic human meshes 从合成的人体网格收集数据<br>Step2: Predict functional maps using denoising diffusion models 使用扩散模型预测函数映射<br>Step3: Unsupervised sign correction of eigenvectors 进行特征向量符号的无监督校正<br>Output: Accurate shape correspondences 输出：精确的形状对应 |
9.0 | [[9.0] 2503.00746 DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting](https://arxiv.org/abs/2503.00746) <br> [{'name': 'Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>depth-of-field<br>3D reconstruction<br>multiview images | Input: Multi-view images 多视角图像<br>Step1: Lens-based imaging model development 镜头模型开发<br>Step2: Depth prior adjustments 深度先验调整<br>Step3: Defocus-to-focus adaptation defocus到聚焦适应<br>Output: Controlled depth-of-field 受控制的景深效果 |
9.0 | [[9.0] 2503.01448 Generative Human Geometry Distribution](https://arxiv.org/abs/2503.01448) <br> [{'name': 'Xiangjun Tang, Biao Zhang, Peter Wonka'}] | 3D Generation 三维生成 | v2<br>3D generation<br>human geometry | Input: Pose and normal image input 请输入姿势和法线图像输入<br>Step1: Generate human geometry distribution 生成人体几何分布<br>Step2: Sample high-fidelity human geometry 采样高保真人体几何<br>Output: Realistic 3D human geometries 输出：逼真的3D人类几何 |
8.5 | [[8.5] 2503.00051 Correspondence-Free Pose Estimation with Patterns: A Unified Approach for Multi-Dimensional Vision](https://arxiv.org/abs/2503.00051) <br> [{'name': 'Quan Quan, Dun Dai'}] | Pose Estimation 位姿估计 | v2<br>6D pose estimation<br>correspondence-free methods<br>robot vision<br>optimization | Input: Point sets without explicit correspondences 无显式对应点集<br>Step1: Formulate equations for optimization 公式化优化方程<br>Step2: Solve optimization problems 解决优化问题<br>Output: Estimated pose 估计的姿态 |
8.5 | [[8.5] 2503.00063 NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary](https://arxiv.org/abs/2503.00063) <br> [{'name': 'Zezeng Li, Xiaoyu Du, Na Lei, Liming Chen, Weimin Wang'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>adversarial attack<br>point cloud<br>optimal transport<br>3D models | Input: Point cloud data 点云数据<br>Step1: Calculate OT mapping from noise to feature space 计算从噪声到特征空间的OT映射<br>Step2: Identify singular boundaries 确定奇异边界<br>Step3: Sample along singular boundaries to generate adversarial point clouds 沿奇异边界采样生成对抗点云<br>Output: Adversarial point clouds 生成对抗点云 |
8.5 | [[8.5] 2503.00068 PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing](https://arxiv.org/abs/2503.00068) <br> [{'name': 'Ziyu Wu, Yufan Xiong, Mengting Niu, Fangting Xie, Quan Wan, Qijun Ying, Boyan Liu, Xiaohui Cai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>pressure sensing<br>human shape estimation | Input: Pressure sequences 压力序列<br>Step1: SMPLify-IB optimization to generate high-quality 3D annotations SMPLify-IB 优化生成高质量3D注释<br>Step2: Development of PI-HMR for human shape estimation 开发PI-HMR进行人体形状估计<br>Step3: Model evaluation and comparison with SOTA models 模型评估并与 SOTA 模型比较<br>Output: Accurate 3D human shape models 准确的3D人形模型 |
8.5 | [[8.5] 2503.00357 CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression](https://arxiv.org/abs/2503.00357) <br> [{'name': 'Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, Wen-Hsiao Peng'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>rate-distortion optimization<br>compression | Input: 3D Gaussian primitives 3D高斯原语<br>Step1: Triplane projection onto multi-scale triplanes multipscale triplane投影<br>Step2: Spatial autoregressive coding in 2D planes 二维平面中的空间自回归编码<br>Step3: Channel-wise autoregressive coding for individual Gaussian primitives 针对每个高斯原语的通道自回归编码<br>Output: Rate-distortion-optimized compressed representation 率失真优化的压缩表示 |
8.5 | [[8.5] 2503.00371 Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis](https://arxiv.org/abs/2503.00371) <br> [{'name': 'Xuehao Gao, Yang Yang, Shaoyi Du, Guo-Jun Qi, Junwei Han'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human motion synthesis<br>scene-aware motion analysis<br>text-to-motion generation | Input: Textual command and 3D scene context 文本命令和3D场景<br>Step1: Goal inferring 目标推断<br>Step2: Path planning 路径规划<br>Step3: Pose synthesizing 姿势合成<br>Output: 3D human motions in a scene 3D场景中的人类运动 |
8.5 | [[8.5] 2503.00436 HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning](https://arxiv.org/abs/2503.00436) <br> [{'name': 'Maria Lymperaiou, Giorgos FIlandrianos, Angeliki Dimitriou, Athanasios Voulodimos, Giorgos Stamou'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>hallucination detection<br>explainable evaluation<br>vision-language models<br>image captioning | Input: Vision-language models 视觉语言模型<br>Step1: Identify hallucinations 识别幻觉<br>Step2: Apply conceptual counterfactuals 应用概念反事实<br>Step3: Evaluate and interpret results 评估和解释结果<br>Output: Explainable hallucination detection framework 可解释的幻觉检测框架 |
8.5 | [[8.5] 2503.00518 Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence](https://arxiv.org/abs/2503.00518) <br> [{'name': 'Zhan Qu, Shuzhou Yuan, Michael F\\"arber, Marius Brennfleck, Niklas Wartha, Anton Stephan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Point Cloud Segmentation<br>Wake Vortex Detection<br>LiDAR | Input: LiDAR point cloud data LiDAR点云数据<br>Step1: Semantic segmentation 语义分割<br>Step2: Clustering techniques 聚类技术<br>Step3: Model evaluation 模型评估<br>Output: Wake vortex detection results 醒目涡流检测结果 |
8.5 | [[8.5] 2503.00731 LightEndoStereo: A Real-time Lightweight Stereo Matching Method for Endoscopy Images](https://arxiv.org/abs/2503.00731) <br> [{'name': 'Yang Ding, Can Han, Sijia Du, Yaqi Wang, Dahong Qian'}] | Multi-view Stereo 多视角立体 | v2<br>stereo matching 立体匹配<br>depth estimation 深度估计<br>endoscopy 内窥镜 | Input: Endoscopic images 内窥镜图像<br>Step1: Feature extraction 特征提取<br>Step2: Cost aggregation 成本聚合<br>Step3: Disparity refinement 视差优化<br>Output: Accurate depth information 准确的深度信息 |
8.5 | [[8.5] 2503.00747 Unifying Light Field Perception with Field of Parallax](https://arxiv.org/abs/2503.00747) <br> [{'name': 'Fei Teng, Buyin Deng, Boyuan Zheng, Kai Luo, Kunyu Peng, Jiaming Zhang, Kailun Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Light Field<br>multi-task learning<br>3D reconstruction<br>autonomous driving | Input: Light Field representations 光场表示<br>Step1: Angular-specific difference capture 捕捉特定角度差异<br>Step2: Contextual consistency consolidation 上下文一致性整合<br>Output: Unified features for multi-task learning 多任务学习的统一特征 |
8.5 | [[8.5] 2503.00793 Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning](https://arxiv.org/abs/2503.00793) <br> [{'name': 'Ukcheol Shin, Kyunghyun Lee, Jean Oh'}] | Depth Estimation 深度估计 | v2<br>depth estimation 深度估计<br>multi-spectral fusion 多光谱融合<br>autonomous vehicles 自动驾驶车辆 | Input: Multi-spectral images 多光谱图像<br>Step1: Align embedding spaces 对齐嵌入空间<br>Step2: Minimize contrastive loss 最小化对比损失<br>Step3: Train fusion module 训练融合模块<br>Output: Depth estimation output 深度估计结果 |
8.5 | [[8.5] 2503.00801 STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds](https://arxiv.org/abs/2503.00801) <br> [{'name': 'Zikuan Li, Honghua Chen, Yuecheng Wang, Sibo Wu, Mingqiang Wei, Jun Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>point cloud processing<br>edge extraction<br>thin-walled structures | Input: Unstructured point clouds 无结构点云<br>Step1: Create local spherical curve representation 创建局部球曲线表示<br>Step2: Construct structure-aware neighborhoods 构建结构感知邻域<br>Step3: Classify edge points using MLP 使用多层感知器分类边缘点<br>Step4: Optimize edge projection to true edges 优化边缘投影至真实边缘<br>Output: Refined edge points 提升的边缘点 |
8.5 | [[8.5] 2503.00972 Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration](https://arxiv.org/abs/2503.00972) <br> [{'name': 'Wanwen Chen, Carson Studders, Jamie J. Y. Kwon, Emily H. T. Pang, Eitan Prisman, Septimiu E. Salcudean'}] | Point Cloud Processing 点云处理 | v2<br>Point cloud registration<br>Iterative Closest Point<br>Elastic Energy Regularization | Input: Point cloud data 点云数据<br>Step1: Utilize semantic labels for robust matching 使用语义标签进行稳健匹配<br>Step2: Apply elastic energy regularization 应用弹性能量正则化<br>Output: Enhanced registration accuracy 改进的配准精度 |
8.5 | [[8.5] 2503.01100 Fence Theorem: Towards Dual-Objective Semantic-Structure Isolation in Preprocessing Phase for 3D Anomaly Detection](https://arxiv.org/abs/2503.01100) <br> [{'name': 'Hanzhe Liang, Jie Zhou, Xuanxin Chen, Tao Dai, Jinbao Wang, Can Gao'}] | 3D Anomaly Detection 3D异常检测 | v2<br>3D anomaly detection<br>preprocessing<br>Fence Theorem | Input: Anomaly detection task for 3D data 3D数据的异常检测任务<br>Step1: Establish the Fence Theorem 建立围栏定理<br>Step2: Implement two-stage process of Semantic-Division and Spatial-Constraints 实施语义划分和空间约束的两阶段过程<br>Step3: Develop Patch3D method with Patch-Cutting and Patch-Matching modules 开发包含Patch-Cutting和Patch-Matching模块的Patch3D方法<br>Output: Enhanced point-level anomaly detection performance 提高的点级异常检测性能 |
8.5 | [[8.5] 2503.01107 VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors](https://arxiv.org/abs/2503.01107) <br> [{'name': 'Juil Koo, Paul Guerrero, Chun-Hao Paul Huang, Duygu Ceylan, Minhyuk Sung'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D object composition<br>video editing<br>generative models<br>temporal consistency | Input: Video frames 视频帧<br>Step1: Lift features from generative model 提取生成模型中的特征<br>Step2: Edit 3D positions of objects 编辑物体的三维位置<br>Step3: Project features back to video frames 将特征投影回视频帧<br>Output: Edited video with composed objects 输出: 编辑后合成对象的视频 |
8.5 | [[8.5] 2503.01187 DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution](https://arxiv.org/abs/2503.01187) <br> [{'name': 'Xingyuan Li, Zirui Wang, Yang Zou, Zhixin Chen, Jun Ma, Zhiying Jiang, Long Ma, Jinyuan Liu'}] | Image Generation 图像生成 | v2<br>infrared image super-resolution<br>diffusion models<br>autonomous driving | Input: Low-resolution infrared images 低分辨率红外图像<br>Step1: Gradient-based modulation to optimize diffusion gradients 梯度调节以优化扩散梯度<br>Step2: Incorporate visual guidance for spectral fidelity 融入视觉引导以保持光谱完整性<br>Step3: Integrate perceptual features from foundational models 融合基础模型的感知特征<br>Output: Enhanced high-resolution infrared images 改进的高分辨率红外图像 |
8.5 | [[8.5] 2503.01199 LiteGS: A High-Performance Modular Framework for Gaussian Splatting Training](https://arxiv.org/abs/2503.01199) <br> [{'name': 'Kaimin Liao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Gaussian splatting<br>3D reconstruction<br>modular framework<br>efficiency improvements | Input: 3D Gaussian splatting 3D高斯点云<br>Step1: Modular design of splatting process splatting过程的模块化设计<br>Step2: Optimization of operators 操作优化<br>Step3: Implementation of dual API support 实现双API支持<br>Output: High-performance Gaussian splatting framework 高性能高斯点云框架 |
8.5 | [[8.5] 2503.01257 SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion](https://arxiv.org/abs/2503.01257) <br> [{'name': 'Xuan Zhu, Jijun Xiang, Xianqi Wang, Longliang Liu, Yu Wang, Hong Zhang, Fei Guo, Xin Yang'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D reconstruction<br>Time-of-Flight sensors | Input: Sparse dToF depth maps and corresponding RGB images 输入：稀疏的直接飞行时间（dToF）深度图和相应的RGB图像<br>Step1: Multi-frame feature extraction 多帧特征提取<br>Step2: Adaptive Freqency Selective Fusion (AFSF) implementation 自适应频率选择融合（AFSF）实现<br>Step3: Cross-window consistency loss application 横向窗口一致性损失应用<br>Output: Refined and consistent depth video 输出：精细化和一致的深度视频 |
8.5 | [[8.5] 2503.01547 AI-Driven Relocation Tracking in Dynamic Kitchen Environments](https://arxiv.org/abs/2503.01547) <br> [{'name': 'Arash Nasr Esfahani, Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor, Hedieh Sajedi'}] | Autonomous Systems and Robotics 自动化系统与机器人技术 | v2<br>3D reconstruction<br>object tracking<br>AI systems<br>dynamic environments | Input: Dynamic kitchen environment 动态厨房环境<br>Step1: Data collection 数据收集<br>Step2: Object detection using YOLOv5 使用YOLOv5进行物体检测<br>Step3: Frame scoring for object relocation 帧评分以跟踪物体位置<br>Output: Accurate tracking of object relocation 准确的物体位置跟踪 |
8.5 | [[8.5] 2503.01610 Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior](https://arxiv.org/abs/2503.01610) <br> [{'name': 'Chen Guo, Junxuan Li, Yash Kant, Yaser Sheikh, Shunsuke Saito, Chen Cao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human avatars<br>universal prior model<br>monocular video<br>photorealistic rendering | Input: Monocular in-the-wild videos 单目野外视频<br>Step1: Learn a universal prior model from multi-view data 从多视角数据中学习通用先验模型<br>Step2: Fine-tune the model using inverse rendering with monocular video 通过反向渲染对单目视频进行模型微调<br>Output: Photorealistic and animatable 3D avatars 真实感和可动画的三维人形头像 |
8.5 | [[8.5] 2503.01785 Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785) <br> [{'name': 'Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang'}] | Vision-Language Models (VLMs) 视觉语言模型 | visual reinforcement fine-tuning<br>large vision-language models | Input: Large Vision-Language Models (LVLMs) 大型视觉语言模型<br>Step1: Generate multiple responses 生成多个响应<br>Step2: Apply visual perception verifiable reward functions 应用视觉感知可验证奖励函数<br>Step3: Update model through policy optimization 更新模型通过策略优化<br>Output: Enhanced reasoning and adaptability 增强的推理和适应性 |
7.5 | [[7.5] 2503.01167 Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data](https://arxiv.org/abs/2503.01167) <br> [{'name': 'Haoxin Li, Boyang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>compositional understanding<br>synthetic data | Input: Image-caption pairs 图像-文本对<br>Step1: Image feature injection into T2I model 将图像特征注入到T2I模型中<br>Step2: Adaptive margin loss implementation 自适应边际损失实现<br>Step3: Model evaluation on benchmarks 模型在基准测试中评估<br>Output: Improved compositionality of VLMs 改进的VLM组合性 |
7.5 | [[7.5] 2503.01263 Generalizable Prompt Learning of CLIP: A Brief Overview](https://arxiv.org/abs/2503.01263) <br> [{'name': 'Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao'}] | VLM & VLA 视觉语言模型与对齐 | v2<br>Vision-Language Models<br>CLIP<br>Prompt Learning | Review of existing methods for few-shot prompt learning.<br>Analysis of performance across various datasets.<br>Discussion of the integration of visual and textual models. |


## Arxiv 2025-03-03

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.20511 Best Foot Forward: Robust Foot Reconstruction in-the-wild](https://arxiv.org/abs/2502.20511) <br> [{'name': 'Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, Ujwal Bonde'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Structure-from-Motion<br>foot modeling<br>point clouds<br>healthcare | Input: Multi-view images 多视角图像<br>Step1: SfM and MVS for initial point cloud estimation 使用SfM和MVS估计初始点云<br>Step2: Viewpoint prediction module for alignment 视点预测模块进行对齐<br>Step3: Shape completion through attention-based network 使用基于注意力的网络进行形状补全<br>Output: Complete 3D foot model 完成的3D脚模型 |
9.5 | [[9.5] 2502.20814 Improved 3D Point-Line Mapping Regression for Camera Relocalization](https://arxiv.org/abs/2502.20814) <br> [{'name': 'Bach-Thuan Bui, Huy-Hoang Bui, Yasuyuki Fujii, Dinh-Tuan Tran, Joo-Ho Lee'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>camera relocalization<br>point-line mapping | Input: 3D point and line features 3D点线特征<br>Step1: Data preparation 数据准备<br>Step2: Separate regression streams for points and lines 为点和线分别建立回归流<br>Step3: Training with self-attention layers 采用自注意力层进行训练<br>Output: Improved camera localization performance 改进的相机定位性能 |
9.5 | [[9.5] 2502.20861 MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image](https://arxiv.org/abs/2502.20861) <br> [{'name': 'Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic cues<br>single image | Input: Single image 单张图像<br>Step1: Effective Semantic Mining Module (ESM)实施有效语义挖掘模块<br>Step2: 3D Semantic Prior Learning Module (3DSPL)实施3D语义先验学习模块<br>Step3: Reconstruct 3D shape 进行3D形状重建<br>Output: Accurate 3D models 精确的三维模型 |
9.5 | [[9.5] 2502.21093 FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering](https://arxiv.org/abs/2502.21093) <br> [{'name': 'Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, Hongsheng Li'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction 三维重建<br>autonomous driving 自动驾驶<br>Inverse View Warping 逆视角扭曲 | Input: Multi-view images 多视角图像<br>Step1: Inverse View Warping technique 逆视角扭曲技术<br>Step2: Depth Bootstrap strategy 深度引导策略<br>Step3: Evaluation on driving simulators 驾驶模拟器上的评估<br>Output: Enhanced out-of-path reconstruction 改进的路径外重建 |
9.5 | [[9.5] 2502.21280 Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints](https://arxiv.org/abs/2502.21280) <br> [{'name': 'Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>stereo vision<br>depth estimation | Input: Stereo pair of images 立体图像对<br>Step1: Analytical 3D surface modeling 3D表面模型分析<br>Step2: Integration of learned stereo features 学习到的立体特征集成<br>Step3: Occlusion and texture-less region recovery 遮挡和无纹理区域恢复<br>Output: Enhanced depth maps 改进的深度图 |
9.0 | [[9.0] 2502.20732 CADDreamer: CAD object Generation from Single-view Images](https://arxiv.org/abs/2502.20732) <br> [{'name': 'Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Generation 三维生成 | v2<br>3D generation 三维生成<br>CAD objects CAD对象<br>diffusion models 扩散模型 | Input: Single-view images 单视图图像<br>Step1: Multi-view generation 多视角生成<br>Step2: Geometric and topological extraction 几何和拓扑提取<br>Output: Boundary representation (B-rep) of CAD models CAD模型的边界表示 |
8.5 | [[8.5] 2502.20669 EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering](https://arxiv.org/abs/2502.20669) <br> [{'name': 'John J. Han, Jie Ying Wu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>differentiable rendering<br>surgical simulations<br>lighting estimation<br>material properties | Input: Endoscopic images and known geometry 内窥镜图像与已知几何<br>Step1: Differentiable rendering framework differentiation 可微渲染框架分离<br>Step2: Model lighting and material properties 建模光照和材料属性<br>Step3: Generate photorealistic images 生成照相真实的图像<br>Output: Enhanced depth estimation outputs 改进的深度估计输出 |
8.5 | [[8.5] 2502.20676 SciceVPR: Stable Cross-Image Correlation Enhanced Model for Visual Place Recognition](https://arxiv.org/abs/2502.20676) <br> [{'name': 'Shanshan Wan, Yingmei Wei, Lai Kang, Tianrui Shen, Haixuan Wang, Yee-Hong Yang'}] | Visual Place Recognition 视觉位置识别 | v2<br>Visual Place Recognition<br>autonomous systems<br>feature extraction<br>DINOv2 | Input: Query images and database images 查询图像和数据库图像<br>Step1: Extract features using DINOv2 from multi-layer outputs 使用DINOv2提取多层输出特征<br>Step2: Fusion of multi-layer features especially in channel and spatial dimensions 在通道和空间维度融合多层特征<br>Step3: Distillation of cross-image invariant information into the self-enhanced encoder 将跨图像的不变信息蒸馏到自我增强编码器中<br>Output: Robust global descriptors 生成鲁棒的全局描述符 |
8.5 | [[8.5] 2502.20685 EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching](https://arxiv.org/abs/2502.20685) <br> [{'name': 'Dongki Jung, Jaehoon Choi, Yonghan Lee, Somi Jeong, Taejae Lee, Dinesh Manocha, Suyong Yeon'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>dense matching<br>omnidirectional images | Input: Equirectangular Projection (ERP) images 360度图像<br>Step1: Feature extraction 特征提取<br>Step2: Spherical Spatial Alignment Module (SSAM) implementation 球面空间对齐模块实施<br>Step3: Geodesic Flow Refinement 几何流修正<br>Output: Dense matching results 稠密匹配结果 |
8.5 | [[8.5] 2502.20694 WorldModelBench: Judging Video Generation Models As World Models](https://arxiv.org/abs/2502.20694) <br> [{'name': 'Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu'}] | Video Generation 视频生成 | v2<br>video generation<br>world modeling<br>autonomous driving | Input: Video generation models 视频生成模型<br>Step1: Benchmark design 基准设计<br>Step2: Human annotation large-scale crowd-sourcing 大规模人类注释众包<br>Step3: Model evaluation 模型评估<br>Output: Evaluation scores for world modeling capabilities 评估世界建模能力的得分 |
8.5 | [[8.5] 2502.20964 Fine-Grained Retrieval-Augmented Generation for Visual Question Answering](https://arxiv.org/abs/2502.20964) <br> [{'name': 'Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>Retrieval-Augmented Generation<br>Multimodal Learning<br>Knowledge Units | Input: Visual content and natural language questions 图像内容和自然语言问题<br>Step1: Merge fine-grained knowledge units 组合细粒度知识单元<br>Step2: Implement knowledge unit retrieval-augmented generation (KU-RAG) 实现知识单元检索增强生成 (KU-RAG)<br>Step3: Employ knowledge correction chain (KCC) 使用知识校正链 (KCC)<br>Output: Enhanced reasoning capabilities through precise knowledge retrieval 通过精确知识检索增强推理能力 |
8.5 | [[8.5] 2502.21067 Fast 3D point clouds retrieval for Large-scale 3D Place Recognition](https://arxiv.org/abs/2502.21067) <br> [{'name': "Chahine-Nicolas Zede, Laurent Carrafa, Val\\'erie Gouet-Brunet"}] | 3D Retrieval and Place Recognition 3D 检索与地点识别 | v2<br>3D retrieval<br>point clouds<br>autonomous navigation<br>place recognition | Input: 3D point clouds<br>Step1: Adapt Differentiable Search Index (DSI) for point clouds<br>Step2: Generate 1D identifiers based on point descriptors<br>Step3: Use Vision Transformers for descriptor mapping<br>Output: Efficient retrieval system for 3D point clouds |
7.5 | [[7.5] 2502.21151 A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images](https://arxiv.org/abs/2502.21151) <br> [{'name': 'Zineb Sordo, Eric Chagnon, Daniela Ushizima'}] | Image Generation 图像生成 | v2<br>text-to-image generation<br>image-to-image generation<br>generative AI | Input: Text data and existing images 文本数据与现有图像<br>Step1: Comparative analysis of architectures 架构的比较分析<br>Step2: Discussion of architectural innovations 架构创新的讨论<br>Step3: Exploration of open challenges 开放挑战的探索<br>Output: Summary of generative AI advancements and challenges 生成式AI的进展与挑战总结 |


## Arxiv 2025-02-28

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.19630 Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras](https://arxiv.org/abs/2502.19630) <br> [{'name': 'Hoonhee Cho, Jae-young Kang, Youngho Kim, Kuk-Jin Yoon'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>event cameras<br>autonomous driving | Input: Event camera and 3D point cloud data 输入: 事件相机和3D点云数据<br>Step1: Data acquisition 数据获取<br>Step2: Event data integration 事件数据集成<br>Step3: 3D object detection algorithm development 3D物体检测算法开发<br>Step4: Performance evaluation 性能评估<br>Output: Real-time 3D object detection results 实时3D物体检测结果 |
9.5 | [[9.5] 2502.19698 You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving](https://arxiv.org/abs/2502.19698) <br> [{'name': 'Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D instance segmentation 3D 实例分割<br>LiDAR point cloud 激光雷达点云<br>autonomous driving 自动驾驶 | Input: Outdoor LiDAR point cloud data 户外激光雷达点云数据<br>Step1: Generate 3D pseudo labels using click annotations 使用点击注释生成 3D 伪标签<br>Step2: Enhance pseudo label generation using vision foundation models 使用视觉基础模型提高伪标签生成<br>Step3: Improve label quality through temporal and spatial updating module 通过时间和空间更新模块提高标签质量<br>Output: High-quality 3D instance segmentation labels 高质量 3D 实例分割标签 |
9.5 | [[9.5] 2502.19739 LUCAS: Layered Universal Codec Avatars](https://arxiv.org/abs/2502.19739) <br> [{'name': 'Di Liu, Teng Deng, Giljoo Nam, Yu Rong, Stanislav Pidhorskyi, Junxuan Li, Jason Saragih, Dimitris N. Metaxas, Chen Cao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>avatar modeling | Input: 3D head model 3D头模型<br>Step1: Separate face and hair representation 分离面部与头发表示<br>Step2: Implement layered modeling 实施分层建模<br>Step3: Real-time performance optimization 实时性能优化<br>Output: Realistic codec avatars 逼真的编解码头像 |
9.5 | [[9.5] 2502.19782 Open-Vocabulary Semantic Part Segmentation of 3D Human](https://arxiv.org/abs/2502.19782) <br> [{'name': 'Keito Suzuki, Bang Du, Girish Krishnan, Kunyao Chen, Runfa Blark Li, Truong Nguyen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D part segmentation 3D部件分割<br>open-vocabulary segmentation 开放词汇分割<br>vision-language models 视觉语言模型 | Input: 3D models and textual prompts 输入: 3D模型和文本提示<br>Step1: Generate multi-view proposals using SAM 第一步: 使用SAM生成多视角提议<br>Step2: Create embeddings with HumanCLIP 第二步: 使用HumanCLIP创建嵌入<br>Step3: Apply MaskFusion for 3D segmentation 第三步: 应用MaskFusion进行3D分割<br>Output: 3D semantic masks 输出: 3D语义掩模 |
9.5 | [[9.5] 2502.19800 No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics](https://arxiv.org/abs/2502.19800) <br> [{'name': 'Dongbo Shi, Shen Cao, Lubin Fan, Bojian Wu, Jinhui Guo, Renjie Chen, Ligang Liu, Jieping Ye'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>novel view synthesis<br>camera parameters<br>3D reconstruction | Input: Image collection 图像集<br>Step1: Joint optimization of 3DGS and camera parameters 3D高斯点云和相机参数的联合优化<br>Step2: Gradients derivation of camera intrinsics 相机内参的梯度推导<br>Step3: Multi-view consistency and reprojection loss enforcement 强制多视角一致性和重投影损失<br>Output: Improved novel view synthesis 改进的视图合成 |
9.5 | [[9.5] 2502.19896 GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors](https://arxiv.org/abs/2502.19896) <br> [{'name': 'An Li, Zhe Zhu, Mingqiang Wei'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>point cloud completion<br>3D generative models | Input: Partial point clouds 部分点云<br>Step1: Depth Prompting module to extract depth depth images深度图像提取<br>Step2: Generating 3D shape using image-to-3D generative models利用图像到3D生成模型生成三维形状<br>Step3: Geometric Preserving Fusion to align generated shape with input生成形状与输入对齐<br>Output: Completed point clouds 完成的点云 |
9.5 | [[9.5] 2502.20108 VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers](https://arxiv.org/abs/2502.20108) <br> [{'name': 'Ziang Guo, Konstantin Gubernatorov, Selamawit Asfaw, Zakhar Yagudin, Dzmitry Tsetserukou'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>decision-making<br>diffusion Transformers | Input: Surrounding images from multiple cameras 多个相机的图像<br>Step1: Extract feature grids using a BEV encoder 使用BEV编码器提取特征网格<br>Step2: Fine-tune VLM for contextual interpretation 微调VLM以进行上下文解释<br>Step3: Condition diffusion process with BEV features and textual embeddings 使用BEV特征和文本嵌入调节扩散过程<br>Output: Optimized action predictions for autonomous driving 输出：优化的自主驾驶动作预测 |
9.5 | [[9.5] 2502.20154 Cutting-edge 3D reconstruction solutions for underwater coral reef images: A review and comparison](https://arxiv.org/abs/2502.20154) <br> [{'name': 'Jiageng Zhong, Ming Li, Armin Gruen, Konrad Schindler, Xuan Liao, Qinghua Guo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>coral reefs<br>underwater imaging | Input: Underwater coral reef images 水下珊瑚礁图像<br>Step1: Data collection and preparation 数据收集与准备<br>Step2: Camera pose estimation 相机姿态估计<br>Step3: Dense surface reconstruction 密集表面重建<br>Output: Digital surface models and orthomosaics 输出: 数字表面模型和正射影像图 |
9.5 | [[9.5] 2502.20208 4Deform: Neural Surface Deformation for Robust Shape Interpolation](https://arxiv.org/abs/2502.20208) <br> [{'name': 'Lu Sang, Zehranaz Canfes, Dongliang Cao, Riccardo Marin, Florian Bernard, Daniel Cremers'}] | 3D Shape Interpolation 3D形状插值 | v2<br>3D shape interpolation<br>point clouds<br>neural implicit representation | Input: Sparse temporal sequence of point clouds 稀疏的时间序列点云<br>Step1: Establishing correspondences 建立匹配关系<br>Step2: Representing shapes with implicit field 使用隐式场表示形状<br>Step3: Modeling deformation with velocity field 使用速度场建模变形<br>Output: Realistic intermediate shapes 真实的中间形状 |
9.5 | [[9.5] 2502.20220 Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars](https://arxiv.org/abs/2502.20220) <br> [{'name': 'Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nie{\\ss}ner, Shunsuke Saito'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>animatable avatars<br>Gaussian modeling<br>facial animation | Input: Few images of a person's head 少量图像<br>Step1: Position map computation 位置图计算<br>Step2: Gaussian prediction 高斯预测<br>Step3: Animation through cross-attention 通过交叉注意力进行动画<br>Output: High-quality 3D head avatars 高质量的三维头像 |
9.5 | [[9.5] 2502.20316 Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds](https://arxiv.org/abs/2502.20316) <br> [{'name': 'Mohamed Abdelsamad, Michael Ulrich, Claudius Gl\\"aser, Abhinav Valada'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>self-supervised learning<br>LiDAR point clouds<br>autonomous driving | Input: LiDAR point clouds 从LiDAR点云输入<br>Step1: Voxel masking 体素遮罩化<br>Step2: Multiscale occupancy reconstruction 多尺度占用重建<br>Step3: Model evaluation 模型评估<br>Output: Improved point cloud representations 改进的点云表示 |
9.5 | [[9.5] 2502.20378 Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling](https://arxiv.org/abs/2502.20378) <br> [{'name': 'Hanyang Kong, Xingyi Yang, Xinchao Wang'}] | Neural Rendering 神经渲染 | v2<br>dynamic scene rendering<br>Gaussian splatting<br>3D modeling | Input: Monocular videos 单目视频<br>Step1: Sparse attribute modeling 稀疏属性建模<br>Step2: Kernel-based motion representation 基于核的运动表示<br>Step3: Gaussian filtering for dynamics 高斯过滤以处理动态<br>Output: Efficient rendering outputs 高效渲染输出 |
9.5 | [[9.5] 2502.20389 LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding](https://arxiv.org/abs/2502.20389) <br> [{'name': 'Ang Cao, Sergio Arnaud, Oleksandr Maksymets, Jianing Yang, Ayush Jain, Sriram Yenamandra, Ada Martin, Vincent-Pierre Berges, Paul McVay, Ruslan Partsey, Aravind Rajeswaran, Franziska Meier, Justin Johnson, Jeong Joon Park, Alexander Sax'}] | 3D Vision-Language Grounding 3D视觉语言对齐 | v2<br>3D vision-language grounding<br>differentiable rendering<br>pseudo-labeling | Input: Multi-view images 和 2D labels 2D标签<br>Step1: Data integration 数据集成<br>Step2: Differentiable rendering differentiation 渲染偏微分<br>Step3: Training with pseudo-labels 使用伪标签训练<br>Output: 3D masks and bounding boxes 3D掩膜和边界框 |
9.2 | [[9.2] 2502.19660 Noise-Injected Spiking Graph Convolution for Energy-Efficient 3D Point Cloud Denoising](https://arxiv.org/abs/2502.19660) <br> [{'name': 'Zikuan Li, Qiaoyun Wu, Jialin Zhang, Kaijun Zhang, Jun Wang'}] | Point Cloud Processing 点云处理 | v2<br>3D point cloud processing<br>energy-efficient networks<br>spiking neural networks | Input: Raw point clouds 原始点云<br>Step1: Build noise-injected spiking neurons 建立注入噪声的脉冲神经元<br>Step2: Design noise-injected spiking graph convolution 设计注入噪声的脉冲图卷积<br>Step3: Create SNN-based denoising networks 建立基于SNN的去噪网络<br>Output: Denoised 3D point clouds 去噪后的3D点云 |
9.2 | [[9.2] 2502.20110 UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler](https://arxiv.org/abs/2502.20110) <br> [{'name': 'Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, Luc Van Gool'}] | Depth Estimation 深度估计 | v2<br>Monocular Metric Depth Estimation<br>3D Reconstruction<br>Camera Prediction<br>Depth Estimation | Input: Single image 单幅图像<br>Step1: Predict 3D points from the input image 通过输入图像预测3D点<br>Step2: Implement self-promptable camera module 实现自提示摄像机模块<br>Step3: Apply geometric invariance loss 应用几何不变损失<br>Output: Metric depth estimation metric depth metrics度量深度估计 |
8.5 | [[8.5] 2502.19623 3D Nephrographic Image Synthesis in CT Urography with the Diffusion Model and Swin Transformer](https://arxiv.org/abs/2502.19623) <br> [{'name': 'Hongkun Yu, Syed Jamal Safdar Gardezi, E. Jason Abel, Daniel Shapiro, Meghan G. Lubner, Joshua Warner, Matthew Smith, Giuseppe Toia, Lu Mao, Pallavi Tiwari, Andrew L. Wentland'}] | Image Generation 图像生成 | v2<br>3D reconstruction<br>image synthesis<br>CT urography<br>diffusion model<br>Swin Transformer | Input: Multi-phase CT images 多相CT图像<br>Step1: Data registration 数据注册<br>Step2: Model training 模型训练<br>Step3: Image synthesis 图像合成<br>Output: Synthetic nephrographic images 合成肾成像图像 |
8.5 | [[8.5] 2502.19689 3D Trajectory Reconstruction of Moving Points Based on a Monocular Camera](https://arxiv.org/abs/2502.19689) <br> [{'name': 'Huayu Huang, Banglei Guan, Yang Shang, Qifeng Yu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D trajectory reconstruction<br>monocular camera<br>ridge estimation | Input: Monocular camera images 单目相机图像<br>Step1: Motion representation using temporal polynomials 运动表示使用时间多项式<br>Step2: Ridge estimation for ill-conditioning mitigation 岭估计以减轻病态<br>Step3: Order determination of temporal polynomials for accuracy 精确度的时间多项式的阶数确定<br>Output: 3D trajectory reconstruction of moving points 移动点的三维轨迹重建 |
8.5 | [[8.5] 2502.19694 BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance](https://arxiv.org/abs/2502.19694) <br> [{'name': 'Xin Ye, Burhaneddin Yaman, Sheng Cheng, Feng Tao, Abhirup Mallik, Liu Ren'}] | Autonomous Driving 自动驾驶 | v2<br>Bird's-eye-view (BEV)<br>Diffusion model<br>3D object detection | Input: BEV feature maps 鸟瞰图特征图<br>Step1: Data integration 数据集成<br>Step2: Diffusion model training 扩散模型训练<br>Step3: Denoising BEV representations 去噪鸟瞰图表示<br>Output: Enhanced BEV models 改进的鸟瞰图模型 |
8.5 | [[8.5] 2502.19930 Identity-preserving Distillation Sampling by Fixed-Point Iterator](https://arxiv.org/abs/2502.19930) <br> [{'name': 'SeonHwa Kim, Jiwon Kim, Soobin Park, Donghoon Ahn, Jiwon Kang, Seungryong Kim, Kyong Hwan Jin, Eunju Cha'}] | Image and Video Generation 图像生成 | v2<br>3D object generation 3D物体生成<br>Neural Radiance Fields (NeRF)神经辐射场<br>Score Distillation Sampling (SDS)分数蒸馏采样 | Input: Image-to-image editing (NeRF) 图像编辑 （神经辐射场）<br>Step1: Analyze text-conditioned scores 分析文本条件分数<br>Step2: Introduce fixed-point iterative regularization (FPR) 提出固定点迭代正则化<br>Step3: Modify gradients to preserve identity 修改梯度以保持身份<br>Output: Enhanced identity-preserved images 改进的身份保留图像 |
8.5 | [[8.5] 2502.19955 RUBIK: A Structured Benchmark for Image Matching across Geometric Challenges](https://arxiv.org/abs/2502.19955) <br> [{'name': 'Thibaut Loiseau, Guillaume Bourmaud'}] | Multi-view and Stereo Vision 多视角立体视觉 | v2<br>camera pose estimation<br>image matching<br>benchmark | Input: Image pairs 图像对<br>Step1: Camera pose estimation 相机姿态估计<br>Step2: Data evaluation 数据评估<br>Step3: Benchmarking different methods 不同方法基准测试<br>Output: Performance metrics 性能指标 |
8.5 | [[8.5] 2502.20036 A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization](https://arxiv.org/abs/2502.20036) <br> [{'name': 'Yejun Zhang, Shuzhe Wang, Juho Kannala'}] | Camera Pose Estimation 相机位姿估计 | v2<br>visual localization<br>camera pose estimation<br>Graph Neural Networks<br>2D-3D matching<br>descriptor-free methods | Input: 2D query images and 3D models 2D查询图像和3D模型<br>Step1: Establish pixel-to-point correspondences 建立像素到点的对应关系<br>Step2: Use Angle-Annular GNN for geometric representation 使用角环形图神经网络进行几何表示<br>Step3: Evaluate matching and localization performance 评估匹配和定位性能<br>Output: Accurate camera pose estimation 精确的相机位姿估计 |
8.5 | [[8.5] 2502.20041 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds](https://arxiv.org/abs/2502.20041) <br> [{'name': 'Hengshuo Chu, Xiang Deng, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie'}] | 3D Affordance Detection 3D 可供性检测 | v2<br>3D affordance detection<br>Instruction Reasoning Affordance Segmentation<br>large language models | Input: 3D point clouds 3D 点云<br>Step1: Reformulate affordance detection as IRAS reformulate 将可供性检测重构为 IRAS<br>Step2: Combine with large language models (LLMs) 集成大型语言模型 (LLMs)<br>Step3: Multi-stage training strategy 多阶段训练策略<br>Output: Affordance masks affordance masks |
8.5 | [[8.5] 2502.20077 SegLocNet: Multimodal Localization Network for Autonomous Driving via Bird's-Eye-View Segmentation](https://arxiv.org/abs/2502.20077) <br> [{'name': 'Zijie Zhou, Zhangshuo Qi, Luqi Cheng, Guangming Xiong'}] | Autonomous Systems and Robotics 自主系统与机器人技术 | v2<br>autonomous driving<br>localization<br>bird's-eye-view segmentation | Input: Multi-view images and LiDAR point clouds 多视角图像和激光雷达点云<br>Step1: BEV segmentation network for generating semantic maps BEV分割网络生成语义地图<br>Step2: Exhaustive matching process for ego pose estimation 彻底匹配过程以估计自我姿态<br>Output: Precise localization results 精确的定位结果 |
8.5 | [[8.5] 2502.20111 MITracker: Multi-View Integration for Visual Object Tracking](https://arxiv.org/abs/2502.20111) <br> [{'name': 'Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang'}] | Multi-view Stereo 多视角立体 | v2<br>multi-view object tracking<br>3D feature volume<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Construct a 3D feature volume 生成3D特征体<br>Step2: Integrate features from multiple views 集成多个视角的特征<br>Step3: Utilize attention mechanism 采用注意机制<br>Output: Improved tracking outcomes 改进的跟踪结果 |
8.5 | [[8.5] 2502.20172 Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think](https://arxiv.org/abs/2502.20172) <br> [{'name': 'Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang'}] | Image Generation 图像生成 | v2<br>text-to-image generation<br>multimodal models | Input: Text-image interleaved control inputs 文本-图像交错控制输入<br>Step1: Replace text encoders with multimodal information encoders 将文本编码器替换为多模态信息编码器<br>Step2: Joint text-image alignment 共同文本-图像对齐<br>Step3: Multimodal interleaved instruction tuning 多模态交错指令调优<br>Output: Enhanced image generation models 改进的图像生成模型 |
8.5 | [[8.5] 2502.20323 ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model](https://arxiv.org/abs/2502.20323) <br> [{'name': 'Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada'}] | 3D Generation 三维生成 | v2<br>3D facial animation<br>autoregressive model<br>speech-driven motion generation | Input: Speech audio 输入: 语音音频<br>Step1: Divide speech into time windows 步骤1: 将语音分成时间窗口<br>Step2: Encode multi-scale motion codes 步骤2: 编码多尺度运动编码<br>Step3: Generate realistic lip movements and head poses 步骤3: 生成逼真的唇部动作和头部姿态<br>Output: Real-time 3D facial animation 输出: 实时3D面部动画 |
8.0 | [[8.0] 2502.20387 InsTaG: Learning Personalized 3D Talking Head from Few-Second Video](https://arxiv.org/abs/2502.20387) <br> [{'name': 'Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Jun Zhou, Lin Gu'}] | 3D Generation 三维生成 | v2<br>3D talking head<br>personalization<br>motion adaptation | Input: Few-second video clips 短视频片段<br>Step1: Identity-Free Pre-training strategy 身份无关预训练策略<br>Step2: Motion-Aligned Adaptation strategy 动作对齐适应策略<br>Output: Personalized 3D talking heads 个性化三维对话头 |
7.5 | [[7.5] 2502.19672 Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack](https://arxiv.org/abs/2502.19672) <br> [{'name': 'Chenhe Gu, Jindong Gu, Andong Hua, Yao Qin'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Dynamic Vision-Language Alignment<br>adversarial attacks<br>Multimodal Large Language Models | Input: Vision-language models 视觉语言模型<br>Step1: Introduce dynamic perturbations 引入动态扰动<br>Step2: Perturb attention components 扰动注意力组件<br>Step3: Evaluate transferability 评估可转移性<br>Output: Enhanced adversarial transferability 改进的对抗转移性 |
7.5 | [[7.5] 2502.19777 InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models](https://arxiv.org/abs/2502.19777) <br> [{'name': 'Shuchang Zhou'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>Prompt tuning 提示调优<br>Zero-shot learning 零样本学习 | Input: Learnable tokens and prior knowledge 学习标记和先验知识<br>Step1: Prior knowledge infusion 先验知识注入<br>Step2: Interaction reinforcement 交互增强<br>Step3: Text-to-vision projection layer introduction 引入文本到视觉投影层<br>Output: Improved visual-text alignment 改进的视觉文本对齐 |
7.5 | [[7.5] 2502.19797 MFSR: Multi-fractal Feature for Super-resolution Reconstruction with Fine Details Recovery](https://arxiv.org/abs/2502.19797) <br> [{'name': 'Lianping Yang, Peng Jiao, Jinshan Pan, Hegui Zhu, Su Guo'}] | Image Generation 图像生成 | v2<br>super-resolution<br>diffusion model<br>fractal features | Input: Low-resolution images (LR) 低分辨率图像<br>Step1: Feature extraction 特色提取<br>Step2: Diffusion model processing 扩散模型处理<br>Step3: Denoising process 去噪过程<br>Output: High-resolution images (HR) 高分辨率图像 |
7.5 | [[7.5] 2502.20388 Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation](https://arxiv.org/abs/2502.20388) <br> [{'name': 'Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen'}] | Image Generation 图像生成 | v2<br>autoregressive models<br>visual generation<br>flexible prediction units | Input: Noisy entities 噪声实体<br>Step1: Continuous entity regression 连续实体回归<br>Step2: Flow-matching method application 流匹配方法应用<br>Step3: Prediction using flexible units 使用灵活单位进行预测<br>Output: Improved image generation 改进的图像生成 |


## Arxiv 2025-02-27

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.19247 ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding](https://arxiv.org/abs/2502.19247) <br> [{'name': 'Qihang Peng, Henry Zheng, Gao Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D visual grounding<br>point cloud enhancement<br>Proxy Transformation | Input: RGB-D images and language instructions RGB-D图像和语言指令<br>Step1: Deformable Point Clustering to identify sub-manifolds 使用可变形点聚类识别子流形<br>Step2: Proxy Attention to guide transformations 使用代理注意力指导变换<br>Step3: Generate transformation matrices and translation vectors 生成变换矩阵和位移向量<br>Output: Enhanced point cloud for 3D visual grounding 输出: 增强的三维视觉定位点云 |
8.5 | [[8.5] 2502.18496 Physical Depth-aware Early Accident Anticipation: A Multi-dimensional Visual Feature Fusion Framework](https://arxiv.org/abs/2502.18496) <br> [{'name': 'Hongpu Huang, Wei Zhou, Chen Wang'}] | Autonomous Driving 自动驾驶 | v2<br>accident anticipation<br>depth estimation<br>3D information<br>multi-dimensional features | Input: Dashcam videos 行车记录仪视频<br>Step 1: Extract monocular depth features 提取单眼深度特征<br>Step 2: Integrate visual interaction and dynamic features 结合视觉交互和动态特征<br>Step 3: Construct frame graph based on features 构建基于特征的帧图<br>Output: Early accident anticipation predictions 提早事故预警结果 |
8.5 | [[8.5] 2502.18724 Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers](https://arxiv.org/abs/2502.18724) <br> [{'name': 'Anthony Etim, Jakub Szefer'}] | Autonomous Driving 自动驾驶 | v2<br>universal perturbation<br>traffic sign recognition<br>adversarial attacks<br>autonomous systems<br>security in autonomous driving | Input: Traffic sign images 交通标志图像<br>Step1: Generate universal perturbations 生成通用扰动<br>Step2: Apply stickers to traffic signs 将贴纸应用于交通标志<br>Step3: Evaluate misclassification rates 评估误分类率<br>Output: Information on vulnerability 攻击模型的脆弱性信息 |
8.5 | [[8.5] 2502.19048 An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection](https://arxiv.org/abs/2502.19048) <br> [{'name': 'Tresor Y. Koffi, Youssef Mourchid, Mohammed Hindawi, Yohan Dupuis'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D skeletons<br>fall detection<br>image processing<br>impact detection | Input: 3D skeleton data 3D骨架数据<br>Step1: Preprocessing techniques 预处理技术<br>Step2: Data enhancement 数据增强<br>Step3: Experimentation using ML/DL algorithms 使用机器学习/深度学习算法进行实验<br>Output: Improved fall detection dataset 改进的跌倒检测数据集 |
8.5 | [[8.5] 2502.19125 The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields](https://arxiv.org/abs/2502.19125) <br> [{'name': 'Ziyuan Luo, Anderson Rocha, Boxin Shi, Qing Guo, Haoliang Li, Renjie Wan'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D reconstruction<br>digital watermarking | Input: NeRF model and signatures (输入: NeRF模型和签名)<br>Step1: Codebook-aided Signature Embedding (步骤1: 基于代码本的签名嵌入)<br>Step2: Joint pose-patch encryption (步骤2: 联合姿态补丁加密)<br>Step3: Complexity-Aware Key Selection (步骤3: 复杂性感知密钥选择)<br>Output: Watermarked NeRF model (输出: 水印NeRF模型) |
8.5 | [[8.5] 2502.19128 SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation](https://arxiv.org/abs/2502.19128) <br> [{'name': 'Junlong Ren, Hao Wu, Hui Xiong, Hao Wang'}] | 3D Retrieval 跨模态3D检索 | v2<br>cross-modal retrieval<br>3D shapes<br>data augmentation | Input: 3D shapes and captions 3D形状和描述<br>Step1: Data augmentation 数据增强<br>Step2: Component alignment 组件对齐<br>Step3: Cross-modal similarity calculation 跨模态相似度计算<br>Output: Enhanced cross-modal retrieval 强化的跨模态检索 |
8.5 | [[8.5] 2502.19177 Knowledge Distillation for Semantic Segmentation: A Label Space Unification Approach](https://arxiv.org/abs/2502.19177) <br> [{'name': 'Anton Backhaus, Thorsten Luettel, Mirko Maehlisch'}] | Autonomous Driving 自动驾驶 | v2<br>knowledge distillation<br>semantic segmentation<br>autonomous driving | Input: Source dataset with given taxonomy 源数据集与给定的分类法<br>Step1: Teacher model training 教师模型训练<br>Step2: Pseudo-label generation 伪标签生成<br>Step3: Student model training 学生模型训练<br>Output: Improved student models 改进的学生模型 |
8.5 | [[8.5] 2502.19204 Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator](https://arxiv.org/abs/2502.19204) <br> [{'name': 'Xiankang He, Dongyan Guo, Hongji Li, Ruibo Li, Ying Cui, Chi Zhang'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation 单目深度估计<br>Cross-Context Distillation 交叉上下文蒸馏<br>Depth Normalization 深度归一化<br>Multi-Teacher Framework 多教师框架 | Input: Single RGB image 单幅RGB图像<br>Step1: Analyze depth normalization strategies 分析深度归一化策略<br>Step2: Propose Cross-Context Distillation 提出交叉上下文蒸馏<br>Step3: Develop multi-teacher distillation framework 开发多教师蒸馏框架<br>Output: Enhanced depth predictions 改进的深度预测 |
8.5 | [[8.5] 2502.19260 EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region](https://arxiv.org/abs/2502.19260) <br> [{'name': 'Nadya Abdel Madjid, Murad Mebrahtu, Abdelmoamen Nasser, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>multi-task dataset<br>trajectory prediction | Input: Dash-camera footage from the Arab Gulf region 阿拉伯海湾地区的行车记录仪视频<br>Step1: Data collection 数据收集<br>Step2: Annotation of frames and objects 帧和物体标注<br>Step3: Development of evaluation benchmarks 开发评估基准<br>Output: EMT dataset with annotated tasks 提供带注释的EMT数据集和任务 |
8.5 | [[8.5] 2502.19313 CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query](https://arxiv.org/abs/2502.19313) <br> [{'name': 'Zhe Wang, Shaocong Xu, Xucai Zhuang, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang'}] | Autonomous Driving 自动驾驶 | v2<br>3D detection<br>cooperative perception<br>autonomous driving<br>object queries | Input: Raw sensor data from agents 代理的原始传感器数据<br>Step1: Single-agent query generation 单代理查询生成<br>Step2: Cross-agent query fusion 跨代理查询融合<br>Output: Object-level detection results 物体级别检测结果 |
7.5 | [[7.5] 2502.18536 FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA](https://arxiv.org/abs/2502.18536) <br> [{'name': 'S M Sarwar'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>Retrieval-Augmented Generation<br>Zero-shot Learning | Input: Visual Question Answering (VQA) system 图像问答系统<br>Step1: Integrate visual and textual understanding 视觉与文本理解集成<br>Step2: Implement retrieval-augmented generation 实现检索增强生成<br>Step3: Reduce hallucinations through external knowledge retrieval 通过外部知识检索减少幻觉<br>Output: Improved VQA responses 改进的VQA响应 |
6.5 | [[6.5] 2502.18512 FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression](https://arxiv.org/abs/2502.18512) <br> [{'name': 'Jianjian Li, Junquan Fan, Feng Tang, Gang Huang, Shitao Zhu, Songlin Liu, Nian Xie, Wulong Liu, Yong Liao'}] | VLM & VLA 视觉语言模型 | v2<br>Vision-Language Models<br>Token Compression | Input: High-resolution images 高分辨率图像<br>Step1: Self-distillation pre-training 自蒸馏预训练阶段<br>Step2: Visual token compression 视觉标记压缩<br>Step3: Post-training with high-quality datasets 后训练与高质量数据集<br>Output: Efficiently compressed visual tokens 有效压缩的视觉标记 |
6.5 | [[6.5] 2502.19269 Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models](https://arxiv.org/abs/2502.19269) <br> [{'name': 'Jiawei Kong, Hao Fang, Sihang Guo, Chenxi Qing, Bin Chen, Bin Wang, Shu-Tao Xia'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>backdoor attacks<br>contrastive learning | Input: Pre-trained Vision-Language Models (VLMs) 预训练视觉语言模型<br>Step1: Identify backdoor triggers 确定后门触发器<br>Step2: Contrastive learning for trigger inversion 使用对比学习进行触发器反转<br>Step3: Class-wise prompt tuning for defense 类别化的提示调整进行防御<br>Output: Modified text prompts for backdoor resistance 输出：经过修改的文本提示，增强后门抵御能力 |


## Arxiv 2025-02-26

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.17648 CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement](https://arxiv.org/abs/2502.17648) <br> [{'name': 'Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao'}] | Multi-sensor Calibration 多传感器校准 | v2<br>LiDAR-camera calibration<br>autonomous driving<br>sensor fusion | Input: Raw LiDAR point clouds and camera images 原始LiDAR点云和相机图像<br>Step1: Train a Common Feature Discriminator using detected objects 使用检测到的对象训练公用特征判别器<br>Step2: Perform coarse homography-based calibration 粗略的同质性基础校准<br>Step3: Iterative refinement to improve alignment 迭代精炼以改进对齐<br>Step4: Attention-based refinement leveraging Vision Transformer 利用视觉变换器进行基于注意力的精细调整<br>Output: Accurate sensor calibration results 准确的传感器校准结果 |
9.5 | [[9.5] 2502.17822 Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking](https://arxiv.org/abs/2502.17822) <br> [{'name': 'Peng Zhang, Xin Li, Xin Lin, Liang He'}] | 3D Multi-Object Tracking 3D多物体跟踪 | v2<br>3D multi-object tracking<br>autonomous driving<br>real-time systems | Input: Multi-modal data 多模态数据<br>Step1: Augmented Proposal Generation 数据增强提议生成<br>Step2: Data association 数据关联<br>Step3: Motion modeling 运动建模<br>Step4: Life-cycle management 生命周期管理<br>Output: Enhanced 3D tracking results 改进的三维跟踪结果 |
9.5 | [[9.5] 2502.17860 UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting](https://arxiv.org/abs/2502.17860) <br> [{'name': 'Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang'}] | Multi-view and Stereo Vision 多视角立体视觉 | v2<br>3D representation<br>multi-modal learning<br>Gaussian Splatting | Input: Multi-view images and point clouds 多视角图像和点云<br>Step1: 3D Gaussian Splatting representation creation 创建3D高斯点云表示<br>Step2: Language-Image-3D pre-training 语言-图像-3D预训练<br>Step3: Gaussian-Aware Guidance module integration 高斯引导模块集成<br>Output: Unified multi-modal representations 统一的多模态表示 |
9.5 | [[9.5] 2502.18150 Joint Reconstruction of Spatially-Coherent and Realistic Clothed Humans and Objects from a Single Image](https://arxiv.org/abs/2502.18150) <br> [{'name': 'Ayushi Dutta, Marco Pesavento, Marco Volino, Adrian Hilton, Armin Mustafa'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>human-object interaction<br>neural implicit models | Input: Single-view images 单视角图像<br>Step1: Handle occlusion using a generative model 处理遮挡<br>Step2: Estimate implicit representation using an attention-based neural model 估计隐式表示<br>Step3: Incorporate semantic features to ensure spatial coherence 融合语义特征以确保空间连贯性<br>Output: Realistic human-object 3D reconstructions 逼真的人类-物体三维重建 |
9.5 | [[9.5] 2502.18219 Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training](https://arxiv.org/abs/2502.18219) <br> [{'name': 'Botao Ye, Sifei Liu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>novel view synthesis | Input: Single reference image 单个参考图像<br>Step1: Retrieve overlapping information from reference views 从参考视图中检索重叠信息<br>Step2: Synthesize target views using epipolar attention 使用极线注意力合成目标视图<br>Step3: Evaluate generated views against consistency criteria 评估生成的视图的一致性标准<br>Output: Consistent multi-view images 一致的多视角图像 |
9.0 | [[9.0] 2502.17852 Sketch-1-to-3: One Single Sketch to 3D Detailed Face Reconstruction](https://arxiv.org/abs/2502.17852) <br> [{'name': 'Liting Wen, Zimo Yang, Xianlin Zhang, Chi Ding, Yue Zhang, Mingdao Wang, Xueming Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D face reconstruction<br>sketch-based modeling<br>high-fidelity | Input: Single sketch 单一素描<br>Step1: Extract geometric contours and texture details 提取几何轮廓和纹理细节<br>Step2: Alignment with 3D facial space 对齐3D面部空间<br>Step3: Model training with domain adaptation 模型训练与领域适应<br>Output: Detailed 3D face model 详细的3D面部模型 |
8.5 | [[8.5] 2502.17706 IBURD: Image Blending for Underwater Robotic Detection](https://arxiv.org/abs/2502.17706) <br> [{'name': 'Jungseok Hong, Sakshi Singh, Junaed Sattar'}] | Robotic Perception 机器人感知 | v2<br>image blending<br>synthetic data<br>underwater robotics | Input: Source object images and annotations 目标物体图像与标签<br>Step1: Use Poisson editing for image blending 使用泊松编辑进行图像融合<br>Step2: Apply style transfer to match background styles 应用风格迁移以匹配背景样式<br>Step3: Generate synthetic images with annotations 生成带标签的合成图像<br>Output: Realistic synthetic images for training realistic synthetic图像用于训练 |
8.5 | [[8.5] 2502.17796 LAM: Large Avatar Model for One-shot Animatable Gaussian Head](https://arxiv.org/abs/2502.17796) <br> [{'name': 'Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian heads<br>real-time rendering | Input: Single image 单幅图像<br>Step1: Generate animatable Gaussian head 生成可动画的高斯头像<br>Step2: Utilize FLAME canonical points 采用FLAME标准点<br>Step3: Render in real-time 实时渲染<br>Output: Instant animation and rendering 即时动画和渲染 |
8.5 | [[8.5] 2502.17843 Automatic Vehicle Detection using DETR: A Transformer-Based Approach for Navigating Treacherous Roads](https://arxiv.org/abs/2502.17843) <br> [{'name': 'Istiaq Ahmed Fahad, Abdullah Ibne Hanif Arean, Nazmus Sakib Ahmed, Mahmudul Hasan'}] | Autonomous Driving 自动驾驶 | v2<br>Vehicle Detection 车辆检测<br>Autonomous Navigation 自动驾驶<br>Transformer-Based Methods 基于变换器的方法 | Input: Diverse driving environments 多样化的驾驶环境<br>Step1: Dataset preparation 数据集准备<br>Step2: Model fine-tuning 模型微调<br>Step3: Evaluation against traditional methods 评估传统方法<br>Output: Improved vehicle detection model 改进的车辆检测模型 |
8.5 | [[8.5] 2502.17863 ASurvey: Spatiotemporal Consistency in Video Generation](https://arxiv.org/abs/2502.17863) <br> [{'name': 'Zhiyu Yin, Kehai Chen, Xuefeng Bai, Ruili Jiang, Juntao Li, Hongdong Li, Jin Liu, Yang Xiang, Jun Yu, Min Zhang'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Video Generation 视频生成<br>Spatiotemporal Consistency 时空一致性 | Input: Review of recent advances 在视频生成领域的最新进展<br>Step1: Summarize foundation models 总结基础模型<br>Step2: Discuss information representations 讨论信息表示方法<br>Step3: Review generation schemes 评审生成方案<br>Step4: Present post-processing techniques 提供后处理技术<br>Step5: Evaluate metrics 评估指标<br>Output: Insights on spatiotemporal consistency 输出: 时空一致性的见解 |
8.5 | [[8.5] 2502.18041 OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation](https://arxiv.org/abs/2502.18041) <br> [{'name': 'Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Navigation<br>3D Gaussian Splatting<br>Aerial Dataset | Input: Aerial images from UAVs 无人机获取的空中图像<br>Step1: Automatic point cloud acquisition 自动点云获取<br>Step2: Scene semantic segmentation 场景语义分割<br>Step3: Flight trajectory creation 飞行轨迹创建<br>Step4: Instruction generation 指令生成<br>Output: Large-scale aerial VLN dataset 大规模空中视觉语言导航数据集 |
8.5 | [[8.5] 2502.18042 VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion](https://arxiv.org/abs/2502.18042) <br> [{'name': 'Pei Liu (The Hong Kong University of Science and Technology), Haipeng Liu (Li Auto Inc), Haichao Liu (The Hong Kong University of Science and Technology), Xin Liu (The Hong Kong University of Science and Technology), Jinxin Ni (Xiamen University), Jun Ma (The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology)'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>multimodal information fusion | Input: Front-view images 前视图像<br>Step1: Generate initial text descriptions 使用 VLM 生成文本描述<br>Step2: Semantic refinement of texts 语义细化文本<br>Step3: Integrate textual descriptions with BEV 特征将文本与 BEV 特征集成<br>Output: Enhanced decision-making performance 提升的决策性能 |
8.5 | [[8.5] 2502.18368 Near-Shore Mapping for Detection and Tracking of Vessels](https://arxiv.org/abs/2502.18368) <br> [{'name': 'Nicholas Dalhaug, Annette Stahl, Rudolf Mester, Edmund F{\\o}rland Brekke'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>LiDAR<br>autonomous vessels<br>mapping<br>tracking | Input: LiDAR data and camera images LiDAR数据和相机图像<br>Step1: Off-line mapping process 离线映射过程<br>Step2: Combination of LiDAR and image data 利用LiDAR和图像数据的结合<br>Step3: Object detection and segmentation 目标检测和分割<br>Output: Improved maritime situational awareness 提高海洋态势感知 |
8.5 | [[8.5] 2502.18373 EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity](https://arxiv.org/abs/2502.18373) <br> [{'name': 'Dominik Hollidt, Paul Streli, Jiaxi Jiang, Yasaman Haghighi, Changlin Qian, Xintong Liu, Christian Holz'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D pose estimation<br>multi-view rendering<br>egocentric vision | Input: Body-worn camera footage 身体佩戴相机视频<br>Step1: Data simulation 数据模拟<br>Step2: Dataset creation 数据集创建<br>Step3: 3D pose estimation model training 3D姿态估计模型训练<br>Output: Available dataset and trained models 可用数据集和训练模型 |
7.5 | [[7.5] 2502.18012 High-precision visual navigation device calibration method based on collimator](https://arxiv.org/abs/2502.18012) <br> [{'name': 'Shunkun Liang, Dongcai Tan, Banglei Guan, Zhang Li, Guangcheng Dai, Nianpeng Pan, Liang Shen, Yang Shang, Qifeng Yu'}] | Autonomous Systems 自主系统 | v2<br>Visual navigation devices<br>Camera calibration<br>Attitude calibration | Input: Navigation devices 导航设备<br>Step1: Single-image camera calibration using collimator 单图像相机校准<br>Step2: Attitude calibration using precision adjustment mechanism 姿态校准<br>Step3: Calibration evaluation 校准评估<br>Output: High-precision calibration results 高精度校准结果 |


## Arxiv 2025-02-25

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.16419 DeProPose: Deficiency-Proof 3D Human Pose Estimation via Adaptive Multi-View Fusion](https://arxiv.org/abs/2502.16419) <br> [{'name': 'Jianbin Jiao, Xina Cheng, Kailun Yang, Xiangrong Zhang, Licheng Jiao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human pose estimation<br>multi-view perception<br>deficiency-aware estimation | Input: Multi-view images 多视角图像<br>Step 1: Simplification of network architecture 网络结构简化<br>Step 2: Feature extraction from images 从图像中提取特征<br>Step 3: Adaptive multi-view feature fusion 自适应多视角特征融合<br>Output: 3D human pose estimations 3D 人体姿态估计 |
9.5 | [[9.5] 2502.16475 Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control](https://arxiv.org/abs/2502.16475) <br> [{'name': 'Jinbo Yan, Alan Zhao, Yixin Hu'}] | 3D Generation 三维生成 | v2<br>3D Generation 3D生成<br>Geometric Consistency 几何一致性<br>User Control 用户控制 | Input: Single image and point cloud 单幅图像和点云<br>Step1: Generate sparse seed points 生成稀疏种子点<br>Step2: Map seed points to anchor latents 映射种子点到锚潜在<br>Step3: Generate 3D Gaussian representations 生成3D高斯表示<br>Output: Multi-view consistent 3D models 输出：多视角一致的3D模型 |
9.5 | [[9.5] 2502.16488 Geometry-Aware 3D Salient Object Detection Network](https://arxiv.org/abs/2502.16488) <br> [{'name': 'Chen Wang, Liyuan Zhang, Le Hui, Qi Liu, Yuchao Dai'}] | 3D Salient Object Detection 三维显著目标检测 | v2<br>3D salient object detection<br>point cloud<br>geometry-aware | Input: 3D point clouds 3D点云<br>Step1: Superpoint partitioning 超点划分<br>Step2: Point feature learning 点特征学习<br>Step3: Geometry enhancement geometry enhancement<br>Output: Salient object map 显著目标图 |
9.5 | [[9.5] 2502.16575 Efficient 4D Gaussian Stream with Low Rank Adaptation](https://arxiv.org/abs/2502.16575) <br> [{'name': 'Zhenhuan Liu, Shuai Liu, Yidong Lu, Yirui Chen, Jie Yang, Wei Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Dynamic novel view synthesis 动态新视图合成<br>3D Gaussian Splatting 3D高斯点云 | Input: Video frames 视频帧<br>Step1: Scene representation using 3D Gaussians 使用3D高斯表示场景<br>Step2: Low-rank adaptation for bandwidth reduction 低秩适应以减少带宽<br>Step3: Continuous dynamic reconstruction 进行连续动态重建<br>Output: Scalable dynamic novel views 可扩展的动态新视图 |
9.5 | [[9.5] 2502.16652 Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration](https://arxiv.org/abs/2502.16652) <br> [{'name': 'Kim Jun-Seong, GeonU Kim, Kim Yu-Ji, Yu-Chiang Frank Wang, Jaesung Choe, Tae-Hyun Oh'}] | 3D Scene Understanding 三维场景理解 | v2<br>3D Gaussian Splatting<br>open-vocabulary scene understanding<br>language embedding<br>3D reconstruction<br>3D perception | Input: 3D Gaussians 3D高斯点云<br>Step1: Feature registration 特征注册<br>Step2: Direct language embedding association 直接语言嵌入关联<br>Step3: Evaluation of 3D perception tasks 3D感知任务评估<br>Output: Enhanced scene understanding 改进的场景理解 |
9.5 | [[9.5] 2502.16826 Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising](https://arxiv.org/abs/2502.16826) <br> [{'name': 'Xiangbin Wei'}] | Point Cloud Processing 点云处理 | v2<br>point cloud denoising<br>unsupervised learning<br>Tweedie's formula | Input: Noisy point cloud data 含噪点云数据<br>Step1: Gradient learning from noisy data 从含噪数据中学习梯度<br>Step2: Single-step denoising using Tweedie's formula 使用Tweedie公式进行单步去噪<br>Output: Denoised point cloud 输出: 去噪点云 |
9.5 | [[9.5] 2502.17053 PointSea: Point Cloud Completion via Self-structure Augmentation](https://arxiv.org/abs/2502.17053) <br> [{'name': 'Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Point Cloud Completion 点云补全<br>Self-structure Augmentation 自结构增强 | Input: Incomplete point cloud data 不完整的点云数据<br>Step1: Data augmentation 数据增强<br>Step2: Self-view fusion network self-view融合网络<br>Step3: Feature fusion feature融合<br>Step4: Point generation point生成<br>Output: Completed point cloud 完成的点云 |
9.5 | [[9.5] 2502.17288 GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow](https://arxiv.org/abs/2502.17288) <br> [{'name': 'Simon Boeder, Fabian Gigengack, Benjamin Risse'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>occupancy estimation<br>3D Gaussian representation<br>autonomous driving<br>Gaussian Splatting | Input: Multi-view images 多视角图像<br>Step1: Construct a sparse 3D Gaussian representation 构建稀疏的3D高斯表示<br>Step2: Integrate temporal flow estimation 整合时间流估计<br>Step3: Utilize Gaussian Splatting for training 采用高斯点云训练<br>Output: Efficient occupancy estimation 高效的占用率估计 |
9.5 | [[9.5] 2502.17377 Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting](https://arxiv.org/abs/2502.17377) <br> [{'name': 'Chong Cheng, Gaochao Song, Yiyang Yao, Qinzheng Zhou, Gangjian Zhang, Hao Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>scene reconstruction<br>autonomous driving | Input: Images captured by RGB cameras RGB摄像机拍摄的图像<br>Step1: Spatial prior-based scene structure estimation 空间先验场景结构估计<br>Step2: Camera graph creation 相机图创建<br>Step3: Graph-guided optimization guided by multi-view consistency graph-guided多视角一致性优化<br>Output: High-fidelity 3D reconstruction of scenes 改进的大规模三维场景重建 |
9.5 | [[9.5] 2502.17429 CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation](https://arxiv.org/abs/2502.17429) <br> [{'name': 'Vishal Thengane, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Lu Yin, Xiatian Zhu, Salman Khan'}] | 3D Instance Segmentation 3D实例分割 | v2<br>3D instance segmentation 3D实例分割<br>continual learning 持续学习<br>class imbalance 类别不平衡 | Input: RGB-D images with 3D instance annotations 采用带有3D实例标注的RGB-D图像<br>Step1: Implement a unified framework 实现统一框架<br>Step2: Integrate Exemplar Replay, Knowledge Distillation, and Imbalance Correction 集成样本重放、知识蒸馏和不平衡修正<br>Step3: Create benchmark scenarios for evaluation 创建基准场景进行评估<br>Output: Improved 3D instance segmentation performance 提高3D实例分割性能 |
9.0 | [[9.0] 2502.16779 Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model](https://arxiv.org/abs/2502.16779) <br> [{'name': 'Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>room layout estimation<br>multi-view geometry<br>DUSt3R<br>autonomous systems | Input: Multi-view images 多视角图像<br>Step1: 2D plane detection 2D 平面检测<br>Step2: 3D point representation 密集 3D 点表示<br>Step3: Plane correspondence establishment 平面对应关系的建立<br>Output: Estimated room layout 估计的房间布局 |
9.0 | [[9.0] 2502.16907 MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation](https://arxiv.org/abs/2502.16907) <br> [{'name': 'Jiehao Luo, Jintao Cheng, Xiaoyu Tang, Qingwen Zhang, Bohuan Xue, Rui Fan'}] | Scene Flow Estimation 场景流估计 | v2<br>Scene Flow Estimation<br>State Space Model<br>3D motion | Input: Consecutive point cloud frames 连续点云帧<br>Step1: Model design 模型设计<br>Step2: Feature extraction 特征提取<br>Step3: Scene flow estimation 场景流估计<br>Output: Motion vectors 运动向量 |
9.0 | [[9.0] 2502.17237 MegaLoc: One Retrieval to Place Them All](https://arxiv.org/abs/2502.17237) <br> [{'name': 'Gabriele Berton, Carlo Masone'}] | Visual Place Recognition 视觉位置识别 | v2<br>3D reconstruction<br>Visual Place Recognition<br>Image Retrieval<br>SLAM | Input: Diverse image datasets 多样的图像数据集<br>Step1: Data integration 数据集成<br>Step2: Model training using combined techniques 模型训练结合多种技术<br>Step3: Evaluation across multiple tasks 在多项任务中评估模型<br>Output: Robust image retrieval model 可靠的图像检索模型 |
8.5 | [[8.5] 2502.15888 Understanding and Evaluating Hallucinations in 3D Visual Language Models](https://arxiv.org/abs/2502.15888) <br> [{'name': 'Ruiying Peng, Kaiyuan Li, Weichen Zhang, Chen Gao, Xinlei Chen, Yong Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D-LLMs<br>hallucinations<br>evaluation metrics<br>point cloud | Input: 3D point-cloud data 3D点云数据<br>Step1: Definition of 3D hallucinations 3D幻觉的定义<br>Step2: Evaluation of hallucinations in 3D-LLMs 对3D-LLMs中的幻觉进行评估<br>Step3: Analysis of underlying causes 分析潜在原因<br>Output: New evaluation metrics for hallucinations 针对幻觉的新评估指标 |
8.5 | [[8.5] 2502.16012 Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving](https://arxiv.org/abs/2502.16012) <br> [{'name': 'Prashant Shekhar, Bidur Devkota, Dumindu Samaraweera, Laxima Niure Kandel, Manoj Babu'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>adversarial attacks<br>semantic segmentation | Input: Adversarial patch training adversarial 图像片段训练<br>Step1: Attack formulation 攻击形式化<br>Step2: Model performance analysis 模型性能分析<br>Step3: Cross-model transferability evaluation 跨模型转移性评估<br>Output: Insights on attack susceptibility 输出: 攻击易受性的洞见 |
8.5 | [[8.5] 2502.16164 A Deep Learning Framework with Geographic Information Adaptive Loss for Remote Sensing Images based UAV Self-Positioning](https://arxiv.org/abs/2502.16164) <br> [{'name': 'Mingkun Li, Ziming Wang, Guang Huo, Wei Chen, Xiaoning Zhao'}] | Autonomous Systems and Robotics 自动驾驶与机器人 | v2<br>UAV self-positioning<br>remote sensing<br>deep learning | Input: Remote sensing images and UAV images 遥感图像与无人机图像<br>Step1: Data alignment 数据对齐<br>Step2: Adaptive loss integration 自适应损失集成<br>Step3: Model evaluation 模型评估<br>Output: Precise UAV positioning output 精确的无人机定位输出 |
8.5 | [[8.5] 2502.16214 SalM$2$: An Extremely Lightweight Saliency Mamba Model for Real-Time Cognitive Awareness of Driver Attention](https://arxiv.org/abs/2502.16214) <br> [{'name': 'Chunyu Zhao, Wentao Mu, Xian Zhou, Wenbo Liu, Fei Yan, Tao Deng'}] | Autonomous Driving 自动驾驶 | v2<br>driver attention recognition<br>real-time model<br>semantic information | Input: Driving scene data 驾驶场景数据<br>Step 1: Extract bottom-up image features 提取自下而上的图像特征<br>Step 2: Extract top-down semantic information 提取自上而下的语义信息<br>Step 3: Integrate extracted features and map driver attention 集成提取的特征并映射驾驶者注意力<br>Output: Driver attention map 驾驶员注意力图 |
8.5 | [[8.5] 2502.16302 DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation](https://arxiv.org/abs/2502.16302) <br> [{'name': 'Yuxuan Xiong, Yue Shi, Yishun Dou, Bingbing Ni'}] | 3D Scene Editing 三维场景编辑 | v2<br>3D scene editing<br>Neural Radiance Fields<br>Text-driven generation | Input: Text instructions and 3D scene representation 文本指令与三维场景表示<br>Step1: Introduce dual-field representation 引入双场表示<br>Step2: Simulated annealing strategy implementation 模拟退火策略实现<br>Step3: Apply CLIP-based consistency indicator 应用CLIP一致性指标<br>Output: Edited 3D scenes with preserved backgrounds 输出：保留背景的编辑后的三维场景 |
8.5 | [[8.5] 2502.16303 Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field](https://arxiv.org/abs/2502.16303) <br> [{'name': 'Wenhao Hu, Wenhao Chai, Shengyu Hao, Xiaotong Cui, Xuexiang Wen, Jenq-Neng Hwang, Gaoang Wang'}] | 3D Segmentation 3D分割 | v2<br>3D segmentation<br>Gaussian segmentation<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Establish pixel correspondence 建立像素对应关系<br>Step2: Optimize mask association using the Hungarian algorithm 使用匈牙利算法优化掩膜关联<br>Step3: Apply piecewise-plane constraints 施加分段平面约束<br>Output: Consistent and compact 3D segmentation field 一致且紧凑的3D分割场 |
8.5 | [[8.5] 2502.16351 AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor Removal](https://arxiv.org/abs/2502.16351) <br> [{'name': 'Luca Gough, Adrian Azzarelli, Fan Zhang, Nantheera Anantrasirichai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Neural Radiance Fields<br>3D reconstruction<br>Underwater imaging | Input: Underwater scenes focusing on static objects 水下静态物体场景<br>Step1: Model the cumulative density of volumes along a ray 建模沿光线的体积累积密度<br>Step2: Apply Gaussian distribution for transmittance modeling 应用高斯分布建模透过率<br>Step3: Optimize the Gaussian distribution for stable rendering 优化高斯分布以实现稳定渲染<br>Output: Enhanced 3D representation of underwater scenes 改进的水下场景的三维表示 |
8.5 | [[8.5] 2502.16389 An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving](https://arxiv.org/abs/2502.16389) <br> [{'name': 'Tianchen Ji, Neeloy Chakraborty, Andre Schreiber, Katherine Driggs-Campbell'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>anomaly detection<br>autonomous driving | Input: Egocentric videos 自我中心视频<br>Step1: Scene analysis 场景分析<br>Step2: Expert model development 专家模型开发<br>Step3: Anomaly score fusion 异常分数融合<br>Output: Anomaly detection scores 异常检测分数 |
8.5 | [[8.5] 2502.16915 Multi-Dimensional Quality Assessment for Text-to-3D Assets: Dataset and Model](https://arxiv.org/abs/2502.16915) <br> [{'name': 'Kang Fu, Huiyu Duan, Zicheng Zhang, Xiaohong Liu, Xiongkuo Min, Jia Wang, Guangtao Zhai'}] | 3D Reconstruction 三维重建 | v2<br>text-to-3D generation<br>quality assessment<br>3D modeling | Input: 3D assets generated via text prompts 生成的3D资产与文本提示<br>Step1: Database creation 数据库创建<br>Step2: Quality feature extraction 质量特征提取<br>Step3: Model evaluation and benchmarking 模型评估与基准测试<br>Output: Quality assessment scores 质量评估分数 |
8.5 | [[8.5] 2502.16941 Gaussian Difference: Find Any Change Instance in 3D Scenes](https://arxiv.org/abs/2502.16941) <br> [{'name': 'Binbin Jiang, Rui Huang, Qingyi Zhao, Yuxiang Zhang'}] | 3D Change Detection 三维变化检测 | v2<br>3D change detection<br>Gaussian distributions<br>instance segmentation | Input: Multi-view images 多视角图像<br>Step1: Embed images into 4D Gaussians 将图像嵌入4D高斯中<br>Step2: Segment images and assign IDs 分割图像并分配ID<br>Step3: Compare IDs for change detection 比较ID以进行变化检测<br>Output: Change maps from any viewpoint 从任何视点生成变化图 |
8.5 | [[8.5] 2502.16992 Semantic Neural Radiance Fields for Multi-Date Satellite Data](https://arxiv.org/abs/2502.16992) <br> [{'name': 'Valentin Wagner, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>multi-date satellite images<br>3D reconstruction | Input: Multi-date satellite images with semantic labels 多日期卫星图像和语义标签<br>Step1: Model adaptation for satellite images 针对卫星图像的模型适应<br>Step2: Semantic and color fusion 语义与颜色融合<br>Step3: Robustness evaluation and improvement 可靠性评估与改进<br>Output: 3D semantic representations 输出三维语义表示 |
8.5 | [[8.5] 2502.17039 LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR](https://arxiv.org/abs/2502.17039) <br> [{'name': 'Xinxin Feng, Haoran Sun, Haifeng Zheng, Huacong Chen, Wenqiang Chen'}] | Autonomous Driving 自动驾驶 | v2<br>3D object detection<br>LiDAR<br>collaborative perception | Input: Data collected from low-resolution LiDAR and cameras 低分辨率LiDAR和相机收集的数据<br>Step1: Feature extraction 特征提取<br>Step2: Voxel-wise fusion voxel级融合<br>Step3: Feature offset correction 特征偏移矫正<br>Step4: Regional feature enhancement 区域特征增强<br>Output: Enhanced 3D object detection 改进的三维物体检测 |
8.0 | [[8.0] 2502.15956 Human Motion Prediction, Reconstruction, and Generation](https://arxiv.org/abs/2502.15956) <br> [{'name': 'Canxuan Gang, Yiran Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>human motion prediction<br>3D reconstruction<br>motion generation | Input: Historical motion data 历史运动数据<br>Step1: Pose forecasting 姿势预测<br>Step2: 3D motion reconstruction 三维运动重建<br>Step3: Motion generation 运动生成<br>Output: Realistic human motion sequences 真实的人类动作序列 |
7.5 | [[7.5] 2502.16427 Fine-Grained Video Captioning through Scene Graph Consolidation](https://arxiv.org/abs/2502.16427) <br> [{'name': 'Sanghyeok Chu, Seonguk Seo, Bohyung Han'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>video captioning<br>visual-language models<br>scene graphs | Input: Video frames 视频帧<br>Step1: Generate frame-level captions using an image VLM 使用图像视觉语言模型生成帧级字幕<br>Step2: Convert captions into scene graphs 将字幕转换为场景图<br>Step3: Consolidate frame-level scene graphs into a video-level scene graph 将帧级场景图整合为视频级场景图<br>Output: Comprehensive video captions 生成综合视频字幕 |
7.5 | [[7.5] 2502.16493 Trunk-branch Contrastive Network with Multi-view Deformable Aggregation for Multi-view Action Recognition](https://arxiv.org/abs/2502.16493) <br> [{'name': 'Yingyuan Yang, Guoyuan Liang, Can Wang, Xiaojun Wu'}] | Multi-view Stereo 多视角立体 | v2<br>Multi-view action recognition<br>Contrastive learning<br>Feature fusion | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Feature aggregation 特征聚合<br>Step2: Contrastive learning against trunk features 对比学习<br>Step3: Model evaluation on datasets 模型在数据集上的评估<br>Output: Enhanced action representations 改进的动作表征 |
7.5 | [[7.5] 2502.16618 Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?](https://arxiv.org/abs/2502.16618) <br> [{'name': 'Qipan Xu, Zhenting Wang, Xiaoxiao He, Ligong Han, Ruixiang Tang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>copyright detection<br>Generative AI | Input: Image samples 图像样本<br>Step1: Dataset creation 数据集创建<br>Step2: Model evaluation 模型评估<br>Step3: Analysis of failure cases 失败案例分析<br>Output: Proposed solutions 提出的解决方案 |
6.5 | [[6.5] 2502.16368 Concept Corrector: Erase concepts on the fly for text-to-image diffusion models](https://arxiv.org/abs/2502.16368) <br> [{'name': 'Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong'}] | Image Generation 图像生成 | v2<br>concept erasure<br>text-to-image generation | Input: Intermediate-generated images 中间生成图像<br>Step1: Concept presence checking 概念存在检查<br>Step2: Concept removal correction 概念移除修正<br>Output: Corrected images 修正后的图像 |


## Arxiv 2025-02-24

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.14891 CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection](https://arxiv.org/abs/2502.14891) <br> [{'name': 'Zhe Huang, Shuo Wang, Yongcai Wang, Lei Wang'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>autonomous driving<br>diffusion models | Input: Point clouds from multiple agents 多个代理的点云<br>Step1: Feature extraction from point clouds 从点云中提取特征<br>Step2: Information sharing between agents 代理之间共享信息<br>Step3: Noise reduction using diffusion models 使用扩散模型进行噪声减少<br>Output: Accurate collaborative 3D object detection 准确的协作3D物体检测 |
9.5 | [[9.5] 2502.14938 GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian Splatting Models](https://arxiv.org/abs/2502.14938) <br> [{'name': 'Miao Tao, Yuanzhen Zhou, Haoran Xu, Zeyu He, Zhenyu Yang, Yuchang Zhang, Zhongling Su, Linning Xu, Zhenxiang Ma, Rong Fu, Hengjie Li, Xingcheng Zhang, Jidong Zhai'}] | Neural Rendering 神经渲染 | v2<br>3D Gaussian Splatting<br>neural rendering<br>real-time rendering<br>virtual reality | Input: Large-scale 3D Gaussian Splatting models 大规模3D高斯点云模型<br>Step1: Design cache-centric rendering pipeline 设计基于缓存的渲染管线<br>Step2: Implement multi-GPU scheduling 实现多GPU调度<br>Step3: Optimize CUDA kernels to enhance performance 优化CUDA内核以提高性能<br>Output: Real-time rendered 3D scenes 实时渲染的3D场景 |
9.5 | [[9.5] 2502.14940 FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models](https://arxiv.org/abs/2502.14940) <br> [{'name': 'Thomas Froech, Olaf Wysocki, Yan Xia, Junyu Xie, Benedikt Schwab, Daniel Cremers, Thomas H. Kolbe'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image inpainting<br>Stable Diffusion<br>conflict maps<br>diffusion models | Input: 3D building models and laser scanning point clouds 3D建筑模型和激光扫描点云<br>Step1: Deriving 2D conflict maps by deterministic ray analysis 通过确定性光线分析推导二维冲突图<br>Step2: Personalizing a Stable Diffusion model for inpainting 个性化稳定扩散模型进行修复<br>Step3: Generating synthetic conflict maps for training 生成合成冲突图用于训练<br>Output: Completed conflict maps for 3D semantic reconstruction 输出用于三维语义重建的完整冲突图 |
9.5 | [[9.5] 2502.15011 CrossOver: 3D Scene Cross-Modal Alignment](https://arxiv.org/abs/2502.15011) <br> [{'name': 'Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni'}] | 3D Scene Understanding 三维场景理解 | v2<br>3D scene understanding<br>cross-modal alignment<br>point clouds | Input: Multi-modal 3D data 多模态三维数据<br>Step1: Flexible modality alignment 灵活的模态对齐<br>Step2: Unified embedding space learning 统一嵌入空间学习<br>Step3: Scene retrieval and object localization 场景检索与物体定位<br>Output: Enhanced scene understanding 改进的场景理解 |
9.5 | [[9.5] 2502.15076 Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving Scenarios](https://arxiv.org/abs/2502.15076) <br> [{'name': 'Richard Marcus, Christian Vogel, Inga Jatzkowski, Niklas Knoop, Marc Stamminger'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D object detection 3D目标检测<br>LiDAR<br>Synthetic data 合成数据<br>Domain randomization 域随机化<br>Autonomous driving 自动驾驶 | Input: LiDAR point clouds and synthetic data 输入: LiDAR点云和合成数据<br>Step1: Sensor modeling 传感器建模<br>Step2: Domain randomization 域随机化<br>Step3: Object detection training 目标检测训练<br>Step4: Performance evaluation 性能评估<br>Output: Enhanced object detection模型输出: 改进的目标检测 |
9.5 | [[9.5] 2502.15438 LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight Spatio-Temporal Correlation](https://arxiv.org/abs/2502.15438) <br> [{'name': 'Fengcheng Yu, Haoran Xu, Canming Xia, Guang Tan'}] | 3D Scene Reconstruction 3D场景重建 | v2<br>3D occupancy networks 3D占用网络<br>autonomous driving 自动驾驶<br>spatio-temporal correlation 空间时间相关性 | Input: Multi-view images 多视角图像<br>Step1: Tokenization of baseline and motion features 基线和运动特征的标记化<br>Step2: Tri-stream fusion architecture for correlation establishment 三流融合架构进行关系建立<br>Step3: Generation of occupancy results 生成占用结果<br>Output: Enhanced occupancy predictions 改进的占用预测 |
9.5 | [[9.5] 2502.15633 RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes](https://arxiv.org/abs/2502.15633) <br> [{'name': 'Sicheng Yu, Chong Cheng, Yifan Zhou, Xiaojun Yang, Hao Wang'}] | Simultaneous Localization and Mapping (SLAM) 同时定位与地图构建 | v2<br>RGB-only SLAM<br>3D Gaussian Splatting<br>outdoor scenes<br>pose estimation | Input: RGB images RGB图像<br>Step1: Pointmap regression to generate spatial relationships 生成空间关系点图回归<br>Step2: Pose estimation based on pointmaps 使用点图进行位姿估计<br>Step3: 3D Gaussian Splatting for rendering 进行3D高斯喷涂渲染<br>Output: High-fidelity novel views 输出高保真新视图 |
9.5 | [[9.5] 2502.15635 Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis](https://arxiv.org/abs/2502.15635) <br> [{'name': 'Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>novel view synthesis<br>multi-lane dataset<br>autonomous driving<br>3D reconstruction<br>LiDAR | Input: Multi-lane dataset containing LiDAR and camera data  multi车道数据集，包括LiDAR和相机数据<br>Step1: Data collection通过多次扫描收集数据<br>Step2: Multi-sensor pose optimization多传感器姿态优化<br>Step3: Dataset registration数据集注册<br>Output: Evaluated novel view synthesis capabilities评估新的视图合成能力 |
9.2 | [[9.2] 2502.15488 Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2502.15488) <br> [{'name': 'Jiangyong Yu, Changyong Shu, Dawei Yang, Zichen Yu, Xing Hu, Yan Chen'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>quantization<br>autonomous driving | Input: Multi-view images 多视角图像<br>Step1: Identify quantization issues 识别量化问题<br>Step2: Propose Q-PETR model 提出Q-PETR模型<br>Step3: Evaluate performance evaluation 性能评估<br>Output: Enhanced detection accuracy 提高的检测精度 |
9.2 | [[9.2] 2502.15516 Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection](https://arxiv.org/abs/2502.15516) <br> [{'name': 'Yue Sun, Yeqiang Qian, Chunxiang Wang, Ming Yang'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection 3D目标检测<br>4D millimeter-wave radar 4D毫米波雷达 | Input: 4D radar spectra and depth-aware camera images 4D雷达谱和深度感知相机图像<br>Step1: Feature extraction from RGB and depth images 从RGB和深度图像中提取特征<br>Step2: Feature fusion in BEV feature space 在鸟瞰特征空间中融合特征<br>Step3: 3D object detection using fused features 使用融合特征进行3D目标检测<br>Output: Enhanced 3D object detection results 改进的3D目标检测结果 |
8.8 | [[8.8] 2502.15448 MVIP -- A Dataset and Methods for Application Oriented Multi-View and Multi-Modal Industrial Part Recognition](https://arxiv.org/abs/2502.15448) <br> [{'name': 'Paul Koch, Marian Schl\\"uter, J\\"org Kr\\"uger'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>Multi-View<br>Multi-Modal<br>Industrial Part Recognition | Input: Multi-view RGBD dataset 多视角RGBD数据集<br>Step1: Data acquisition 数据采集<br>Step2: Modality integration 模态集成<br>Step3: Model training and evaluation 模型训练与评估<br>Output: Robust industrial classifiers 稳健的工业分类器 |
8.5 | [[8.5] 2502.14908 KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models](https://arxiv.org/abs/2502.14908) <br> [{'name': 'Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, Kathleen M. Carley'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>knowledge conflicts<br>robustness | Input: Visual Question Answering (VQA) with multimodal sources 视觉问答与多模态源<br>Step1: Introduce targeted perturbations 引入目标扰动<br>Step2: Evaluate model robustness 评估模型的鲁棒性<br>Step3: Fine-tune models to improve reasoning 优化模型以提高推理能力<br>Output: Enhanced understanding of knowledge conflicts 增强对知识冲突的理解 |
8.5 | [[8.5] 2502.14917 Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning](https://arxiv.org/abs/2502.14917) <br> [{'name': 'Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>3D spatial understanding<br>multimodal learning | Input: Local scene videos and global BEV maps 本地场景视频和全局鸟瞰图<br>Step1: Modal encoders align visual representations 模态编码器对齐视觉表示<br>Step2: Generate natural language responses 生成自然语言响应<br>Step3: Enhance model performance through extensive training 通过大量训练提升模型性能<br>Output: Improved perception and reasoning for autonomous driving 改进的感知和推理能力用于自动驾驶 |
8.5 | [[8.5] 2502.15180 OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework](https://arxiv.org/abs/2502.15180) <br> [{'name': 'Junliang Chen, Huaiyuan Xu, Yi Wang, Lap-Pui Chau'}] | Autonomous Driving 自动驾驶 | v2<br>occupancy forecasting<br>autonomous driving<br>3D perception | Input: Multi-camera video input 多摄像头视频输入<br>Step1: Feature extraction 特征提取<br>Step2: Future occupancy forecasting 未来占用状态预测<br>Step3: Refinement of predictions 预测优化<br>Output: Future occupancy map 未来占用图 |
8.5 | [[8.5] 2502.15307 Road Traffic Sign Recognition method using Siamese network Combining Efficient-CNN based Encoder](https://arxiv.org/abs/2502.15307) <br> [{'name': 'Zhenghao Xi, Yuchao Shao, Yang Zheng, Xiang Liu, Yaqi Liu, Yitong Cai'}] | Autonomous Driving 自动驾驶 | v2<br>Traffic Sign Recognition 交通标志识别<br>Siamese Network 西安网络<br>Efficient-CNN 高效卷积神经网络 | Input: Traffic sign images 交通标志图像<br>Step1: Feature extraction 特征提取 using Efficient-CNN based encoders<br>Step2: Distance computation 距离计算 using Siamese network<br>Step3: Classification 分类 using fully-connected layer<br>Output: Recognized traffic sign categories 识别的交通标志类别 |
8.5 | [[8.5] 2502.15342 PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments](https://arxiv.org/abs/2502.15342) <br> [{'name': 'Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>3D detection<br>dataset<br>pedestrian detection<br>semi-structured environments | Input: Multi-modal data input 多模态数据输入<br>Step1: Dataset annotation 数据集标注<br>Step2: Hybrid Multi-Scale Fusion Network framework development 混合多尺度融合网络框架开发<br>Step3: Performance evaluation 性能评估<br>Output: Improved pedestrian detection results 改进的行人检测结果 |
8.5 | [[8.5] 2502.15398 Enhancing Vehicle Make and Model Recognition with 3D Attention Modules](https://arxiv.org/abs/2502.15398) <br> [{'name': 'Narges Semiromizadeh, Omid Nejati Manzari, Shahriar B. Shokouhi, Sattar Mirzakuchaki'}] | Autonomous Driving 自动驾驶 | v2<br>Vehicle Make and Model Recognition<br>Attention Module<br>Deep Learning | Input: Vehicle images from various makes and models 车辆图像输入<br>Step1: Integrate attention module into convolutional model 将注意力模块集成到卷积模型中<br>Step2: Enhance focus on distinguishing vehicle features 提高对识别车辆特征的关注<br>Step3: Evaluate performance on Stanford Cars dataset 在斯坦福汽车数据集上评估性能<br>Output: Improved VMMR accuracy 提高的车辆品牌和型号识别准确度 |
8.5 | [[8.5] 2502.15601 WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM Agents](https://arxiv.org/abs/2502.15601) <br> [{'name': 'Xinhang Liu, Chi-Keung Tang, Yu-Wing Tai'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D world creation<br>LLM agents<br>procedural generation | Input: User natural language commands 用户自然语言指令<br>Step1: Interaction with LLM agents 与LLM代理交互<br>Step2: Object customization and control 对象自定义与控制<br>Step3: Scene layout optimization 场景布局优化<br>Output: Photorealistic 3D scenes 照相真实3D场景 |
8.5 | [[8.5] 2502.15672 VaViM and VaVAM: Autonomous Driving through Video Generative Modeling](https://arxiv.org/abs/2502.15672) <br> [{'name': "Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, \\'Eloi Zablocki, Andrei Bursuc, Eduardo Valle, Matthieu Cord"}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>video generative models | Input: Driving video sequences 驾驶视频序列<br>Step1: Frame prediction 帧预测<br>Step2: Representation learning 表示学习<br>Step3: Action generation 动作生成<br>Output: Driving trajectories 驾驶轨迹 |
8.0 | [[8.0] 2502.15079 Can Hallucination Correction Improve Video-Language Alignment?](https://arxiv.org/abs/2502.15079) <br> [{'name': "Lingjun Zhao, Mingyang Xie, Paola Cascante-Bonilla, Hal Daum\\'e III, Kwonjoon Lee"}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>video-language alignment<br>hallucination correction | Input: Video and textual descriptions 视频和文本描述<br>Step1: Identify hallucinations 识别幻觉<br>Step2: Correct inconsistencies 修正不一致性<br>Step3: Enhance alignment 增强对齐<br>Output: Improved video-language alignment 改进的视频语言对齐 |
7.5 | [[7.5] 2502.14888 The Multi-Faceted Monosemanticity in Multimodal Representations](https://arxiv.org/abs/2502.14888) <br> [{'name': 'Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, Yulan He, Yifei Wang'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | v2<br>multimodal models<br>interpretability<br>CLIP | Input: CLIP features from image-text pairs CLIP特征<br>Step1: Feature extraction 特征提取<br>Step2: Classification into vision, language, and visual-language categories 分类为视觉、语言和视觉-语言类别<br>Step3: Evaluation of Modality Dominance Score (MDS) MDS评估<br>Output: Categorized and interpretable multimodal features 分类和可解释的多模态特征 |
7.5 | [[7.5] 2502.15389 The Role of Background Information in Reducing Object Hallucination in Vision-Language Models: Insights from Cutoff API Prompting](https://arxiv.org/abs/2502.15389) <br> [{'name': 'Masayo Tomita, Katsuhiko Hayashi, Tomoyuki Kaneko'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucination<br>background context | Input: Visual-Language Models (VLMs) 视觉语言模型<br>Step1: Analyze object hallucination in outputs 分析输出中的物体幻觉<br>Step2: Examine effectiveness of background context 研究背景上下文的有效性<br>Step3: Evaluate visual prompting techniques 评估视觉提示技术<br>Output: Recommendations for reducing hallucination 输出：减少幻觉的建议 |
7.5 | [[7.5] 2502.15563 Bridging vision language model (VLM) evaluation gaps with a framework for scalable and cost-effective benchmark generation](https://arxiv.org/abs/2502.15563) <br> [{'name': 'Tim R\\"adsch, Leon Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Bar{\\i}\\c{s} \\"Ozt\\"urk, Klaus Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, Lena Maier-Hein'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>benchmark generation 基准生成<br>task augmentation 任务增强 | Input: Existing VLM tasks 现有的VLM任务<br>Step1: Task augmentation for diverse tasks 任务增强以生成多样化任务<br>Step2: Benchmark creation for multiple domains 基准创建以适应多个领域<br>Step3: Performance evaluation performance performance evaluation 评估22个VLMs的表现<br>Output: Resource-efficient VLM benchmarks 资源高效的VLM基准 |


## Arxiv 2025-02-21

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.14129 GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian](https://arxiv.org/abs/2502.14129) <br> [{'name': 'Bang Du, Runfa Blark Li, Chen Du, Truong Nguyen'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>inverse rendering<br>glossy surfaces<br>NeRF<br>Gaussian Splatting | Input: Multi-view images 多视角图像<br>Step1: Model surface normals and BRDF parameters 模型表面法线和BRDF参数<br>Step2: Use Anisotropic Spherical Gaussian to approximate reflections 使用各向异性球面高斯近似反射<br>Step3: Apply regularization for better normal estimation 应用正则化以提高法线估计<br>Output: Efficiently rendered glossy 3D surfaces 经过高效渲染的光泽3D表面 |
9.5 | [[9.5] 2502.14142 Token Adaptation via Side Graph Convolution for Temporally and Spatially Efficient Fine-tuning of 3D Point Cloud Transformers](https://arxiv.org/abs/2502.14142) <br> [{'name': 'Takahiko Furuya'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud<br>Transformer<br>fine-tuning | Input: 3D point cloud data 三维点云数据<br>Step1: Define graph convolutional network 定义图卷积网络<br>Step2: Implement Side Token Adaptation 进行侧边令牌适应<br>Step3: Evaluate performance on benchmarks 在基准上评估性能<br>Output: Efficiently fine-tuned models 高效微调模型 |
9.5 | [[9.5] 2502.14235 OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving](https://arxiv.org/abs/2502.14235) <br> [{'name': 'Yedong Shen, Xinran Zhang, Yifan Duan, Shiqi Zhang, Heng Li, Yilong Wu, Jianmin Ji, Yanyong Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving | Input: Surround-view camera images 环视摄像头图像<br>Step1: Generate Occupancy Grids 使用占用网格生成<br>Step2: Separate dynamic and static objects 分离动态与静态物体<br>Step3: Convert Occupancy Grids to point clouds 将占用网格转换为点云<br>Step4: Estimate poses and trajectories 估计姿态和轨迹<br>Output: 3D reconstructed scene 3D重建场景 |
9.5 | [[9.5] 2502.14520 Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance](https://arxiv.org/abs/2502.14520) <br> [{'name': 'Meng Wang, Fan Wu, Ruihui Li, Yunchuan Qin, Zhuo Tang, Kenli Li'}] | 3D Scene Completion 三维场景补全 | v2<br>3D Semantic Scene Completion<br>optical flow<br>autonomous driving<br>temporal modeling | Input: Temporal RGB images 时间RGB图像<br>Step1: Optical flow estimation 光流估计<br>Step2: Flow-guided temporal aggregation 模块流引导的时间聚合<br>Step3: Occlusion-guided voxel refinement 模块遮挡引导的体素细化<br>Output: 3D semantic scene completion 3D语义场景补全 |
9.5 | [[9.5] 2502.14789 Structurally Disentangled Feature Fields Distillation for 3D Understanding and Editing](https://arxiv.org/abs/2502.14789) <br> [{'name': 'Yoel Levy, David Shavin, Itai Lang, Sagie Benaim'}] | 3D Understanding and Editing 3D理解与编辑 | v2<br>3D Understanding 3D理解<br>3D Editing 3D编辑<br>Feature Distillation 特征提炼 | Input: 2D feature maps obtained from large pre-trained models 基于大型预训练模型的2D特征图<br>Step1: Distillation of 2D features to 3D structurally disentangled feature fields 2D特征向3D结构解缠特征场的提炼<br>Step2: Control of individual structural components for semantic understanding 语义理解的个体结构成分控制<br>Step3: Application of segmentation and editing capabilities 应用分割与编辑功能<br>Output: Enhanced 3D understanding and editing capabilities 改进的3D理解与编辑能力 |
8.5 | [[8.5] 2502.14061 EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation](https://arxiv.org/abs/2502.14061) <br> [{'name': 'Zixuan Fang, Thomas P\\"ollabauer, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper'}] | Pose Estimation 姿态估计 | v2<br>6D pose estimation<br>autonomous navigation<br>real-time feedback<br>robotics | Input: Monocular RGB-D images 单目RGB-D图像<br>Step1: Architecture adaptation 架构适配<br>Step2: AMIS algorithm implementation AMIS算法实现<br>Step3: Model testing across datasets 在数据集上进行模型测试<br>Output: Optimized 6D pose estimation optimized 6D目标姿态估计 |
8.5 | [[8.5] 2502.14068 A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing](https://arxiv.org/abs/2502.14068) <br> [{'name': 'Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi'}] | Autonomous Driving 自动驾驶 | v2<br>3D reconstruction<br>autonomous driving | Input: Multi-camera image data 多摄像机图像数据<br>Step1: Data collection and annotation 数据收集与注释<br>Step2: Algorithm development using GAN 算法开发，使用生成对抗网络(GAN)<br>Step3: Model evaluation and benchmarking 模型评估与基准测试<br>Output: Track detection results 轨道检测结果 |
8.5 | [[8.5] 2502.14099 Point Cloud Geometry Scalable Coding Using a Resolution and Quality-conditioned Latents Probability Estimator](https://arxiv.org/abs/2502.14099) <br> [{'name': "Daniele Mari, Andr\\'e F. R. Guarda, Nuno M. M. Rodrigues, Simone Milani, Fernando Pereira"}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Coding<br>scalable coding<br>deep learning | Input: Point Cloud geometry points  点云几何点<br>Step1: Development of Scalable Resolution and Quality Hyperprior (SRQH)方案开发<br>Step2: Integration into JPEG PCC 将SRQH集成到JPEG PCC中<br>Step3: Experimental validation 实验验证<br>Output: Scalable coding for point clouds 提供点云的可扩展编码 |
8.5 | [[8.5] 2502.14113 Object-centric Binding in Contrastive Language-Image Pretraining](https://arxiv.org/abs/2502.14113) <br> [{'name': 'Rim Assouel, Pietro Astolfi, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object-centric<br>compositional understanding | Input: CLIP-like models CLIP类模型<br>Step1: Integrating scene graphs with image representations 将场景图与图像表示结合<br>Step2: Developing a binding module 设计绑定模块<br>Step3: Enhancing spatial relationship understanding 加强空间关系理解<br>Output: Improved compositional understanding 提升组合理解 |
8.5 | [[8.5] 2502.14156 Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration](https://arxiv.org/abs/2502.14156) <br> [{'name': 'Katie Z Luo, Minh-Quan Dao, Zhenzhen Liu, Mark Campbell, Wei-Lun Chao, Kilian Q. Weinberger, Ezio Malis, Vincent Fremont, Bharath Hariharan, Mao Shan, Stewart Worrall, Julie Stephany Berrio Perez'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>V2X<br>point cloud<br>LiDAR sensors | Input: LiDAR sensor data from vehicles 和车辆的激光雷达传感器数据<br>Step1: Data collection 数据收集<br>Step2: Data alignment 数据对齐<br>Step3: Statistical analysis 统计分析<br>Output: Comprehensive V2X dataset 综合V2X数据集 |
8.5 | [[8.5] 2502.14190 Stereo Image Coding for Machines with Joint Visual Feature Compression](https://arxiv.org/abs/2502.14190) <br> [{'name': 'Dengchao Jin, Jianjun Lei, Bo Peng, Zhaoqing Pan, Nam Ling, Qingming Huang'}] | Multi-view Stereo 多视角立体 | v2<br>stereo image compression<br>3D visual tasks | Input: Stereo images 立体图像<br>Step1: Feature extraction 特征提取<br>Step2: Feature compression 特征压缩<br>Step3: Data transmission 数据传输<br>Output: Efficiently compressed stereo visual features 高效压缩的立体视觉特征 |
8.5 | [[8.5] 2502.14191 Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models](https://arxiv.org/abs/2502.14191) <br> [{'name': 'Michihiro Yasunaga, Luke Zettlemoyer, Marjan Ghazvininejad'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>reward models<br>vision-language models<br>benchmark | Input: Vision-language models (VLMs) 视觉语言模型<br>Step1: Benchmark creation 基准创建<br>Step2: Expert annotation 专家标注<br>Step3: Model evaluation 模型评估<br>Output: Reward model evaluation reward model评估 |
8.5 | [[8.5] 2502.14195 Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition](https://arxiv.org/abs/2502.14195) <br> [{'name': 'Tianyi Shang, Zhenyu Li, Pengjie Xu, Jinwei Qiao, Gang Chen, Zihan Ruan, Weijun Hu'}] | Visual Place Recognition 视觉地点识别 | v2<br>text-vision registration<br>place recognition<br>cross-modal localization | Input: Multi-view images 多视角图像<br>Step1: Text embedding extraction 文本嵌入提取<br>Step2: Clustering of visual descriptors 视觉描述符聚类<br>Step3: Cross-modal alignment 交模态对齐<br>Output: Place recognition based on text-image pairs 基于文本-图像对的地点识别 |
8.5 | [[8.5] 2502.14279 OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images](https://arxiv.org/abs/2502.14279) <br> [{'name': 'Zhichao Zheng, Henry Williams, Bruce A MacDonald'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>monocular camera<br>autonomous driving | Input: Monocular camera images 单目相机图像<br>Step1: Data collection 数据收集<br>Step2: Depth estimation model training 深度估计模型训练<br>Step3: Consistency monitoring 一致性监测<br>Output: Enhanced depth maps 改进的深度图 |
8.5 | [[8.5] 2502.14316 Textured 3D Regenerative Morphing with 3D Diffusion Prior](https://arxiv.org/abs/2502.14316) <br> [{'name': 'Songlin Yang, Yushi Lan, Honghua Chen, Xingang Pan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D morphing<br>3D diffusion models<br>textured 3D representations | Input: Textured 3D objects 纹理3D对象<br>Step1: Source-target information integration 源-目标信息集成<br>Step2: 3D diffusion model application 3D扩散模型应用<br>Step3: Attention Fusion strategy implementation 注意力融合策略实施<br>Output: Morphing sequence output 变形序列输出 |
8.5 | [[8.5] 2502.14412 Evaluating Precise Geolocation Inference Capabilities of Vision Language Models](https://arxiv.org/abs/2502.14412) <br> [{'name': 'Neel Jay, Hieu Minh Nguyen, Trung Dung Hoang, Jacob Haimes'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>geolocation<br>privacy<br>dataset | Input: Images from Google Street View 来自 Google 街景图像<br>Step1: Dataset collection 数据集收集<br>Step2: Model evaluation 模型评估<br>Step3: Geolocation inference 位置推断<br>Output: Geolocation accuracy results 地理位置精度结果 |
8.5 | [[8.5] 2502.14454 Exploiting Deblurring Networks for Radiance Fields](https://arxiv.org/abs/2502.14454) <br> [{'name': 'Haeyun Choi, Heemin Yang, Janghyeok Han, Sunghyun Cho'}] | Neural Rendering 神经渲染 | v2<br>radiance fields<br>deblurring<br>3D Gaussian<br>novel view synthesis | Input: Blurred multi-view images 退化的多视角图像<br>Step1: RF-guided deblurring RF导向去模糊<br>Step2: Radiance field construction 辐射场构建<br>Step3: Iterative enhancement 迭代增强<br>Output: High-quality novel views 高质量新视图 |
8.5 | [[8.5] 2502.14503 LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera](https://arxiv.org/abs/2502.14503) <br> [{'name': 'Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection 3D物体检测<br>4D radar 4D雷达<br>camera camera | Input: 4D radar and camera data 4D 雷达和相机数据<br>Step1: Depth supervision strategy via radar points 通过雷达点的深度监督策略<br>Step2: Attention-based multi-modal fusion module attention-based 多模态融合模块<br>Step3: Model evaluation on standard datasets 在标准数据集上评估模型<br>Output: Enhanced detection accuracy 改进的检测精度 |
8.5 | [[8.5] 2502.14573 Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining](https://arxiv.org/abs/2502.14573) <br> [{'name': 'Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, Sunghoon Im'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>triplet mining<br>reflective surfaces<br>autonomous driving | Input: Monocular images 单目图像<br>Step1: Triplet mining to identify reflective regions 三元矿挖掘以识别反射区域<br>Step2: Apply reflection-aware triplet mining loss 应用反射感知的三元损失<br>Step3: Knowledge distillation for depth estimation 知识蒸馏以进行深度估计<br>Output: Enhanced depth map 改进的深度图 |
8.5 | [[8.5] 2502.14616 Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion](https://arxiv.org/abs/2502.14616) <br> [{'name': 'Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>segmentation<br>transparent objects | Input: Single RGB image 单幅RGB图像<br>Step1: Feature extraction 特征提取<br>Step2: Semantic and geometric fusion 语义和几何融合<br>Step3: Iterative feature refinement 迭代特征优化<br>Output: Segmentation mask and depth map 分割掩膜和深度图 |
8.5 | [[8.5] 2502.14676 BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution Network for Pedestrian and Heterogeneous Trajectory Prediction](https://arxiv.org/abs/2502.14676) <br> [{'name': 'Ruochen Li, Stamos Katsigiannis, Tae-Kyun Kim, Hubert P. H. Shum'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>trajectory prediction<br>behavioral pseudo-labels<br>autonomous vehicles | Input: Observed agent trajectories 所观测的代理轨迹<br>Step1: Unsupervised behavior clustering module 无监督行为聚类模块<br>Step2: Goal-guided trajectory prediction module 目标引导轨迹预测模块<br>Step3: Cascaded training scheme cascade training scheme<br>Output: Enhanced trajectory predictions 改进的轨迹预测 |
8.5 | [[8.5] 2502.14721 Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes](https://arxiv.org/abs/2502.14721) <br> [{'name': 'Lukas Rauch, Thomas Braml'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud<br>Semantic Segmentation<br>Transformer Models<br>Construction Industry | Input: Point cloud data from shell construction sites 壳体建筑现场的点云数据<br>Step1: Supervised training using custom validation dataset 使用自定义验证数据集进行监督训练<br>Step2: Cross-domain inference with existing datasets 使用现有数据集进行跨域推理<br>Step3: Transfer learning to enhance performance 迁移学习以提高性能<br>Output: Improved semantic segmentation for construction components 改进的建筑组件语义分割 |
8.5 | [[8.5] 2502.14792 RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation](https://arxiv.org/abs/2502.14792) <br> [{'name': 'Henrique Pi\\~neiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Bird's Eye View segmentation 鸟瞰视图分割<br>self-supervised training 自监督训练 | Input: Video sequences 视频序列<br>Step1: Monocular semantic segmentation 单目语义分割<br>Step2: Rendering of perspective views 视角图像渲染<br>Step3: Self-supervised training 自监督训练<br>Output: BEV segmentation results BEV分割结果 |
8.5 | [[8.5] 2502.14801 AVD2: Accident Video Diffusion for Accident Video Description](https://arxiv.org/abs/2502.14801) <br> [{'name': 'Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, Hao Zhao'}] | Autonomous Driving 自动驾驶 | v2<br>Accident Video Diffusion<br>Autonomous Driving<br>Video Understanding | Input: Accident videos 事故视频<br>Step1: Video generation 视频生成<br>Step2: Detailed description alignment 详细描述对齐<br>Step3: Actionable prevention strategies 制定可行动的预防策略<br>Output: Enhanced understanding of accident scenarios 提升对事故场景的理解 |
7.5 | [[7.5] 2502.14221 H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging](https://arxiv.org/abs/2502.14221) <br> [{'name': 'Zhen Huang, Ronghao Xu, Xiaoqian Zhou, Yangbo Wei, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D landmark detection<br>medical image analysis<br>deep learning | Input: 3D volumetric data 3D体积数据<br>Step1: Local feature extraction 局部特征提取<br>Step2: Global dependency modeling 全局依赖建模<br>Step3: Multi-scale feature fusion 多尺度特征融合<br>Output: Accurate 3D landmark detection 精确的3D特征检测 |
7.5 | [[7.5] 2502.14493 CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond](https://arxiv.org/abs/2502.14493) <br> [{'name': 'Yukai Shi, Cidan Shi, Zhipeng Weng, Yin Tian, Xiaoyu Xian, Liang Lin'}] | Image Fusion 图像融合 | v2<br>Infrared-visible fusion 红外可见图像融合<br>Autonomous driving 自动驾驶 | Input: Infrared and visible images 红外和可见图像<br>Step1: External data augmentation by Top-k Selective Vision Alignment 使用 Top-k 选择性视觉对齐的外部数据增强<br>Step2: Internal data augmentation with self-supervised learning 使用自监督学习的内部数据增强<br>Step3: Fusion process 融合过程<br>Output: Enhanced fused images 改进的融合图像 |
6.0 | [[6.0] 2502.14070 DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models](https://arxiv.org/abs/2502.14070) <br> [{'name': 'Daewon Chae, June Suk Choi, Jinkyu Kim, Kimin Lee'}] | Image Generation 图像生成 | v2<br>text-to-image generation<br>reward fine-tuning<br>diffusion models | Input: Text prompts 文本提示<br>Step1: Dynamic scaling of classifier-free guidance 动态调整无分类器引导的规模<br>Step2: Randomly weight prompt phrases 随机加权提示短语<br>Step3: Sample generation and evaluation 样本生成与评估<br>Output: Improved sampling efficiency 改进的采样效率 |


## Arxiv 2025-02-20

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.13335 Geometry-Aware Diffusion Models for Multiview Scene Inpainting](https://arxiv.org/abs/2502.13335) <br> [{'name': 'Ahmad Salimi, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Konstantinos G. Derpanis'}] | 3D Scene Inpainting 3D场景修复 | v2<br>3D inpainting<br>multi-view consistency<br>geometry-aware models | Input: Multi-view images 多视角图像<br>Step1: Image masking 图像遮蔽<br>Step2: Geometry-aware fusion 几何感知融合<br>Step3: Generative inpainting 生成式修复<br>Output: Multi-view consistent images 多视角一致图像 |
9.5 | [[9.5] 2502.13803 3D Gaussian Splatting aided Localization for Large and Complex Indoor-Environments](https://arxiv.org/abs/2502.13803) <br> [{'name': 'Vincent Ress, Jonas Meyer, Wei Zhang, David Skuddis, Uwe Soergel, Norbert Haala'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>visual localization<br>SLAM<br>indoor environments | Input: Multi-view images 多视角图像<br>Step1: Use visual SLAM to generate a 3D Gaussian Splatting (3DGS) based map 使用视觉SLAM生成基于3D高斯的地图<br>Step2: Render images from the 3DGS map to create reference data 从3DGS地图中渲染图像以创建参考数据<br>Step3: Evaluate the performance impact of additional rendered views 评估附加渲染视图对性能的影响<br>Output: Improved localization accuracy 改进的定位精度 |
9.5 | [[9.5] 2502.13968 Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects](https://arxiv.org/abs/2502.13968) <br> [{'name': 'Suhas Gopal, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>neuro-implicit methods<br>multi-view<br>human-object interactions | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Alpha-blending regularization implementation α混合正则化实施<br>Step4: Joint optimization of Signed Distance Fields (SDFs) 联合优化有符号距离场(SDF)<br>Output: Separable 3D geometries 可分离的3D几何 |
8.5 | [[8.5] 2502.13524 MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis](https://arxiv.org/abs/2502.13524) <br> [{'name': 'Wei Dai, Steven Wang, Jun Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D medical imaging<br>segmentation<br>deep learning | Input: 3D medical images 三维医学图像<br>Step1: Data transformation 数据转换<br>Step2: Model enhancement 模型增强<br>Step3: Evaluation on datasets 数据集评估<br>Output: Efficient segmentation results 高效分割结果 |
8.5 | [[8.5] 2502.13883 Multi-view Video-Pose Pretraining for Operating Room Surgical Activity Recognition](https://arxiv.org/abs/2502.13883) <br> [{'name': 'Idris Hamoud, Vinkle Srivastav, Muhammad Abdullah Jamal, Didier Mutter, Omid Mohareri, Nicolas Padoy'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>Surgical Activity Recognition<br>Multi-view<br>Pose Estimation<br>Computer Vision | Input: Multi-view camera recordings 多视角摄像头录制<br>Step1: Align 2D pose and vision embeddings 2D姿势和视觉嵌入对齐<br>Step2: Dual-encoder architecture implementation 双编码器架构实现<br>Step3: Pretraining with geometric constraints 几何约束预训练<br>Output: Enhanced surgical activity recognition model 改进的手术活动识别模型 |


## Arxiv 2025-02-19

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.12456 Not-So-Optimal Transport Flows for 3D Point Cloud Generation](https://arxiv.org/abs/2502.12456) <br> [{'name': 'Ka-Hei Hui, Chao Liu, Xiaohui Zeng, Chi-Wing Fu, Arash Vahdat'}] | 3D Generation 三维生成 | v2<br>3D point cloud generation 3D 点云生成<br>Optimal transport 最优传输<br>Shape completion 形状补全 | Input: 3D point clouds 3D 点云<br>Step1: Analyze existing models 分析现有模型<br>Step2: Propose not-so-optimal transport flow models 提出不那么最优的传输流模型<br>Step3: Empirical study 实证研究<br>Output: Enhanced generation techniques 改进的生成技术 |
9.5 | [[9.5] 2502.12534 NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization](https://arxiv.org/abs/2502.12534) <br> [{'name': 'Zhen Li, Weiwei Sun, Shrisudhan Govindarajan, Shaobo Xia, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>point cloud<br>signed distance field<br>autonomous driving | Input: Irregular point cloud 不规则点云<br>Step1: Convert to signed distance field (SDF) 转换为有符号距离场<br>Step2: Serialize point cloud into tokens 将点云序列化为标记<br>Step3: Predict SDF by aggregating features 通过聚合特征预测SDF值<br>Output: Reconstructed surface  重建表面 |
9.5 | [[9.5] 2502.12545 IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360$^\circ$ Cameras](https://arxiv.org/abs/2502.12545) <br> [{'name': 'Dongki Jung, Jaehoon Choi, Yonghan Lee, Dinesh Manocha'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction 三维重建<br>Omnidirectional cameras 全向摄像头<br>Texture optimization 纹理优化 | Input: Omnidirectional images 全向图像<br>Step1: Feature detection 特征检测<br>Step2: Sparse matching with spherical model 使用球形模型进行稀疏匹配<br>Step3: Neural implicit surface reconstruction 神经隐式表面重建<br>Step4: Texture mapping and optimization 纹理映射和优化<br>Output: Textured meshes with improved rendering quality 改进的三维纹理网格 |
9.5 | [[9.5] 2502.12673 ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by NeRFs Composition](https://arxiv.org/abs/2502.12673) <br> [{'name': "Quoc-Anh Bui, Gilles Rougeron, G\\'eraldine Morin, Simone Gasparini"}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction 3D重建<br>Neural Radiance Fields 神经辐射场<br>visualization 可视化<br>level of detail 细节级别 | Input: Multi-view images 多视角图像<br>Step1: Decompose the scene into Scene NeRF and ROI NeRFs 将场景分解为场景NeRF和感兴趣区域NeRF<br>Step2: Camera selection module chooses relevant cameras 相机选择模块选择相关相机<br>Step3: Ray-level compositional rendering combines NeRFs 使用光线级组合渲染结合NeRF<br>Output: High-fidelity rendered images outputs 高保真渲染图像 |
9.5 | [[9.5] 2502.12894 CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image](https://arxiv.org/abs/2502.12894) <br> [{'name': 'Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, Jingyi Yu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>robotics<br>scene recovery | Input: Single RGB image 单张RGB图像<br>Step1: Extract object-level 2D segmentation 提取物体级2D分割<br>Step2: Analyze inter-object spatial relationships 分析物体间空间关系<br>Step3: Generate object geometries 生成物体几何<br>Step4: Align and integrate meshes with point cloud 对齐并集成网格与点云<br>Step5: Optimize object poses using physics-aware methods 利用物理感知方法优化物体姿态<br>Output: High-quality 3D scene reconstruction 高质量3D场景重建 |
9.5 | [[9.5] 2502.12985 PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization](https://arxiv.org/abs/2502.12985) <br> [{'name': 'Nicolas Talabot, Olivier Clerc, Arda Cinar Demirtas, Doruk Oner, Pascal Fua'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D shape representation<br>implicit neural representation<br>part-based modeling | Input: Composite 3D shapes 复合三维形状<br>Step 1: Supervised part-aware representation 监督的部件感知表示<br>Step 2: Modeling independent parts 模型独立部件<br>Step 3: Shape optimization 形状优化<br>Output: Controllable 3D models 可控的三维模型 |
9.5 | [[9.5] 2502.13071 RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection](https://arxiv.org/abs/2502.13071) <br> [{'name': 'Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, Ming-Hsuan Yang'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection 3D目标检测<br>radar-camera fusion 雷达-相机融合<br>autonomous driving 自动驾驶 | Input: Multi-modal data from radar and camera 传感器与相机的多模态数据<br>Step1: Systematic analysis of noise patterns 噪音模式的系统分析<br>Step2: Development of 3D Gaussian Expansion (3DGE) module 开发3D高斯扩展模块<br>Step3: Implementation of weather-adaptive fusion module 实现天气自适应融合模块<br>Output: Robust 3D object detection results 稳健的3D目标检测结果 |
9.5 | [[9.5] 2502.13144 RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning](https://arxiv.org/abs/2502.13144) <br> [{'name': 'Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, Ying Zhang, Wenyu Liu, Qian Zhang, Xinggang Wang'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>3DGS<br>reinforcement learning | Input: Photorealistic digital replica of the real world 逼真的数字复制环境<br>Step1: Establish closed-loop reinforcement learning paradigm 建立闭环强化学习范式<br>Step2: Incorporate imitation learning for alignment 融入模仿学习以进行对齐<br>Step3: Design specialized reward functions 设计专门的奖励函数<br>Output: Optimized end-to-end driving policy 优化的端到端驾驶策略 |
9.0 | [[9.0] 2502.12231 PUGS: Zero-shot Physical Understanding with Gaussian Splatting](https://arxiv.org/abs/2502.12231) <br> [{'name': 'Yinghao Shuai, Ran Yu, Yuantao Chen, Zijian Jiang, Xiaowei Song, Nan Wang, Jv Zheng, Jianzhu Ma, Meng Yang, Zhicheng Wang, Wenbo Ding, Hao Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Splatting<br>physical properties<br>robotics | Input: Multi-view images 多视角图像<br>Step1: Shape-aware 3D Gaussian Splatting reconstruction 形状感知的3D高斯点云重建<br>Step2: Geometry-aware regularization loss geometry-aware regularization loss functions<br>Step3: Region-aware feature contrastive loss region-aware feature contrastive loss functions<br>Step4: Physical property prediction with VLMs 使用视觉语言模型进行物理属性预测<br>Output: 3D models with physical properties and enhanced quality 具有物理属性和增强质量的3D模型 |
9.0 | [[9.0] 2502.12546 Spatiotemporal Multi-Camera Calibration using Freely Moving People](https://arxiv.org/abs/2502.12546) <br> [{'name': 'Sang-Eun Lee, Ko Nishino, Shohei Nobuhara'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>multi-camera calibration<br>3D reconstruction<br>freely moving people | Input: Multi-view videos with freely moving people 多视角视频与自由移动的人<br>Step1: 3D pose estimation from videos 从视频中进行3D姿态估计<br>Step2: Solve rotation and translation with 3D points 求解与三维点的旋转和平移<br>Step3: Optimize camera poses and temporal offsets 优化相机姿态和时间偏移<br>Output: Accurate camera calibration and scene reconstruction 输出：准确的相机标定和场景重建 |
9.0 | [[9.0] 2502.12752 High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion](https://arxiv.org/abs/2502.12752) <br> [{'name': 'Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers'}] | Novel View Synthesis 新视图合成 | v2<br>Novel View Synthesis<br>Splatting<br>Diffusion Model | Input: Single image 单张图像<br>Step1: Splatting for pixel alignment 像素对齐的点云处理<br>Step2: Diffusion model training 扩散模型训练<br>Step3: Texture generation texture generation通过自适应特征融合<br>Output: High-fidelity novel views 高保真新视图 |
8.5 | [[8.5] 2502.12303 From Gaming to Research: GTA V for Synthetic Data Generation for Robotics and Navigations](https://arxiv.org/abs/2502.12303) <br> [{'name': 'Matteo Scucchia, Matteo Ferrara, Davide Maltoni'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>synthetic data<br>GTA V<br>SLAM<br>Visual Place Recognition<br>robotics | Input: Synthetic environment data from GTA V 以GTA V的合成环境数据为输入<br>Step1: Data generation 数据生成<br>Step2: Algorithm for VPR dataset creation VPR数据集创建算法<br>Step3: Experimentation for SLAM and VPR applications 针对SLAM和VPR应用的实验<br>Output: Usable synthetic datasets for robotics 提供可用的机器人合成数据集 |
8.5 | [[8.5] 2502.12360 Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions](https://arxiv.org/abs/2502.12360) <br> [{'name': 'Sujan Sai Gannamaneni, Rohil Prakash Rao, Michael Mock, Maram Akila, Stefan Wrobel'}] | Vision Models and Safety Analysis 视觉模型与安全分析 | v2<br>systematic weaknesses<br>autonomous driving<br>computer vision | Input: Image dataset 图像数据集<br>Step1: Metadata generation 元数据生成<br>Step2: Slice discovery 模块切片发现<br>Step3: Systematic weakness identification 系统弱点识别<br>Output: Identified weaknesses identified weaknesses |
8.5 | [[8.5] 2502.12640 RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation](https://arxiv.org/abs/2502.12640) <br> [{'name': 'Chenxi Zheng, Yihong Lin, Bangzhen Liu, Xuemiao Xu, Yongwei Nie, Shengfeng He'}] | 3D Generation 三维生成 | v2<br>3D generation<br>text-to-3D generation<br>score distillation | Input: Text-based descriptions 基于文本的描述<br>Step1: Data distribution rectification 数据分布整治<br>Step2: Pose consistency enhancement 姿态一致性增强<br>Step3: Integration with score distillation algorithms 与得分蒸馏算法集成<br>Output: Consistent 3D asset generation 一致的3D资产生成 |
8.5 | [[8.5] 2502.12742 3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces](https://arxiv.org/abs/2502.12742) <br> [{'name': 'Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>brain MRI<br>diffusion model | Input: Continuous cortical shape priors 连续皮层形状先验<br>Step1: Leverage Brownian bridge process 利用布朗桥过程<br>Step2: Map shape contours to synthetic MRIs 将形状轮廓映射到合成MRI<br>Step3: Improve geometric accuracy 改进几何精度<br>Output: Anatomically plausible brain MRIs 解剖学上合理的脑MRI |
8.5 | [[8.5] 2502.12819 Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh Representations](https://arxiv.org/abs/2502.12819) <br> [{'name': 'Hinrich Rahlfs, Markus H\\"ullebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>plaque analysis<br>carotid artery | Input: MRI scans of carotid arteries 磁共振扫描的颈动脉<br>Step1: 3D vessel wall segmentation 3D血管壁分割<br>Step2: Distance encoding to extract plaque mesh 使用距离编码提取斑块网格<br>Step3: Quantification and visualization of plaque parameters 斑块参数的量化和可视化<br>Output: Detailed 3D plaque models 详细的3D斑块模型 |
8.5 | [[8.5] 2502.12860 An Experimental Study of SOTA LiDAR Segmentation Models](https://arxiv.org/abs/2502.12860) <br> [{'name': 'Bike Chen, Antti Tikanm\\"aki, Juha R\\"oning'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Segmentation<br>LiDAR<br>autonomous driving | Input: LiDAR data LiDAR数据<br>Step 1: Data acquisition 数据采集<br>Step 2: Model training and evaluation 模型训练与评估<br>Step 3: Performance comparison 性能比较<br>Output: Selection of optimal PCS models 最优PCS模型选择 |
8.5 | [[8.5] 2502.12994 SHADeS: Self-supervised Monocular Depth Estimation Through Non-Lambertian Image Decomposition](https://arxiv.org/abs/2502.12994) <br> [{'name': 'Rema Daher, Francisco Vasconcelos, Danail Stoyanov'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>specular reflection<br>self-supervised learning | Input: Single images 单幅图像<br>Step1: Image decomposition 图像分解<br>Step2: Depth and light component estimation 深度和光成分估计<br>Step3: Model validation against real data 模型验证与真实数据<br>Output: Depth maps and light components 深度图和光成分 |
8.5 | [[8.5] 2502.13037 Enhancing Power Grid Inspections with Machine Learning](https://arxiv.org/abs/2502.13037) <br> [{'name': 'Diogo Lavado, Ricardo Santos, Andre Coelho, Joao Santos, Alessandra Micheletti, Claudia Soares'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D computer vision<br>3D semantic segmentation<br>power grid inspections | Input: 3D LiDAR point clouds 3D LiDAR 点云<br>Step1: Data preprocessing 数据预处理<br>Step2: 3D semantic segmentation 3D 语义分割<br>Step3: Performance evaluation 性能评估<br>Output: Enhanced detection results 改进的检测结果 |
8.5 | [[8.5] 2502.13130 Magma: A Foundation Model for Multimodal AI Agents](https://arxiv.org/abs/2502.13130) <br> [{'name': 'Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal AI<br>robotic manipulation<br>vision-language models | Input: Heterogeneous multimodal data 其他模态数据<br>Step1: Data labeling for action grounding and planning 动作基础和规划数据标记<br>Step2: Model training with SoM and ToM techniques 使用SoM和ToM技术进行模型训练<br>Step3: Evaluation on various tasks 在各种任务上进行评估<br>Output: A multimodal AI agent capable of understanding and acting on inputs 输出：能够理解和根据输入执行操作的多模态AI代理 |
7.5 | [[7.5] 2502.12801 Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations](https://arxiv.org/abs/2502.12801) <br> [{'name': 'Hinrich Rahlfs, Markus H\\"ullebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth'}] | 3D Segmentation 3D分割 | v2<br>3D segmentation 3D分割<br>clinical annotations 临床标注<br>carotid artery 颈动脉 | Input: Sparse annotations from clinical studies 临床研究中的稀疏标注<br>Step1: Sample perpendicular cross-sections of the carotid artery 采样颈动脉的垂直横截面<br>Step2: Segment using an adversarial 2D network 使用对抗性2D网络进行分割<br>Step3: Transform annotations into 3D pseudo-labels 将标注转换为3D伪标签<br>Output: Train a 3D convolutional neural network 训练3D卷积神经网络 |
7.5 | [[7.5] 2502.13146 Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization](https://arxiv.org/abs/2502.13146) <br> [{'name': 'Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision Language Models<br>cross-modal applications<br>direct preference optimization<br>visual question answering | Input: Vision Language Models data 视觉语言模型数据<br>Step1: Construct dual-preference dataset 构建双重偏好数据集<br>Step2: Fine-tune with rDPO using visual preference signals 使用视觉偏好信号进行rDPO微调<br>Output: Improved VLM alignment 改进的VLM对齐 |


## Arxiv 2025-02-18

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.10674 Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition](https://arxiv.org/abs/2502.10674) <br> [{'name': 'Khanh Nguyen, Ghulam Mubashar Hassan, Ajmal Mian'}] | 3D Object Recognition 3D物体识别 | v2<br>3D object recognition 3D物体识别<br>point clouds 点云<br>occlusion-aware遮挡感知 | Input: Synthetic 3D models from ShapeNetCore 3D模型<br>Step1: Generate partial point clouds from 3D models 从3D模型生成部分点云<br>Step2: Implement occlusion-aware pretraining 进行遮挡感知预训练<br>Step3: Evaluate recognition performance 评估识别性能<br>Output: Improved recognition accuracy 提高识别准确性 |
9.5 | [[9.5] 2502.10704 Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy](https://arxiv.org/abs/2502.10704) <br> [{'name': 'Mingyang Zhao, Gaofeng Meng, Dong-Ming Yan'}] | Point Cloud Processing 点云处理 | v2<br>non-rigid registration<br>point cloud alignment<br>occlusion handling | Input: Point cloud data 点云数据<br>Step1: Identify occluded regions 确定遮挡区域<br>Step2: Apply maximum correntropy criterion 采用最大相关熵准则<br>Step3: Optimize deformation field 优化变形场<br>Output: Accurately aligned point clouds 准确对齐的点云 |
9.5 | [[9.5] 2502.10827 E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting](https://arxiv.org/abs/2502.10827) <br> [{'name': 'Sohaib Zahid, Viktor Rudnev, Eddy Ilg, Vladislav Golyanik'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>novel view synthesis<br>event cameras<br>3D rendering<br>Gaussian splatting | Input: Event camera data 事件相机数据<br>Step1: Data processing 数据处理<br>Step2: 3D Gaussian representation construction 3D高斯表示构建<br>Step3: Novel view synthesis 利用生成新的视角<br>Output: High-quality rendered scenes 高质量渲染场景 |
9.5 | [[9.5] 2502.10842 Mobile Robotic Multi-View Photometric Stereo](https://arxiv.org/abs/2502.10842) <br> [{'name': 'Suryansh Kumar'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Multi-View Photometric Stereo<br>3D acquisition<br>Mobile Robotics | Input: Multi-view images 多视角图像<br>Step1: Supervised learning setup for predicting surface normals, object depth, and uncertainty 监督学习设置以预测表面法线、物体深度和不确定性<br>Step2: Solve MVPS-driven optimization problem to refine depth maps 解决基于MVPS的优化问题以细化深度图<br>Step3: Fuse refined depth maps while tracking camera pose 融合精细化深度图并跟踪相机位姿<br>Output: Globally consistent 3D geometry 具有全局一致性的3D几何体 |
9.5 | [[9.5] 2502.10982 TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://arxiv.org/abs/2502.10982) <br> [{'name': 'Yunfei Liu, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li'}] | 3D Reconstruction 三维重建 | v2<br>3D facial reconstruction<br>expression capture<br>neural renderer | Input: A single in-the-wild image 一张单一的野外图像<br>Step1: Extract hybrid facial parameters 提取混合面部参数<br>Step2: Design multi-scale tokenizer 设计多尺度标记器<br>Step3: Implement token-guided neural renderer 实现标记引导的神经渲染器<br>Step4: Train with token cycle loss 采用标记周期损失进行训练<br>Output: High-fidelity facial expressions output 高保真的面部表情输出 |
9.5 | [[9.5] 2502.10988 OMG: Opacity Matters in Material Modeling with Gaussian Splatting](https://arxiv.org/abs/2502.10988) <br> [{'name': 'Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie'}] | Neural Rendering 神经渲染 | v2<br>neural rendering<br>3D Gaussian Splatting<br>material modeling<br>opacity | Input: Images 图像<br>Step1: Inverse rendering process 逆向渲染过程<br>Step2: Opacity modeling 透明度建模<br>Step3: Algorithm integration 集成算法<br>Output: Improved material properties 改进的材料属性 |
9.5 | [[9.5] 2502.11390 MARS: Mesh AutoRegressive Model for 3D Shape Detailization](https://arxiv.org/abs/2502.11390) <br> [{'name': 'Jingnan Gao, Weizhe Liu, Weixuan Sun, Senbo Wang, Xibin Song, Taizhang Shang, Shenzhou Chen, Hongdong Li, Xiaokang Yang, Yichao Yan, Pan Ji'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape detailization<br>Generative Adversarial Networks (GANs)<br>geometry-consistency<br>MARS<br>autoregressive model | Input: Coarse mesh shapes 低质量网格形状<br>Step1: Tokenization of meshes 网格的标记化<br>Step2: Geometry-consistency supervision geometry-consistency 监督<br>Step3: Autoregressive detailization 自回归细节化<br>Output: Detailed meshes 细化的网格 |
9.5 | [[9.5] 2502.11618 Real-time Neural Rendering of LiDAR Point Clouds](https://arxiv.org/abs/2502.11618) <br> [{'name': 'Joni Vanherck, Brent Zoomers, Tom Mertens, Lode Jorissen, Nick Michiels'}] | Neural Rendering 神经渲染 | v2<br>Neural Rendering<br>LiDAR Point Clouds<br>Real-time Rendering | Input: LiDAR point clouds LiDAR点云<br>Step1: Point cloud projection 点云投影<br>Step2: Depth-based filtering based on heuristics 基于启发式的深度过滤<br>Step3: Final image reconstruction using U-Net 使用U-Net进行最终图像重建<br>Output: Photorealistic images of LiDAR scans LiDAR扫描的照片真实图像 |
9.5 | [[9.5] 2502.11777 Deep Neural Networks for Accurate Depth Estimation with Latent Space Features](https://arxiv.org/abs/2502.11777) <br> [{'name': 'Siddiqui Muhammad Yasir, Hyunsik Ahn'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>3D scene reconstruction | Input: RGB image to depth image mapping<br>Step1: Feature extraction using latent space<br>Step2: Dual encoder-decoder architecture<br>Step3: Introduce a novel loss function<br>Output: Enhanced depth maps with improved boundaries |
9.5 | [[9.5] 2502.11801 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency](https://arxiv.org/abs/2502.11801) <br> [{'name': 'Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang'}] | 3D Inpainting 3D修复 | v2<br>3D Gaussian Inpainting<br>Neural Radiance Field<br>multi-view consistency<br>3D reconstruction<br>computer vision | Input: Multi-view images 多视角图像<br>Step1: Infer Depth-Guided Inpainting Masks 深度引导的修复掩码推断<br>Step2: Update inpainting mask based on background pixels 更新修复掩码基于背景像素<br>Step3: Perform 3D inpainting with cross-view consistency 在视图间一致性下进行3D修复<br>Output: High-fidelity 3D inpainting results 高保真3D修复结果 |
9.5 | [[9.5] 2502.12135 MagicArticulate: Make Your 3D Models Articulation-Ready](https://arxiv.org/abs/2502.12135) <br> [{'name': 'Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, Guosheng Lin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D models<br>articulation<br>skeleton generation<br>skinning weights | Input: Static 3D models 静态3D模型<br>Step1: Dataset creation 数据集合成<br>Step2: Skeleton generation 骨架生成<br>Step3: Skinning weight prediction 皮肤权重预测<br>Output: Articulation-ready 3D models 准备好的关节动作3D模型 |
9.5 | [[9.5] 2502.12138 FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views](https://arxiv.org/abs/2502.12138) <br> [{'name': 'Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>camera pose estimation<br>novel view synthesis | Input: Uncalibrated sparse-view images 未标定稀疏视图<br>Step1: Camera pose estimation 摄像机姿态估计<br>Step2: Geometry and appearance estimation 几何体和外观估计<br>Step3: Novel-view synthesis 新视图合成<br>Output: High-quality 3D geometry 高质量三维几何体 |
9.2 | [[9.2] 2502.10492 Multi-view 3D surface reconstruction from SAR images by inverse rendering](https://arxiv.org/abs/2502.10492) <br> [{'name': 'Emile Barbier--Renard (IDS, IMAGES), Florence Tupin (IMAGES, IDS), Nicolas Trouv\\\'e (LabHC), Lo\\"ic Denis (LabHC)'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction<br>SAR Imaging<br>Inverse Rendering<br>Deep Learning | Input: SAR images from radar sensors 合成孔径雷达图像<br>Step1: Develop a differentiable rendering model 开发可微分的渲染模型<br>Step2: Implement a coarse-to-fine MLP strategy 实施精细训练的多层感知器策略<br>Step3: Train the model on synthetic datasets 在合成数据集上训练模型<br>Output: 3D surface reconstruction results 3D表面重建结果 |
9.2 | [[9.2] 2502.10606 HIPPo: Harnessing Image-to-3D Priors for Model-free Zero-shot 6D Pose Estimation](https://arxiv.org/abs/2502.10606) <br> [{'name': 'Yibo Liu, Zhaodong Jiang, Binbin Xu, Guile Wu, Yuan Ren, Tongtong Cao, Bingbing Liu, Rui Heng Yang, Amir Rasouli, Jinjun Shan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>6D pose estimation<br>image-to-3D<br>Diffusion Models | Input: Images and scenes from robotics applications<br>Step1: Utilize image-to-3D priors to generate initial meshes<br>Step2: Estimate the 6D pose of observed objects<br>Step3: Continuously refine the mesh and pose estimation based on new observations<br>Output: Enhanced 3D mesh and accurate 6D pose estimation |
8.7 | [[8.7] 2502.11663 MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction](https://arxiv.org/abs/2502.11663) <br> [{'name': 'Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>video generation<br>mask reconstruction | Input: Video sequences 视频序列<br>Step1: Video mask reconstruction 视频掩码重建<br>Step2: Diffusion Transformer training 扩散变换器训练<br>Step3: Model evaluation 模型评估<br>Output: Generalizable driving world model 通用驾驶世界模型 |
8.7 | [[8.7] 2502.12080 HumanGif: Single-View Human Diffusion with Generative Prior](https://arxiv.org/abs/2502.12080) <br> [{'name': 'Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human reconstruction<br>novel view synthesis<br>human avatars | Input: Single-view image 单视图图像<br>Step1: Integrate generative priors from diffusion models 从扩散模型中集成生成先验<br>Step2: Implement Human NeRF module 引入Human NeRF模块<br>Step3: Optimize with image-level loss 使用图像级损失进行优化<br>Output: Novel view and pose consistent human avatars 输出: 新视图和姿态一致的人类头像 |
8.5 | [[8.5] 2502.10498 The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2502.10498) <br> [{'name': 'Sifan Tu, Xin Zhou, Dingkang Liang, Xingyu Jiang, Yumeng Zhang, Xiaofan Li, Xiang Bai'}] | Autonomous Driving 自动驾驶 | v2<br>Driving World Model<br>autonomous driving<br>scene prediction<br>3D perception | Step1: Literature review and categorization of DWM approaches 进行文献回顾并对DWM方法进行分类<br>Step2: Analysis of existing methodologies and datasets 对现有方法和数据集进行分析<br>Step3: Discussion on limitations and future directions 讨论局限性和未来方向 |
8.5 | [[8.5] 2502.10603 Adaptive Neural Networks for Intelligent Data-Driven Development](https://arxiv.org/abs/2502.10603) <br> [{'name': 'Youssef Shoeb, Azarm Nowzad, Hanno Gottschalk'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>adaptive neural networks<br>autonomous driving<br>out-of-distribution learning | Input: Autonomous driving environments 自动驾驶环境<br>Step1: Data collection 数据收集<br>Step2: Dynamic integration of new object classes 新对象类别的动态集成<br>Step3: Continuous learning 模型的持续学习<br>Output: Adaptive perception system 自适应感知系统 |
8.5 | [[8.5] 2502.10720 NPSim: Nighttime Photorealistic Simulation From Daytime Images With Monocular Inverse Rendering and Ray Tracing](https://arxiv.org/abs/2502.10720) <br> [{'name': 'Shutong Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>mesh reconstruction<br>autonomous driving<br>nighttime simulation | Input: Daytime images and semantic labels 白天图像和语义标签<br>Step1: Mesh reconstruction 网格重建<br>Step2: Relighting 重光照<br>Step3: Nighttime image simulation 夜间图像仿真<br>Output: Realistic nighttime images 真实的夜间图像 |
8.5 | [[8.5] 2502.10724 Semantics-aware Test-time Adaptation for 3D Human Pose Estimation](https://arxiv.org/abs/2502.10724) <br> [{'name': 'Qiuxia Lin, Rongyu Chen, Kerui Gu, Angela Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Human Pose Estimation<br>Test-time adaptation<br>Semantics-aware motion prior | Input: Video sequences containing human poses 视频 sequences containing human poses<br>Step1: Identify semantics from video using language models 使用语言模型识别视频中的语义<br>Step2: Integrate motion prior with semantic information 将运动先验与语义信息整合<br>Step3: Adapt 3D pose predictions during test-time adaptation (TTA) 在测试时间适应中调整3D姿势预测<br>Output: Refined 3D pose estimations 提炼的3D姿势估计 |
8.5 | [[8.5] 2502.11287 MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring](https://arxiv.org/abs/2502.11287) <br> [{'name': 'Arpitsinh Vaghela, Duo Lu, Aayush Atul Verma, Bharatesh Chakravarthi, Hua Wei, Yezhou Yang'}] | 3D Perception 3D感知 | v2<br>3D perception 3D感知<br>traffic monitoring 交通监测<br>multi-camera 多摄像头<br>occupancy detection 占用检测 | Input: Multi-camera images 多摄像头图像<br>Step1: Data acquisition 数据收集<br>Step2: Background integration 背景集成<br>Step3: Late and early fusion 方法的融合<br>Output: BEV occupancy map BEV占用图 |
8.5 | [[8.5] 2502.11307 Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection](https://arxiv.org/abs/2502.11307) <br> [{'name': 'Jiaxiang Wang, Haote Xu, Xiaolu Chen, Haodi Xu, Yue Huang, Xinghao Ding, Xiaotong Tu'}] | 3D Anomaly Detection 3D异常检测 | v2<br>anomaly detection<br>3D point cloud<br>Point-Language model | Input: 3D point clouds 3D点云<br>Step1: Dual-prompt learning 双提示学习<br>Step2: Dynamic prompt creation 动态提示创建<br>Step3: Anomaly detection 异常检测<br>Output: Enhanced anomaly detection performance 改进的异常检测性能 |
8.5 | [[8.5] 2502.11586 Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis from Japanese Haiku](https://arxiv.org/abs/2502.11586) <br> [{'name': 'Chunan Yu, Yidong Han, Chaotao Ding, Ying Zang, Lanyun Zhu, Xinhao Chen, Zejian Li, Renjun Xu, Tianrun Chen'}] | 3D Scene Generation 三维场景生成 | v2<br>3D scene synthesis<br>Japanese Haiku | Input: Japanese Haiku 日本俳句<br>Step1: Literary analysis 文学分析<br>Step2: Spatial representation 空间表现<br>Step3: 3D scene synthesis 三维场景合成<br>Output: Navigable 3D scenes 可导航三维场景 |
8.5 | [[8.5] 2502.11642 GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text](https://arxiv.org/abs/2502.11642) <br> [{'name': 'Gyumin Shim, Sangmin Lee, Jaegul Choo'}] | Image Generation 图像生成 | v2<br>3D human models<br>Gaussian Splatting<br>text-to-3D generation<br>animation | Input: Textual descriptions 文本描述<br>Step1: Data integration 数据集成<br>Step2: Model optimization 模型优化<br>Step3: Animation generation 动画生成<br>Output: Animatable 3D avatars 可动画的三维头像 |
8.5 | [[8.5] 2502.11697 MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow](https://arxiv.org/abs/2502.11697) <br> [{'name': 'Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang'}] | Image and Video Generation 图像生成 | v2<br>4D generation<br>multiview diffusion models<br>autonomous systems | Input: Monocular videos 单目视频<br>Step1: Generate multiview images using multiview diffusion models 利用多视角扩散模型生成多视角图像<br>Step2: Associate pixels using token flow technique 使用令牌流技术关联像素<br>Step3: Refine the coarse 4D field 细化粗糙的4D场<br>Output: High-quality 4D field 高质量4D场 |
8.5 | [[8.5] 2502.11710 The Worse The Better: Content-Aware Viewpoint Generation Network for Projection-related Point Cloud Quality Assessment](https://arxiv.org/abs/2502.11710) <br> [{'name': 'Zhiyong Su, Bingxu Xie, Zheng Li, Jincan Wu, Weiqing Li'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Quality Assessment 点云质量评估<br>Content-Aware Viewpoint Generation 内容感知视点生成<br>Geometric Features 几何特征 | Input: Degraded point clouds 退化点云<br>Step1: Extract multi-scale geometric and texture features 提取多尺度几何和纹理特征<br>Step2: Refine features per viewpoint 针对每个视点进行特征优化<br>Step3: Generate optimized viewpoints 生成优化视角<br>Output: Optimized viewpoints for projection-related PCQA 用于投影相关PCQA的优化视角 |
8.5 | [[8.5] 2502.11726 No-reference geometry quality assessment for colorless point clouds via list-wise rank learning](https://arxiv.org/abs/2502.11726) <br> [{'name': 'Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su'}] | Geometry Quality Assessment 几何质量评估 | v2<br>geometry quality assessment<br>point clouds<br>3D reconstruction | Input: Colorless point clouds 颜色点云<br>Step1: Construct LRL dataset 生成 LRL 数据集<br>Step2: Design GQANet to extract geometric features 设计 GQANet 提取几何特征<br>Step3: Use LRLNet for ranking the quality of point clouds 使用 LRLNet 对点云品质进行排序<br>Output: Predicted geometry quality index 预测的几何质量指数 |
8.5 | [[8.5] 2502.11742 Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition](https://arxiv.org/abs/2502.11742) <br> [{'name': 'Jianyi Peng, Fan Lu, Bin Li, Yuan Huang, Sanqing Qu, Guang Chen'}] | Visual Place Recognition 视觉地点识别 | v2<br>Visual Place Recognition<br>Cross-modal<br>RGB images<br>LiDAR<br>Bird's Eye View | Input: RGB images and LiDAR point clouds<br>Step1: Initial retrieval using global descriptor similarity<br>Step2: Re-ranking based on Bird's Eye View (BEV) images<br>Output: Improved Visual Place Recognition results |
8.5 | [[8.5] 2502.11864 Does Knowledge About Perceptual Uncertainty Help an Agent in Automated Driving?](https://arxiv.org/abs/2502.11864) <br> [{'name': 'Natalie Grabowsky, Annika M\\"utze, Joshua Wendland, Nils Jansen, Matthias Rottmann'}] | Autonomous Driving 自动驾驶 | v2<br>Perceptual Uncertainty<br>Reinforcement Learning<br>Automated Driving | Input: Perturbed observation space 观察空间<br>Step1: Introduce uncertainty 引入不确定性<br>Step2: Inform agent of uncertainty 通知代理不确定性<br>Step3: Reward agent for navigating safely 奖励代理安全导航<br>Output: Adjusted behavior with uncertainty 根据不确定性调整行为 |
8.5 | [[8.5] 2502.11971 Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance](https://arxiv.org/abs/2502.11971) <br> [{'name': 'Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, Jian Yang'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>6DoF pose tracking<br>augmented reality<br>contour-based methods<br>object tracking<br>intelligent manufacturing | Input: 6DoF object poses 6DoF 物体姿态<br>Step1: Robust contour-based tracking 方法 提出了一种基于轮廓的跟踪<br>Step2: CPU-only strategy for symmetric objects 针对对称物体的CPU仅策略<br>Step3: Unified energy function formulation 统一能量函数的表述<br>Output: Accurate tracking and assembly guidance 精确的跟踪和装配指导 |
8.5 | [[8.5] 2502.12151 VoLUT: Efficient Volumetric streaming enhanced by LUT-based super-resolution](https://arxiv.org/abs/2502.12151) <br> [{'name': 'Chendong Wang, Anlan Zhang, Yifan Yang, Lili Qiu, Yuqing Yang, Xinyang Jiang, Feng Qian, Suman Banerjee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D volumetric video<br>super-resolution<br>bandwidth reduction<br>lookup tables (LUTs) | Input: Low-resolution volumetric data 低分辨率体积数据<br>Step1: Downsampling data to reduce bandwidth 数据下采样以减少带宽<br>Step2: Applying super-resolution algorithm to upscale data 应用超分辨率算法对数据进行上采样<br>Step3: Utilizing lookup tables (LUTs) for efficient processing 使用查找表 (LUTs) 进行高效处理<br>Output: Enhanced volumetric video for streaming 改进的体积视频用于流传输 |
7.8 | [[7.8] 2502.10444 A Survey of Representation Learning, Optimization Strategies, and Applications for Omnidirectional Vision](https://arxiv.org/abs/2502.10444) <br> [{'name': 'Hao Ai, Zidong Cao, Lin Wang'}] | 3D Geometry and Motion Estimation 3D几何与运动估计 | v2<br>Omnidirectional vision<br>Deep learning<br>3D geometry<br>Autonomous driving | Input: Omnidirectional images 全景图像<br>Step 1: Literature review 文献综述<br>Step 2: Challenges and complexities analysis 挑战与复杂性分析<br>Step 3: Taxonomy development 分类法开发<br>Objective: Summarize DL methods for omnidirectional vision 总结全景视觉的深度学习方法 |
7.5 | [[7.5] 2502.12095 Descriminative-Generative Custom Tokens for Vision-Language Models](https://arxiv.org/abs/2502.12095) <br> [{'name': 'Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, Stefano Soatto'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>image retrieval<br>custom tokens | Input: Concept images and text 描述概念的图像和文本<br>Step1: Learn custom tokens 学习自定义token<br>Step2: Align text and image features 对齐文本和图像特征<br>Step3: Use in VLMs 应用于视觉语言模型<br>Output: Improved query performance 改进的查询性能 |


## Arxiv 2025-02-17

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.2 | [[9.2] 2502.09672 IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter](https://arxiv.org/abs/2502.09672) <br> [{'name': 'Xiaohong Liu, Xulong Zhao, Gang Liu, Zili Wu, Tao Wang, Lei Meng, Yuhan Wang'}] | 3D Multi-Object Tracking 3D多目标跟踪 | v2<br>3D Multi-Object Tracking<br>Interacting Multiple Model filter<br>3D point clouds | Input: 3D point clouds and images 3D点云和图像<br>Step1: Damping Window mechanism for trajectory management 轨迹管理的阻尼窗口机制<br>Step2: Interacting Multiple Model filter for dynamic tracking 动态跟踪的交互多个模型滤波器<br>Step3: Distance-Based Score Enhancement for detection scores 检测分数的基于距离的增强<br>Output: Enhanced 3D multi-object tracking system 改进的3D多目标跟踪系统 |
9.0 | [[9.0] 2502.09980 V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models](https://arxiv.org/abs/2502.09980) <br> [{'name': 'Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen'}] | Autonomous Driving 自动驾驶 | v2<br>Autonomous Driving<br>Cooperative Perception<br>Large Language Models | Input: Perception information from multiple CAVs 从多个CAV获取感知信息<br>Step1: Data integration 数据集成<br>Step2: LLM-based fusion 方法：基于LLM的特征融合<br>Step3: Question answering 问题回答<br>Output: Driving-related answers 驾驶相关答案 |
8.5 | [[8.5] 2502.09652 GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing](https://arxiv.org/abs/2502.09652) <br> [{'name': 'Lei (Rachel),  Chen, Juheon Lee, Juan Carlos Catana, Tsegai Yhdego, Nathan Moroney, Mohammad Amin Nabian, Hui Wang, Jun Zeng'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D printing 3D 打印<br>shape deviation 形状偏差<br>additive manufacturing 增材制造 | Input: Point cloud data 点云数据<br>Step1: Integrate positional factors 集成位置因素<br>Step2: Develop compensation algorithms 开发补偿算法<br>Step3: Validate and refine with experimental data 验证和完善实验数据<br>Output: Enhanced shape accuracy 改进的形状精度 |
8.5 | [[8.5] 2502.09669 Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation](https://arxiv.org/abs/2502.09669) <br> [{'name': 'Maizhe Yang, Kaiyuan Tang, Chaoli Wang'}] | Volumetric Reconstruction 体积重建 | v2<br>implicit neural representation<br>volumetric data<br>meta-learning<br>3D reconstruction<br>volume rendering | Input: Volumetric dataset 体积数据集<br>Step1: Meta-pretraining on subsampled data 亚采样数据上的元预训练<br>Step2: Volume-specific finetuning on complete data 对完整数据的卷特定微调<br>Output: Adapted implicit neural representations (INRs) 调整后的隐式神经表征 |
8.5 | [[8.5] 2502.09795 Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions](https://arxiv.org/abs/2502.09795) <br> [{'name': 'Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis'}] | Autonomous Systems and Robotics 自动驾驶机器人 | v2<br>Map-based Localization<br>Mars<br>image registration<br>deep learning | Input: Onboard images and reference map<br>Step1: Development of Geo-LoFTR model<br>Step2: Incorporation of geometric context<br>Step3: Simulation of Martian terrain<br>Output: Enhanced localization accuracy |
8.5 | [[8.5] 2502.10028 ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation](https://arxiv.org/abs/2502.10028) <br> [{'name': 'Yuxin He, Qiang Nie'}] | 3D Flow and Action Prediction 3D流和动作预测 | v2<br>3D flow<br>action prediction<br>robotic manipulation | Input: Language instructions and video data 语言指令和视频数据<br>Step 1: 3D flow prediction 3D流预测<br>Step 2: Model training using causal transformer 使用因果变换器训练模型<br>Output: Fine-grained action predictions and future image generation 输出: 精细的动作预测和未来图像生成 |
8.5 | [[8.5] 2502.10059 RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control](https://arxiv.org/abs/2502.10059) <br> [{'name': 'Teng Li, Guangcong Zheng, Rui Jiang,  Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>image-to-video generation<br>3D scene reconstruction<br>camera control<br>depth estimation | Input: Monocular images 单目图像<br>Step1: Depth estimation 深度估计<br>Step2: 3D scene reconstruction 3D场景重建<br>Step3: Camera trajectory scaling 相机轨迹缩放<br>Output: Interactive video generation 交互式视频生成 |
8.5 | [[8.5] 2502.10127 Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation](https://arxiv.org/abs/2502.10127) <br> [{'name': 'Gamal Elghazaly, Raphael Frank'}] | Autonomous Driving 自动驾驶 | v2<br>Collaboration<br>HD maps<br>V2X<br>Scene Graph Generation | Input: Front-facing camera images 前视相机图像<br>Step1: Extract lane centerlines from images 从图像中提取车道中心线<br>Step2: Represent lane centerlines as directed graphs 将车道中心线表示为有向图<br>Step3: Transmit data to the cloud via V2X 通过V2X将数据传输到云端<br>Output: Generated localized HD map 生成的局部高清地图 |
8.5 | [[8.5] 2502.10377 ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences](https://arxiv.org/abs/2502.10377) <br> [{'name': 'Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni'}] | 3D Generation 三维生成 | v2<br>3D reconstruction<br>style transfer<br>multi-view consistency | Input: Multi-view images 多视角图像<br>Step1: Style transfer to a single view using semantic attention mechanism 在单视图上使用语义注意机制进行风格转移<br>Step2: Lift stylization to additional views using warp-and-refine network 通过变换和细化网络将风格提升到其他视图<br>Output: Consistent stylized results across multiple views 在多个视图中获得一致的风格化结果 |
8.5 | [[8.5] 2502.10392 Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding](https://arxiv.org/abs/2502.10392) <br> [{'name': 'Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu'}] | 3D Visual Grounding 3D视觉定位 | v2<br>3D visual grounding 3D视觉定位<br>sparse convolution 稀疏卷积<br>text features 文本特征 | Input: 3D scene representation and text features 3D场景表示和文本特征<br>Step1: Text-guided pruning to sparsify the 3D voxel features 文本引导的修剪以减少3D体素特征<br>Step2: Completion-based addition to address over-pruned areas 基于补全的添加以解决过度修剪区域<br>Output: Efficiently fused features for 3D visual grounding 高效融合的特征用于3D视觉定位 |
8.0 | [[8.0] 2502.10273 Probing Perceptual Constancy in Large Vision Language Models](https://arxiv.org/abs/2502.10273) <br> [{'name': 'Haoran Sun, Suyang Yu, Yijiang Li, Qingying Gao, Haiyun Lyu, Hokin Deng, Dezhi Luo'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>perceptual constancy<br>vision-language models<br>VLMs<br>cognitive tasks | Input: Vision-Language Models (VLMs) 视觉语言模型<br>Step1: Evaluation using cognitive experiments 使用认知实验进行评估<br>Step2: Testing across dimensions of perceptual constancy 在感知恒常性的各个维度进行测试<br>Step3: Analysis of model variability in performance 对模型性能的变异性进行分析<br>Output: Insights into perceptual constancy capabilities of VLMs 输出: 对VLMs感知恒常性能力的洞察 |
7.5 | [[7.5] 2502.09818 On the robustness of multimodal language model towards distractions](https://arxiv.org/abs/2502.09818) <br> [{'name': 'Ming Liu, Hao Chen, Jindong Wang, Wensheng Zhang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>Robustness of Models 模型鲁棒性 | Input: Vision-language models (VLMs) 视觉语言模型<br>Step1: Develop a benchmark 数据集开发<br>Step2: Introduce distractions in visual and textual inputs 输入中引入干扰<br>Step3: Evaluate model robustness 评估模型鲁棒性<br>Output: Insights on VLM performance 视觉语言模型性能洞察 |


## Arxiv 2025-02-14

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.08902 CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics for Metric 3D Shape Recovery](https://arxiv.org/abs/2502.08902) <br> [{'name': 'Chenghao Zhang, Lubin Fan, Shen Cao, Bojian Wu, Jieping Ye'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape recovery<br>depth estimation<br>camera calibration | Input: Single image 单幅图像<br>Step1: Depth estimation 深度估计<br>Step2: Camera intrinsics estimation 相机内参估计<br>Step3: Collaborative optimization 协同优化<br>Output: Metric 3D shape metric 3D 形状 |
9.5 | [[9.5] 2502.09111 DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior](https://arxiv.org/abs/2502.09111) <br> [{'name': 'Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang'}] | SLAM 同时定位与地图构建 | v2<br>SLAM<br>Neural Radiance Fields<br>3D Reconstruction<br>Gaussian Splatting | Input: RGB-D stream of frames RGB-D帧流<br>Step1: Camera pose and neural radiance fields optimization 相机位姿和神经辐射场优化<br>Step2: Initialize Gaussian primitives using implicit radiance fields based on sampled points 使用样本点的隐式辐射场初始化高斯原语<br>Step3: Implement local loop closure detection and bundle optimization 进行局部闭环检测和捆绑优化<br>Output: Enhanced Gaussian maps with improved tracking and mapping performance 输出：具有改进跟踪和映射性能的增强高斯地图 |
9.5 | [[9.5] 2502.09274 FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation](https://arxiv.org/abs/2502.09274) <br> [{'name': 'Bin Yang, Alexandru Paul Condurache'}] | 3D Scene Understanding 3D场景理解 | v2<br>3D scene understanding<br>LiDAR<br>semantic segmentation<br>autonomous driving | Input: LiDAR point clouds LiDAR点云<br>Step1: Redesign data representation 重新设计数据表示<br>Step2: Implement data augmentation 实施数据增强<br>Step3: Apply post-processing methods 应用后处理方法<br>Output: Enhanced semantic segmentation performance 提升的语义分割性能 |
9.5 | [[9.5] 2502.09278 ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View Gaussian Optimization](https://arxiv.org/abs/2502.09278) <br> [{'name': 'Onat \\c{S}ahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image-to-3D<br>mesh generation | Input: Multi-view images 多视角图像<br>Step1: Generate multi-view prior images 生成多视角先验图像<br>Step2: Use score distillation sampling (SDS) to guide view generation 使用得分蒸馏采样引导视图生成<br>Step3: Optimize rough shape and fine details 优化粗形状和细节<br>Output: View-consistent 3D mesh 视图一致的三维网格 |
9.5 | [[9.5] 2502.09425 A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone Scans with Deep Learning Based Methods Using Geometry and Morphometry Criteria](https://arxiv.org/abs/2502.09425) <br> [{'name': "\\'Alvaro Heredia-Lid\\'on, Alejandro Mo\\~nux-Bernal, Alejandro Gonz\\'alez, Luis M. Echeverry-Quiceno, Max Rubert, Aroa Casado, Mar\\'ia Esther Esteban, Mireia Andreu-Montoriol, Susanna Gallardo, Cristina Ruffo, Neus Mart\\'inez-Abad\\'ias, Xavier Sevillano"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D facial reconstruction<br>morphometric analysis<br>deep learning | Input: Smartphone-based 3D scans and deep learning models 智能手机3D扫描与深度学习模型<br>Step1: Data acquisition 数据采集<br>Step2: Morphometric shape analysis morphometric形状分析<br>Step3: Comparison with ground truth 比较真实模型<br>Output: Evaluation of global and local shape differences 输出：全球和局部形状差异的评估 |
9.5 | [[9.5] 2502.09563 Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction](https://arxiv.org/abs/2502.09563) <br> [{'name': 'Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction 三维重建<br>Camera Calibration 相机校准<br>Gaussian Splatting 高斯点云 | Input: Wide-angle images 广角图像<br>Step1: Optimize camera parameters 优化相机参数<br>Step2: Model lens distortion 建模镜头畸变<br>Step3: Use Gaussian representations 使用高斯表示<br>Step4: Resample with cubemap strategy 使用立方映射策略<br>Output: Accurate 3D scene reconstruction 准确的三维场景重建 |
9.5 | [[9.5] 2502.09613 Latent Radiance Fields with 3D-aware 2D Representations](https://arxiv.org/abs/2502.09613) <br> [{'name': 'Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>latent representations<br>photorealistic rendering | Input: 2D latent representations 2D 潜在表示<br>Step1: Enhance 3D consistency with correspondence-aware autoencoding 方法1: 使用对应感知自编码增强3D一致性<br>Step2: Lift 3D-aware representations into 3D space 方法2: 将3D感知表示提升至3D空间<br>Step3: Align VAE-Radiance Fields for image decoding 方法3: 对齐VAE-放射场以进行图像解码<br>Output: Photorealistic 3D reconstruction output 照片真实的3D重建输出 |
9.5 | [[9.5] 2502.09615 RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets](https://arxiv.org/abs/2502.09615) <br> [{'name': 'Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D assets<br>autoregressive modeling<br>automatic rigging | Input: 3D asset shapes 3D资产形状<br>Step1: Joint probabilistic generation 关节概率生成<br>Step2: Skeleton topology prediction 骨架拓扑预测<br>Step3: Skinning weights assignment 绑定权重分配<br>Output: Rigged 3D asset 装配好的3D资产 |
9.5 | [[9.5] 2502.09623 Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF Architectures](https://arxiv.org/abs/2502.09623) <br> [{'name': 'Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D representation<br>Graph Meta-Networks | Input: Neural Radiance Fields (NeRFs) 神经辐射场<br>Step1: Train a Graph Meta-Network 训练图元网络<br>Step2: Apply contrastive learning 施加对比学习<br>Step3: Perform classification and retrieval tasks 执行分类和检索任务<br>Output: Architecture-agnostic representations 架构无关表示 |
8.8 | [[8.8] 2502.09620 Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620) <br> [{'name': 'Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D LMMs<br>Encoder-free architectures 3D LMMs<br>3D understanding 3D 理解 | Input: 3D point clouds 3D 点云<br>Step1: Semantic Encoding in pre-training 阶段的语义编码<br>Step2: Hierarchical Geometry Aggregation in tuning 调优中的层次几何聚合<br>Output: Encoder-free 3D LMM 编码器自由 3D LMM |
8.5 | [[8.5] 2502.08884 ShapeLib: designing a library of procedural 3D shape abstractions with Large Language Models](https://arxiv.org/abs/2502.08884) <br> [{'name': 'R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape representation<br>procedural modeling<br>Large Language Models | Input: Design intent (text descriptions, seed shapes) 设计意图（文本描述，种子形状）<br>Step1: Library interface design 库接口设计<br>Step2: Function application proposing 函数应用提出<br>Step3: Function implementation formulation 函数实现制定<br>Step4: Geometric validation of functions 几何验证函数<br>Output: Library of procedural shape functions 程序化形状函数库 |
8.5 | [[8.5] 2502.08974 Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning](https://arxiv.org/abs/2502.08974) <br> [{'name': 'Yiming Yang, Yueru Luo, Bingkun He, Erlong Li, Zhipeng Cao, Chao Zheng, Shuqi Mei, Zhen Li'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>lane topology<br>autonomous driving<br>topology reasoning | Input: Perspective views (PV) from cameras<br>Step1: Extract lane topology sequences from PV<br>Step2: Implement dual-decoder architecture for segment and topology decoding<br>Step3: Utilize randomized order prompt-to-sequence learning<br>Output: Enhanced lane topology sequences for autonomous driving |
8.5 | [[8.5] 2502.08977 Text-driven 3D Human Generation via Contrastive Preference Optimization](https://arxiv.org/abs/2502.08977) <br> [{'name': 'Pengfei Zhou, Xukun Shen, Yong Hu'}] | 3D Generation 三维生成 | v2<br>3D human generation<br>text-driven<br>contrastive preferences | Input: Textual descriptions 文本描述<br>Step1: Preference optimization module 偏好优化模块<br>Step2: Integration of multiple preference models 多个偏好模型的集成<br>Step3: Negation preference module 引入否定偏好模块<br>Output: Enhanced 3D human models 改进的三维人类模型 |
8.5 | [[8.5] 2502.09039 Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting](https://arxiv.org/abs/2502.09039) <br> [{'name': 'Lingting Zhu, Guying Lin, Jinnan Chen, Xinjie Zhang, Zhenchao Jin, Zhao Wang, Lequan Yu'}] | 3D Reconstruction and Modeling 三维重建 | Gaussian Splatting<br>3D reconstruction<br>image representation | Input: Large images 大图像<br>Step1: Gaussian point fitting 高斯点拟合<br>Step2: Optimization strategy 优化策略<br>Step3: Level-of-Gaussian reconstruction 高斯层次重建<br>Output: High-quality image representations 高质量图像表示 |
8.5 | [[8.5] 2502.09057 Vision-Language In-Context Learning Driven Few-Shot Visual Inspection Model](https://arxiv.org/abs/2502.09057) <br> [{'name': 'Shiryu Ueno, Yoshikazu Hayashi, Shunsuke Nakatsuka, Yusei Yamada, Hiroaki Aizawa, Kunihito Kato'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Inspection<br>Vision-Language Model<br>In-Context Learning | Input: Few-shot images of products 产品的少量图像<br>Step1: Construct dataset 创建数据集<br>Step2: Fine-tune VLM for inspection 对VLM进行微调以进行检查<br>Step3: Perform visual inspection using In-Context Learning 使用In-Context Learning进行视觉检查<br>Output: Inspection results and defective location detection 检查结果及缺陷位置检测 |
8.5 | [[8.5] 2502.09080 BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization](https://arxiv.org/abs/2502.09080) <br> [{'name': 'Qiwei Wang, Shaoxun Wu, Yujiao Shi'}] | Cross-View Localization 跨视角定位 | v2<br>3D Gaussian primitives<br>cross-view localization<br>autonomous driving | Input: Ground image and satellite image 地面图像与卫星图像<br>Step1: Generate 3D Gaussian primitives 生成三维高斯原语<br>Step2: Synthesize BEV feature map 合成鸟瞩视图特征图<br>Step3: Conduct pose estimation 进行姿态估计<br>Output: Location probability map of the query image 查询图像的位置信息图 |
8.5 | [[8.5] 2502.09528 SteROI-D: System Design and Mapping for Stereo Depth Inference on Regions of Interest](https://arxiv.org/abs/2502.09528) <br> [{'name': 'Jack Erhardt, Ziang Li, Reid Pinkham, Andrew Berkovich, Zhengya Zhang'}] | Multi-view Stereo 多视角立体 | v2<br>Stereo Depth<br>Region of Interest<br>Energy Efficiency<br>AR/VR<br>Dynamic ROIs | Input: Stereo images 立体图像<br>Step1: ROI identification ROI识别<br>Step2: Depth estimation depth estimation<br>Step3: Energy optimization 能耗优化<br>Output: Efficient depth maps 高效深度图 |
8.5 | [[8.5] 2502.09617 LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh](https://arxiv.org/abs/2502.09617) <br> [{'name': 'Jing Wen, Alexander G. Schwing, Shenlong Wang'}] | Neural Rendering 神经渲染 | v2<br>3D reconstruction<br>human rendering<br>computational efficiency | Input: Sparse source images稀疏源图像<br>Step1: Iterative feedback update iterative feedback update<br>Step2: Coupled multi-resolution Gaussians-on-Mesh representation耦合多分辨率高斯-网格表示<br>Output: Animatable human representation可动画人类表示 |
7.5 | [[7.5] 2502.09075 PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration](https://arxiv.org/abs/2502.09075) <br> [{'name': 'Jinhui Guo, Lubin Fan, Bojian Wu, Jiaqi Gu, Shen Cao, Jieping Ye'}] | Camera Calibration 相机校准 | v2<br>PTZ calibration<br>camera parameters<br>3D information | Input: Reference images 参考图像<br>Step1: Image selection 图像选择<br>Step2: Apply PTZ-IBA algorithm 应用PTZ增量束调整算法<br>Step3: Parameter optimization 参数优化<br>Output: Calibrated camera parameters 校准的相机参数 |
7.5 | [[7.5] 2502.09088 Unsupervised Anomaly Detection on Implicit Shape representations for Sarcopenia Detection](https://arxiv.org/abs/2502.09088) <br> [{'name': 'Louise Piecuch (MD), Jeremie Huet (MD), Antoine Frouin (PT), Antoine Nordez (MD), Anne-Sophie Boureau (MD), Diana Mateus'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anomaly detection<br>implicit neural representation<br>sarcopenia | Input: Muscle shape data 肌肉形状数据<br>Step1: Model normal muscle shapes using implicit neural representation (INR) 使用隐式神经表征建模正常肌肉形状<br>Step2: Employ unsupervised anomaly detection based on reconstruction error 使用基于重建误差的无监督异常检测<br>Step3: Classify and separate normal and sarcopenic muscles from learned representations 对学习的表示进行分类和分离正常与肌肉萎缩肌肉<br>Output: Anomaly detection results for sarcopenic and non-sarcopenic muscles 输出：肌肉萎缩及非肌肉萎缩的异常检测结果 |


## Arxiv 2025-02-13

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.07822 PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation](https://arxiv.org/abs/2502.07822) <br> [{'name': 'Ao Liang, Haiyang Hua, Jian Fang, Wenyu Chen, Huaici Zhao'}] | 3D Object Detection 三维物体检测 | v2<br>3D object detection<br>Point Dilation Mechanism<br>autonomous driving | Input: Point cloud data 点云数据<br>Step1: Efficient feature encoding using PointNet-style backbone 使用PointNet风格的骨干网进行高效特征编码<br>Step2: Point Dilation Mechanism (PDM) to expand feature space 使用点膨胀机制（PDM）扩展特征空间<br>Step3: Hybrid detection head for joint learning 设计混合检测头进行联合学习<br>Output: Enhanced 3D object detection results 改进的三维物体检测结果 |
9.5 | [[9.5] 2502.07840 TranSplat: Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation](https://arxiv.org/abs/2502.07840) <br> [{'name': 'Jeongyun Kim, Jeongho Noh, Dong-Guw Lee, Ayoung Kim'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>transparent object manipulation<br>depth completion<br>latent diffusion model<br>robotics | Input: RGB images and surface embeddings RGB图像和表面嵌入<br>Step1: Generate surface embeddings using a latent diffusion model 使用潜在扩散模型生成表面嵌入<br>Step2: Jointly optimize Gaussian splatting with RGB images and surface embeddings 与RGB图像和表面嵌入共同优化高斯点云<br>Step3: Render depth for object manipulation 渲染深度以进行物体操作<br>Output: Accurate depth completion for transparent objects 为透明物体提供准确的深度完成 |
9.5 | [[9.5] 2502.07869 EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera](https://arxiv.org/abs/2502.07869) <br> [{'name': 'Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Alain Pagani, Didier Stricker, Christian Theobalt, Vladislav Golyanik'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human motion capture<br>event cameras<br>egocentric vision | Input: Monocular event camera with fisheye lens 单眼事件相机与鱼眼镜头<br>Step1: Data acquisition from event camera 数据采集<br>Step2: Integration of RGB and event data RGB与事件数据集成<br>Step3: Algorithm development for pose estimation 算法开发以估计姿势<br>Step4: Real-time processing and 3D reconstruction 实时处理与三维重建<br>Output: Accurate 3D human motion capture 精确的三维人类运动捕捉 |
9.5 | [[9.5] 2502.08169 CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus](https://arxiv.org/abs/2502.08169) <br> [{'name': 'Yunjiang Xu, Lingzhi Li, Jin Wang, Benyuan Yang, Zhiwen Wu, Xinhong Chen, Jianping Wang'}] | 3D Object Detection 三维物体检测 | v2<br>3D detection 三维检测<br>autonomous driving 自动驾驶<br>collaborative perception 协同感知 | Input: Sensor data from LiDAR and cameras 传感器数据来自LiDAR和相机<br>Step1: Evaluate dynamic feature trust modulus (DFTM) 评估动态特征信任模数 (DFTM)<br>Step2: Implement multi-scale fusion method 实现多尺度融合方法<br>Step3: Validate performance through extensive experiments 通过广泛实验验证性能<br>Output: Enhanced robustness in 3D object detection 提高三维物体检测的鲁棒性 |
9.5 | [[9.5] 2502.08285 Fully-Geometric Cross-Attention for Point Cloud Registration](https://arxiv.org/abs/2502.08285) <br> [{'name': 'Weijie Wang, Guofeng Mei, Jian Zhang, Nicu Sebe, Bruno Lepri, Fabio Poiesi'}] | 3D Reconstruction 三维重建 | v2<br>Point Cloud Registration 点云配准<br>Geometric Attention 几何注意力<br>Transformer Network 变换网络 | Input: Point clouds 输入: 点云<br>Step1: Cross-attention mechanism development 步骤1: 交叉注意力机制开发<br>Step2: Integration of Gromov-Wasserstein distance into attention 步骤2: 将Gromov-Wasserstein距离集成到注意力机制中<br>Step3: Point feature aggregation through self-attention 步骤3: 通过自注意力聚合点特征<br>Output: Enhanced point cloud registration results 输出: 改进的点云配准结果 |
9.5 | [[9.5] 2502.08352 Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images with Depth and Normal Supervision](https://arxiv.org/abs/2502.08352) <br> [{'name': 'Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>satellite imagery<br>neural networks | Input: Multi-view satellite images 多视角卫星图像<br>Step1: Incorporate explicit depth guidance 引入显式深度指导<br>Step2: Apply surface normal consistency constraints 应用表面法线一致性约束<br>Step3: Utilize a multi-resolution hash grid for efficient reconstruction 使用多分辨率哈希网格进行高效重建<br>Output: Accurate 3D models from satellite images 从卫星图像获得精准的三维模型 |
8.5 | [[8.5] 2502.07829 Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing](https://arxiv.org/abs/2502.07829) <br> [{'name': 'Sihao Wu, Xiaonan Si, Chi Xing, Jianhong Wang, Gaojie Jin, Guangliang Cheng, Lijun Zhang, Xiaowei Huang'}] | Image Generation 图像生成 | v2<br>diffusion models<br>image generation<br>preference alignment<br>autonomous driving | Input: Integration of preference alignment with diffusion models 偏好对齐与扩散模型的结合<br>Step1: Systematic review of optimization techniques 对优化技术进行系统回顾<br>Step2: Exploration of applications across various fields 在多个领域探索应用<br>Step3: Discussion of challenges in preference alignment 讨论偏好对齐中的挑战<br>Output: Insights for future innovation 未来创新的洞察 |
8.5 | [[8.5] 2502.08377 Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features](https://arxiv.org/abs/2502.08377) <br> [{'name': 'Liying Yang, Chen Liu, Zhenwei Zhu, Ajian Liu, Hui Ma, Jian Nong, Yanyan Liang'}] | 3D Generation 三维生成 | v2<br>4D generation<br>dynamic-static features<br>computer vision | Input: Video frames 视频帧<br>Step1: Feature extraction 特征提取<br>Step2: Dynamic-static feature decoupling 动态静态特征解耦<br>Step3: Temporal-spatial similarity fusion 在时间-空间上选择相似特征<br>Output: 4D content generation 4D内容生成 |
8.5 | [[8.5] 2502.08639 CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) <br> [{'name': 'Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai'}] | Image and Video Generation 图像生成 | v2<br>3D-aware<br>text-to-video generation<br>depth maps<br>camera trajectories | Input: User-defined scene parameters 用户定义的场景参数<br>Step1: Interactive workflow for 3D control 3D控制的交互工作流程<br>Step2: Condition signal construction 条件信号构建<br>Step3: Text-to-video generation from control signals 基于控制信号的文本生成视频<br>Output: Generated controllable video 输出: 生成的可控视频 |
8.0 | [[8.0] 2502.08374 AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception](https://arxiv.org/abs/2502.08374) <br> [{'name': 'Yuanhao Huang, Qinfan Zhang, Jiandong Xing, Mengyue Cheng, Haiyang Yu, Yilong Ren, Xiao Xiong'}] | Autonomous Driving 自动驾驶 | v2<br>adversarial attack<br>autonomous driving<br>information swapping | Input: Autonomous vehicle images 自动驾驶车辆图像<br>Step1: Information swapping 信息交换<br>Step2: Adversarial sample generation 对抗样本生成<br>Step3: Evaluation on datasets 在数据集上评估<br>Output: Robust adversarial samples 稳健的对抗样本 |
7.5 | [[7.5] 2502.08646 Poly-Autoregressive Prediction for Modeling Interactions](https://arxiv.org/abs/2502.08646) <br> [{'name': 'Neerja Thakkar, Tara Sadjadpour, Jathushan Rajasegaran, Shiry Ginosar, Jitendra Malik'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous vehicles<br>trajectory prediction<br>multi-agent interactions<br>behavior forecasting | Input: Ego agent's state history and states of other interacting agents 自我代理的状态历史和其他交互代理的状态<br>Step1: Model behavior as a sequence of tokens 将行为建模为状态序列<br>Step2: Use a transformer for prediction 使用变压器进行预测<br>Step3: Apply to different prediction tasks 应用到不同的预测任务<br>Output: Predicted future behavior of the ego agent 输出自我代理的未来行为预测 |
6.5 | [[6.5] 2502.07838 NanoVLMs: How small can we go and still make coherent Vision Language Models?](https://arxiv.org/abs/2502.07838) <br> [{'name': 'Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>lightweight models | Input: Image-text pairs 图像-文本对<br>Step1: Dataset creation 数据集创建<br>Step2: Model training 模型训练<br>Step3: Evaluation using creative scoring 通过创意评分进行评估<br>Output: Lightweight vision-language models 轻量级视觉语言模型 |


## Arxiv 2025-02-12

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.07140 Few-Shot Multi-Human Neural Rendering Using Geometry Constraints](https://arxiv.org/abs/2502.07140) <br> [{'name': 'Qian li, Victoria Fern\\`andez Abrevaya, Franck Multon, Adnane Boukhayma'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-human scenes<br>neural rendering | Input: Sparse multi-view images 稀疏多视角图像<br>Step1: Geometry constraints using SMPL meshes 使用SMPL网格的几何约束<br>Step2: Regularize signed distances for optimization 通过正则化签名距离进行优化<br>Step3: Apply ray and saturation regularization 应用射线和饱和度正则化<br>Output: Accurate multi-human 3D reconstructions and renderings 准确的多人的三维重建和渲染 |
9.5 | [[9.5] 2502.07278 Articulate That Object Part (ATOP): 3D Part Articulation from Text and Motion Personalization](https://arxiv.org/abs/2502.07278) <br> [{'name': 'Aditya Vora, Sauradip Nag, Hao Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D articulation<br>motion personalization<br>video diffusion | Input: Segmented mesh and text prompt 输入：分割网格和文本提示<br>Step1: Few-shot finetuning for category-specific motion generation 第一步：针对特定类别的运动生成进行少量样本微调<br>Step2: Multi-view rendering to generate personalized motion video 第二步：多视角渲染生成个性化运动视频<br>Step3: Differentiable rendering for transferring motion to the 3D object 第三步：可微渲染将运动转移到三维对象<br>Output: Articulated 3D object with realistic motion 输出：具有真实运动的关节三维对象 |
9.5 | [[9.5] 2502.07289 Learning Inverse Laplacian Pyramid for Progressive Depth Completion](https://arxiv.org/abs/2502.07289) <br> [{'name': 'Kun Wang, Zhiqiang Yan, Junkai Fan, Jun Li, Jian Yang'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D reconstruction<br>state-of-the-art | Input: Sparse depth measurements and corresponding color image  稀疏深度测量和相应的彩色图像<br>Step 1: Initial low-resolution depth prediction  初步低分辨率深度预测<br>Step 2: Multi-path feature extraction via MFP module 通过MFP模块进行多路径特征提取<br>Step 3: Depth map refinement through upsampling and selective filtering 通过上采样和选择性过滤进行深度图优化<br>Output: Dense depth map  稠密深度图 |
9.5 | [[9.5] 2502.07309 Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2502.07309) <br> [{'name': 'Xiang Li, Pengfei Li, Yupeng Zheng, Wei Sun, Yan Wang, Yilun Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D occupancy modeling 3D占用建模<br>autonomous driving 自动驾驶<br>scene understanding 场景理解 | Input: Multi-view images 多视角图像<br>Step1: Self-supervised pre-training with 2D labels 自监督预训练以2D标签<br>Step2: Fully-supervised fine-tuning with 3D occupancy labels 全监督微调以3D占用标签<br>Step3: State-conditioned forecasting module for future occupancy 未来占用状态条件预测模块<br>Output: 3D occupancy predictions 3D占用预测 |
9.5 | [[9.5] 2502.07403 Extended monocular 3D imaging](https://arxiv.org/abs/2502.07403) <br> [{'name': 'Zicheng Shen, Feng Zhao, Yibo Ni, Yuanmu Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D imaging 3D成像<br>monocular vision 单目视觉<br>depth estimation 深度估计<br>material identification 材料识别 | Input: Monocular camera with diffractive-refractive hybrid lens 使用具备衍射-折射混合透镜的单目相机<br>Step1: Multi-stage fusion of depth cues 深度线索的多级融合<br>Step2: Snapshot acquisition of 3D point cloud 3D点云的快照获取<br>Step3: Accurate 3D reconstruction 精确的3D重建<br>Output: Enhanced 3D imaging capabilities 改进的3D成像能力 |
9.5 | [[9.5] 2502.07505 Efficient Continuous Group Convolutions for Local SE(3) Equivariance in 3D Point Clouds](https://arxiv.org/abs/2502.07505) <br> [{'name': 'Lisa Weijler, Pedro Hermosilla'}] | Point Cloud Processing 点云处理 | v2<br>3D point clouds 3D点云<br>equivariance 等变性 | Input: 3D point clouds 3D点云<br>Step1: Define Local Reference Frame (LRF) 定义局部参考框架<br>Step2: Implement continuous SE(3) equivariant convolution 实现连续SE(3)等变卷积<br>Step3: Train the model with stochastically sampled frames 用随机采样的框架训练模型<br>Output: Local rotation equivariant features 输出局部旋转等变特征 |
9.5 | [[9.5] 2502.07615 Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors](https://arxiv.org/abs/2502.07615) <br> [{'name': 'Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Siyu Zhu, Zhihao Li, Xun Cao, Yao Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>mesh reconstruction<br>geometry reconstruction | Input: 3D Gaussian Splatting images 3D高斯点云图像<br>Step1: Incorporate pre-trained matching prior 引入预训练匹配先验<br>Step2: Implement Flow Distillation Sampling 算法流蒸馏抽样<br>Step3: Target unobserved views 目标未观察视图<br>Output: Enhanced geometric reconstruction 改进的几何重建 |
9.5 | [[9.5] 2502.07685 Matrix3D: Large Photogrammetry Model All-in-One](https://arxiv.org/abs/2502.07685) <br> [{'name': 'Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>photogrammetry<br>depth estimation<br>pose estimation<br>novel view synthesis | Input: Multi-modal data (images, camera parameters, depth maps) 图像、相机参数和深度图的多模态数据<br>Step 1: Masked input learning 掩码输入学习<br>Step 2: Pose estimation 位置估计<br>Step 3: Depth prediction 深度预测<br>Step 4: Novel view synthesis 新视图合成<br>Output: Comprehensive 3D model 综合三维模型 |
9.0 | [[9.0] 2502.07030 PrismAvatar: Real-time animated 3D neural head avatars on edge devices](https://arxiv.org/abs/2502.07030) <br> [{'name': 'Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D avatar<br>neural rendering<br>real-time animation<br>head modeling | Input: Series of matted images of a head 头部图像序列<br>Step1: Data acquisition and tracking 数据采集与跟踪<br>Step2: Train hybrid mesh-volumetric model 训练混合网格-体积模型<br>Step3: Distillation into rigged mesh and neural textures 蒸馏成具有骨架的网格和神经纹理<br>Output: Real-time animated 3D head avatar 实时动画3D头像 |
8.5 | [[8.5] 2502.06843 Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation](https://arxiv.org/abs/2502.06843) <br> [{'name': 'Namhee Kim, Woojin Park'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>large language models<br>computer vision | Input: Visual inputs and scenarios 视觉输入与场景<br>Step1: Feature extraction using YOLOv4 and ViT 使用YOLOv4和ViT进行特征提取<br>Step2: Integration with LLM for reasoning 与LLM结合进行推理<br>Step3: Generation of situation descriptions and responses 生成情境描述和适当反应<br>Output: Improved autonomous driving assistance system 改进的自动驾驶辅助系统 |
8.5 | [[8.5] 2502.06957 GAS: Generative Avatar Synthesis from a Single Image](https://arxiv.org/abs/2502.06957) <br> [{'name': 'Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>avatar generation<br>3D reconstruction<br>diffusion models | Input: A single image 单幅图像<br>Step1: 3D human reconstruction 人体三维重建<br>Step2: Dense driving signal generation 生成密集驱动信号<br>Step3: Video diffusion model application 应用视频扩散模型<br>Output: View-consistent and temporally coherent avatars 输出：视图一致且时间连贯的头像 |
8.5 | [[8.5] 2502.07001 From Image to Video: An Empirical Study of Diffusion Representations](https://arxiv.org/abs/2502.07001) <br> [{'name': "Pedro V\\'elez, Luisa F. Polan\\'ia, Yi Yang, Chuhan Zhang, Rishab Kabra, Anurag Arnab, Mehdi S. M. Sajjadi"}] | Image and Video Generation 图像生成与视频生成 | v2<br>diffusion models<br>video synthesis<br>image generation<br>depth estimation | Input: Video and image diffusion models 视频与图像扩散模型<br>Step1: Model architecture comparison 模型架构比较<br>Step2: Performance analysis of latent representations 潜在表示性能分析<br>Step3: Feature extraction and qualitative analysis 特征提取与定性分析<br>Output: Insights into representations and performance 表示与性能的见解 |
8.5 | [[8.5] 2502.07007 Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC](https://arxiv.org/abs/2502.07007) <br> [{'name': 'Siwei Meng, Yawei Luo, Ping Liu'}] | Image and Video Generation 图像生成与视频生成 | v2<br>3D generation<br>physics priors<br>AI-generated content<br>physical realism | Input: Generative models 生成模型<br>Step1: Review of physics-aware methods 物理感知方法的回顾<br>Step2: Categorization of generation techniques 生成技术的分类<br>Step3: Comparative analysis 比较分析<br>Output: Insights for future research 未来研究的洞见 |
8.5 | [[8.5] 2502.07120 Is Long Range Sequential Modeling Necessary For Colorectal Tumor Segmentation?](https://arxiv.org/abs/2502.07120) <br> [{'name': 'Abhishek Srivastava, Koushik Biswas, Gorkem Durak, Gulsah Ozden, Mustafa Adli, Ulas Bagci'}] | 3D Segmentation and Reconstruction 3D分割与重建 | v2<br>3D segmentation<br>tumor segmentation<br>colorectal cancer | Input: 3D medical images 3D医学影像<br>Step 1: Evaluate long-range and local token modeling mechanisms 评估长范围和局部标记建模机制<br>Step 2: Propose MambaOutUNet for tumor segmentation 提出MambaOutUNet用于肿瘤分割<br>Step 3: Analyze performance on the CTS-204 dataset 在CTS-204数据集上分析性能<br>Output: Comparative results on tumor segmentation techniques 输出：肿瘤分割技术的比较结果 |
8.5 | [[8.5] 2502.07145 Mesh2SSM++: A Probabilistic Framework for Unsupervised Learning of Statistical Shape Model of Anatomies from Surface Meshes](https://arxiv.org/abs/2502.07145) <br> [{'name': 'Krithika Iyer, Mokshagna Sai Teja Karanam, Shireen Elhabian'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Statistical Shape Modeling<br>Surface Meshes<br>Unsupervised Learning | Input: Surface meshes 表面网格<br>Step1: Estimate correspondences from meshes 估计来自网格的对应关系<br>Step2: Develop probabilistic shape model 开发概率形状模型<br>Step3: Evaluate model performance 评估模型性能<br>Output: Statistical shape model 统计形状模型 |
8.5 | [[8.5] 2502.07194 Dense Object Detection Based on De-homogenized Queries](https://arxiv.org/abs/2502.07194) <br> [{'name': 'Yueming Huang, Chenrui Ma, Hao Zhou, Hao Wu, Guowu Yuan'}] | Autonomous Driving 自动驾驶 | v2<br>dense object detection<br>autonomous driving<br>DETR<br>deep learning<br>computer vision | Input: Dense object detection scenario 密集目标检测场景<br>Step1: Identify issues with existing NMS methods 识别现有NMS方法的问题<br>Step2: Propose differentiated encoding for queries 提出差异化编码以应对查询<br>Step3: Implement joint loss for better query initialization 实施联合损失以更好地初始化查询<br>Output: Enhanced dense object detection framework 改进的密集目标检测框架 |
8.5 | [[8.5] 2502.07372 USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging under Multiple Adverse Weather Conditions](https://arxiv.org/abs/2502.07372) <br> [{'name': 'Yuxu Lu, Ai Chen, Dong Yang, Ryan Wen Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving<br>image restoration | Input: Degraded images 退化图像<br>Step1: Feature extraction 特征提取<br>Step2: Scene restoration 场景恢复<br>Step3: Edge feature extraction 边缘特征提取<br>Output: Enhanced image quality 改进的图像质量 |
8.5 | [[8.5] 2502.07417 Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized Attention Vision Transformer for Autonomous Driving](https://arxiv.org/abs/2502.07417) <br> [{'name': 'Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Wen-Kai Kuo, Jun-Wei Hsieh'}] | Autonomous Driving 自动驾驶 | v2<br>Object Detection 目标检测<br>Autonomous Driving 自动驾驶<br>Vision Transformer 视觉变换器 | Input: Driving scene images 驾驶场景图像<br>Step1: Analyze backbone architectures 分析主干架构<br>Step2: Develop reparameterized attention vision transformer 开发重参数化注意力视觉变换器<br>Step3: Integrate multi-scale feature extraction 集成多尺度特征提取<br>Step4: Model evaluation 模型评估<br>Output: High-performance object detection model 高性能目标检测模型 |
8.5 | [[8.5] 2502.07486 Automated Road Extraction and Centreline Fitting in LiDAR Point Clouds](https://arxiv.org/abs/2502.07486) <br> [{'name': 'Xinyu Wang, Muhammad Ibrahim, Atif Mansoor, Hasnein Tareque, Ajmal Mian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>road extraction<br>3D point clouds<br>LiDAR | Input: 3D LiDAR point clouds 3D LiDAR点云<br>Step 1: Statistical outlier removal 统计离群值去除<br>Step 2: Density-based clustering 基于密度的聚类<br>Step 3: Ground point filtering using grid-based segmentation 使用基于网格的分割进行地面点过滤<br>Step 4: 2D projection and skeletonization 2D投影和骨架化<br>Step 5: Back-projection onto 3D point cloud 反投影到3D点云<br>Output: Refined road points and centreline 提炼的道路点和中心线 |
8.5 | [[8.5] 2502.07631 Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous Driving](https://arxiv.org/abs/2502.07631) <br> [{'name': 'Yinzhe Shen, \\"Omer \\c{S}ahin Ta\\c{s}, Kaiwen Wang, Royden Wagner, Christoph Stiller'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>motion learning<br>semantic learning | Input: Camera data 摄像头数据<br>Step1: Motion and semantic task separation 任务分离<br>Step2: Neural-Bayes motion decoder 运动解码器<br>Step3: Interactive semantic decoder 交互式语义解码器<br>Output: Improved detection and tracking 改进的检测与跟踪 |
8.5 | [[8.5] 2502.07680 Multiview Point Cloud Registration Based on Minimum Potential Energy for Free-Form Blade Measurement](https://arxiv.org/abs/2502.07680) <br> [{'name': 'Zijie Wu, Yaonan Wang, Yang Mo, Qing Zhu, He Xie, Haotian Wu, Mingtao Feng, Ajmal Mian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>point cloud registration<br>noise resistance<br>industrial measurement | Input: Point cloud data 点云数据<br>Step1: Definition of objective function 目标函数定义<br>Step2: Global optimization procedure 全局优化过程<br>Step3: Fine registration using trimmed ICP 精细配准，使用修剪的ICP算法<br>Output: Registered point clouds 注册后的点云 |
8.5 | [[8.5] 2502.07785 Pippo: High-Resolution Multi-View Humans from a Single Image](https://arxiv.org/abs/2502.07785) <br> [{'name': 'Yash Kant, Ethan Weber, Jin Kyu Kim, Rawal Khirodkar, Su Zhaoen, Julieta Martinez, Igor Gilitschenski, Shunsuke Saito, Timur Bagautdinov'}] | 3D Generation 三维生成 | v2<br>3D consistency<br>multi-view generation<br>video generation | Input: Single image of a person 一个人的单张图像<br>Step1: Pre-training on human images 人体图像的预训练<br>Step2: Multi-view mid-training 多视角中期训练<br>Step3: Post-training with pixel-aligned controls 像素对齐控制的后期训练<br>Output: 1K resolution multi-view consistent images 1K分辨率的多视角一致图像 |
8.0 | [[8.0] 2502.07508 Enhance-A-Video: Better Generated Video for Free](https://arxiv.org/abs/2502.07508) <br> [{'name': 'Yang Luo, Xuanlei Zhao, Mengzhao Chen, Kaipeng Zhang, Wenqi Shao, Kai Wang, Zhangyang Wang, Yang You'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>temporal consistency<br>DiT-based models | Input: DiT-based video generation models  基于DiT的视频生成模型<br>Step1: Analyze temporal attention analysis 时序注意力分析<br>Step2: Introduce cross-frame intensity parameters 引入跨帧强度参数<br>Step3: Enhance video quality through adjusted dependencies 调整依赖关系以增强视频质量<br>Output: Enhanced video generation quality 提升的视频生成质量 |
8.0 | [[8.0] 2502.07564 An Elliptic Curve Based Solution to the Perspective-Three-Point Problem](https://arxiv.org/abs/2502.07564) <br> [{'name': 'Michael Q. Rieck'}] | Computer Vision and Pose Estimation 计算机视觉与位姿估计 | v2<br>P3P<br>camera pose<br>elliptic curves | Input: Control points 控制点<br>Step1: Determine directions of lines 计算直线方向<br>Step2: Develop P3P solver 开发P3P求解器<br>Step3: Compare with linear solvers 与线性求解器比较<br>Output: Accurate camera poses 准确的相机位姿 |
7.5 | [[7.5] 2502.07306 TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/abs/2502.07306) <br> [{'name': 'Navid Rajabi, Jana Kosecka'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Navigation<br>modular approach<br>navigation instruction | Input: Navigation instruction and environment map 导航指令和环境地图<br>Step1: Extract landmarks using LLM 提取地标<br>Step2: Retrieve top-k locations using shortest path algorithm 检索前k个位置，使用最短路径算法<br>Step3: Compute alignment score with dynamic programming 使用动态规划计算对齐评分<br>Output: Evaluate path fidelity using nDTW metric 输出：使用nDTW指标评估路径可信度 |
7.5 | [[7.5] 2502.07617 Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617) <br> [{'name': 'Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>cultural diversity<br>multilinguality | Input: 100 billion image-text pairs 1000亿图像-文本对<br>Step1: Empirical investigation 实证研究<br>Step2: Performance analysis 性能分析<br>Step3: Cultural diversity assessment 文化多样性评估<br>Output: Insights on VLM performance 视觉语言模型性能见解 |
7.5 | [[7.5] 2502.07701 Magic 1-For-1: Generating One Minute Video Clips within One Minute](https://arxiv.org/abs/2502.07701) <br> [{'name': 'Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>diffusion models<br>text-to-image<br>image-to-video | Input: Text and video data 文本和视频数据<br>Step1: Task factorization 任务分解<br>Step2: Generative prior injection 生成先验注入<br>Step3: Model optimization 模型优化<br>Output: Efficient video clips 生成高效视频片段 |
7.5 | [[7.5] 2502.07737 Next Block Prediction: Video Generation via Semi-Autoregressive Modeling](https://arxiv.org/abs/2502.07737) <br> [{'name': 'Shuhuai Ren, Shuming Ma, Xu Sun, Furu Wei'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>semi-autoregressive modeling | Input: Video data 视频数据<br>Step1: Block decomposition 块分解<br>Step2: Semi-autoregressive generation 半自回归生成<br>Step3: Bidirectional attention application 双向注意力应用<br>Output: Generated video frames 生成的视频帧 |


## Arxiv 2025-02-11

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.05222 VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic Resolution Management via Q-Learning](https://arxiv.org/abs/2502.05222) <br> [{'name': 'Jayram Palamadai, William Yu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D volumetric reconstruction 3D体积重建<br>dynamic resolution management 动态分辨率管理<br>photorealistic rendering 照相真实渲染 | Input: 2D photographs 二维照片<br>Step1: Image conversion to PlenOctree data structure 图像转换为PlenOctree数据结构<br>Step2: Dynamic resolution management using QuiQ 动态分辨率管理使用QuiQ<br>Step3: Synthesizing novel viewpoints using differentiable rendering 合成新视角使用可微渲染<br>Output: Interactive 3D volumetric images 互动三维体积图像 |
9.5 | [[9.5] 2502.05378 NextBestPath: Efficient 3D Mapping of Unseen Environments](https://arxiv.org/abs/2502.05378) <br> [{'name': "Shiyao Li, Antoine Gu\\'edon, Cl\\'ementin Boittiaux, Shizhe Chen, Vincent Lepetit"}] | 3D Mapping and Reconstruction 3D映射与重建 | v2<br>3D mapping<br>active mapping<br>robotics | Input: Unseen indoor environments 未知室内环境<br>Step1: Create and benchmark a new dataset (AiMDoom) 创建并基准新的数据集 (AiMDoom)<br>Step2: Develop the next-best-path method (NBP) 开发下一最佳路径方法 (NBP)<br>Step3: Plan and optimize trajectory for active mapping 规划和优化主动映射的轨迹<br>Output: Efficiently reconstructed 3D models 有效重建的三维模型 |
9.5 | [[9.5] 2502.05859 SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion](https://arxiv.org/abs/2502.05859) <br> [{'name': 'Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng'}] | Depth Estimation 深度估计 | v2<br>panorama depth estimation<br>3D reconstruction<br>autonomous driving | Input: Panorama images 全景图像<br>Step1: Feature extraction 特征提取<br>Step2: Feature fusion 特征融合<br>Step3: Depth estimation 深度估计<br>Output: Depth map and point cloud 深度图和点云 |
9.5 | [[9.5] 2502.05874 MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation](https://arxiv.org/abs/2502.05874) <br> [{'name': 'Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Xiangde Liu, Guangyao Zhai'}] | 3D Generation 三维生成 | v2<br>3D scene generation<br>geometry control<br>mixed-modality graph | Input: Mixed-Modality Graph combining textual and visual modalities<br>Step1: Process user inputs involving text, image, or both<br>Step2: Visual enhancement module constructs visual representations<br>Step3: Relation predictor infers relationships between nodes<br>Output: Generated 3D indoor scenes with controllable geometry |
9.5 | [[9.5] 2502.06336 DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation](https://arxiv.org/abs/2502.06336) <br> [{'name': 'Sara Monji-Azad, Marvin Kinz, Siddharth Kothari, Robin Khanna, Amrei Carla Mihan, David Maennel, Claudia Scherl, Juergen Hesser'}] | Point Cloud Processing 点云处理 | v2<br>3D reconstruction<br>point cloud registration<br>Transformers | Input: Source and target point clouds 源点云和目标点云<br>Step1: Feature descriptor design 特征描述符设计<br>Step2: Learning displacement vector fields 学习位移向量场<br>Output: Enhanced point cloud registration 改进的点云配准 |
9.5 | [[9.5] 2502.06338 Zero-shot Depth Completion via Test-time Alignment with Affine-invariant Depth Prior](https://arxiv.org/abs/2502.06338) <br> [{'name': 'Lee Hyoseok, Kyeong Seon Kim, Kwon Byung-Ki, Tae-Hyun Oh'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D reconstruction<br>zero-shot learning | Input: Sparse depth measurements and RGB images 输入：稀疏深度测量与RGB图像<br>Step1: Alignment of depth prior with sparse measurements 步骤1：将深度先验与稀疏测量对齐<br>Step2: Optimization loop at test-time to enforce constraints 步骤2：在测试时进行优化循环以强制约束<br>Step3: Depth map completion based on aligned prior 步骤3：基于对齐的先验完成深度图<br>Output: Complete dense depth map 输出：完整的密集深度图 |
9.5 | [[9.5] 2502.06367 FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense Correspondences](https://arxiv.org/abs/2502.06367) <br> [{'name': 'Oliver Boyne, Roberto Cipolla'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-view reconstruction<br>foot model<br>structure-from-motion<br>dense correspondences | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Dataset extension 数据集扩展<br>Step2: Dense correspondence prediction 密集对应关系预测<br>Step3: 3D surface reconstruction via SfM and optimization 通过SfM和优化进行3D表面重建<br>Output: 3D mesh model 输出: 3D网格模型 |
9.5 | [[9.5] 2502.06608 TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](https://arxiv.org/abs/2502.06608) <br> [{'name': 'Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao'}] | 3D Generation 三维生成 | v2<br>3D Generation<br>Shape Diffusion<br>High-Fidelity 3D Models | Input: Images 输入: 图像<br>Step1: Data processing 数据处理<br>Step2: Shape generation 形状生成<br>Step3: Model evaluation 模型评估<br>Output: High-fidelity 3D meshes 输出: 高保真3D网格 |
9.5 | [[9.5] 2502.06682 Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene](https://arxiv.org/abs/2502.06682) <br> [{'name': 'Tai-Yu Pan, Sooyoung Jeon, Mengdi Fan, Jinsu Yoo, Zhenyang Feng, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, Wei-Lun Chao'}] | 3D Generation 三维生成 | v2<br>3D generation<br>collaborative perception<br>autonomous driving<br>point cloud generation | Input: Ego-car sensory data 车载传感器数据<br>Step 1: Data integration 数据集成<br>Step 2: Conditioned diffusion model training 条件扩散模型训练<br>Step 3: Generate realistic point clouds 生成真实的点云<br>Output: Collaborative perception data 协同感知数据 |
9.2 | [[9.2] 2502.05769 Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform](https://arxiv.org/abs/2502.05769) <br> [{'name': 'Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li'}] | 3D Modeling 三维建模 | v2<br>3D modeling<br>Gaussian Splatting<br>urban digital twin<br>GIS integration<br>Large Language Models | Input: Building's address, postal code, or geographic coordinates<br>Step1: Integrate with Google Maps Platform APIs<br>Step2: Perform Gaussian Splatting-based mesh extraction<br>Step3: Retrieve 3D models and visual descriptions<br>Output: Digital twin of the building with 3D models and layers of data |
8.5 | [[8.5] 2502.05409 Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment](https://arxiv.org/abs/2502.05409) <br> [{'name': 'Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder'}] | 3D Simulation and Modeling 三维仿真与建模 | v2<br>3D simulation<br>pose estimation<br>UAV<br>Gaussian splatting | Input: Monocular images from UAV 无人机采集的单目图像<br>Step1: Data integration and simulation 数据集成与仿真<br>Step2: Deep pose estimation algorithm development 深度姿态估计算法开发<br>Step3: Indoor testing and validation 室内测试与验证<br>Output: Accurate pose estimation for UAV relative to the vessel 输出：无人机相对于船只的准确姿态估计 |
8.5 | [[8.5] 2502.05779 A 3D Multimodal Feature for Infrastructure Anomaly Detection](https://arxiv.org/abs/2502.05779) <br> [{'name': 'Yixiong Jing, Wei Lin, Brian Sheil, Sinan Acikgoz'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anomaly detection<br>point clouds<br>crack detection | Input: Point clouds and multimodal features 点云和多模态特征<br>Step1: Feature extraction 特征提取<br>Step2: Integration with PatchCore algorithm 集成至PatchCore算法<br>Step3: Evaluation with statistical methods 使用统计方法进行评估<br>Output: Enhanced defect detection results 改进的缺陷检测结果 |
8.5 | [[8.5] 2502.05964 Revisiting Gradient-based Uncertainty for Monocular Depth Estimation](https://arxiv.org/abs/2502.05964) <br> [{'name': 'Julia Hornauer, Amir El-Ghoussani, Vasileios Belagiannis'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation 单目深度估计<br>Uncertainty Estimation 不确定性估计 | Input: Monocular images 单目图像<br>Step1: Gradient extraction using auxiliary loss 梯度提取与辅助损失<br>Step2: Uncertainty score calculation 不确定性评分计算<br>Output: Depth predictions and uncertainty scores 深度预测与不确定性评分 |
8.5 | [[8.5] 2502.06019 Noise is an Efficient Learner for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2502.06019) <br> [{'name': 'Raza Imam, Asif Hanif, Jian Zhang, Khaled Waleed Dawoud, Yova Kementchedjhieva, Mohammad Yaqub'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>noise adaptation<br>test-time adaptation | Input: Visual representations 视觉表征<br>Step1: Test-time adaptation 测试时适应<br>Step2: Learnable noise optimization 可学习噪声优化<br>Step3: Inter-view representation alignment 视图间表征对齐<br>Output: Enhanced VLM performance 改进的视觉语言模型性能 |
8.5 | [[8.5] 2502.06219 Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing](https://arxiv.org/abs/2502.06219) <br> [{'name': 'Sicen Guo, Tianyou Wen, Chuang-Wei Liu, Qijun Chen, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>RGB-D driving scene parsing<br>Heterogeneous Feature Integration Transformer<br>Vision Foundation Models | Input: RGB and depth data RGB和深度数据<br>Step1: Relative depth estimation 进行相对深度估计<br>Step2: Heterogeneous Feature Integration Transformer (HFIT) development 开发异构特征集成变换器 (HFIT)<br>Step3: Feature integration and evaluation 特征集成与评估<br>Output: Enhanced driving scene parsing model 改进的驾驶场景解析模型 |
8.5 | [[8.5] 2502.06337 Accelerating Outlier-robust Rotation Estimation by Stereographic Projection](https://arxiv.org/abs/2502.06337) <br> [{'name': 'Taosi Xu, Yinlong Liu, Xianbo Wang, Zhi-Xin Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Rotation Estimation<br>Outlier Robustness<br>Stereographic Projection<br>Point Cloud Registration | Input: 3D point sets from different views 3D 点集来自不同视角<br>Step1: Investigate geometric constraints 调查几何约束<br>Step2: Use stereographic projection for rotation axis estimation 使用立体投影进行旋转轴估计<br>Step3: Implement spatial voting for axis identification 实施空间投票以识别轴<br>Output: Optimal rotation estimations  optimal 旋转估计 |
8.5 | [[8.5] 2502.06392 TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles and Viewpoints](https://arxiv.org/abs/2502.06392) <br> [{'name': 'Pengyu Long, Zijun Zhao, Min Ouyang, Qingcheng Zhao, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu'}] | 3D Generation 三维生成 | v2<br>3D hair generation<br>diffusion models<br>multi-view input | Input: Multi-view linearts and images 多视角线稿和图像<br>Step 1: Collecting and annotating diverse hairstyle dataset 收集和标注多样的发型数据集<br>Step 2: Implementing a latent diffusion model with cross-attention 采用具有跨注意力的潜在扩散模型<br>Step 3: Applying parametric post-processing to enforce structural constraints 应用参数后处理以强制执行结构约束<br>Output: High-quality 3D hair strands 高质量三维发丝 |
8.5 | [[8.5] 2502.06543 Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos](https://arxiv.org/abs/2502.06543) <br> [{'name': 'Zhu Chen, Ina Laube, Johannes Stegmaier'}] | 3D Reconstruction  三维重建 | v2<br>3D+t point clouds<br>temporal alignment<br>unsupervised learning | Input: 3D+t point clouds of zebrafish embryos 3D+t 点云<br>Step1: Feature extraction using autoencoder 特征提取通过自编码器<br>Step2: Temporal alignment using regression network 时间对齐通过回归网络<br>Output: Aligned time frames of 3D+t point clouds 对齐的3D+t点云时间帧 |
8.5 | [[8.5] 2502.06782 Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT](https://arxiv.org/abs/2502.06782) <br> [{'name': 'Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, Peng Gao'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>Diffusion Transformers | Input: Video generation task 视频生成任务<br>Step1: Implement Multi-scale Next-DiT architecture 实现多尺度Next-DiT架构<br>Step2: Incorporate motion conditioning 引入运动条件<br>Step3: Progressive and multi-source training for efficiency 进行渐进和多源训练以提高效率<br>Output: High-quality generated videos 高质量生成视频 |
8.5 | [[8.5] 2502.06787 Visual Agentic AI for Spatial Reasoning with a Dynamic API](https://arxiv.org/abs/2502.06787) <br> [{'name': 'Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari'}] | Spatial Reasoning 空间推理 | v2<br>3D spatial reasoning<br>Visual reasoning<br>Dynamic API | Input: Queries for 3D understanding 3D理解的查询<br>Step1: Dynamic API generation 动态API生成<br>Step2: Program synthesis 程序合成<br>Step3: Evaluation with benchmarks 使用基准评估<br>Output: Enhanced 3D spatial reasoning capabilities 改进的3D空间推理能力 |
8.0 | [[8.0] 2502.06023 Dual Caption Preference Optimization for Diffusion Models](https://arxiv.org/abs/2502.06023) <br> [{'name': 'Amir Saeidi, Yiran Luo, Agneet Chatterjee, Shamanthak Hegde, Bimsara Pathiraja, Yezhou Yang, Chitta Baral'}] | Image Generation 图像生成 | v2<br>image generation<br>text-to-image<br>diffusion models | Input: Text-to-image diffusion model 文本到图像扩散模型<br>Step1: Mitigate irrelevant prompts 减少无关提示<br>Step2: Optimize dual caption preferences 优化双重标题偏好<br>Step3: Experiment with different caption strategies 采用不同的标题策略<br>Output: Improved image generation 改进的图像生成 |


## Arxiv 2025-02-10

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.04630 High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting](https://arxiv.org/abs/2502.04630) <br> [{'name': 'Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>sensor fusion<br>Gaussian splatting<br>high-speed imaging | Input: RGB, depth, and event camera data 输入: RGB、深度和事件相机数据<br>Step1: Data integration 数据集成<br>Step2: Scene representation using deformable 3D Gaussians 场景表示使用可变形3D高斯<br>Step3: Joint optimization of Gaussian parameters jointly 优化高斯参数<br>Output: High-quality 3D scene reconstruction 输出: 高质量3D场景重建 |
9.5 | [[9.5] 2502.04734 SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting](https://arxiv.org/abs/2502.04734) <br> [{'name': 'Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>omnidirctional images | Input: 360-degree images 360度图像<br>Step1: Direct pose calibration 直接姿态标定<br>Step2: 3D Gaussians optimization 3D高斯优化<br>Step3: Joint optimization of parameters 参数的联合优化<br>Output: Enhanced omnidirectional radiance fields 改进的全方位辐射场 |
9.5 | [[9.5] 2502.04804 DetVPCC: RoI-based Point Cloud Sequence Compression for 3D Object Detection](https://arxiv.org/abs/2502.04804) <br> [{'name': 'Mingxuan Yan, Ruijie Zhang, Xuedou Xiao, Wei Wang'}] | 3D Object Detection 3D 物体检测 | v2<br>3D reconstruction<br>point cloud compression<br>object detection | Input: 3D point cloud sequences 3D 点云序列<br>Step1: Identify regions of interest (RoIs) 识别兴趣区域 (RoIs)<br>Step2: Apply RoI-based encoding 应用 RoI 基于编码<br>Step3: Compress using VPCC and evaluate compressive performance 基于 VPCC 压缩并评估压缩性能<br>Output: Compressed point cloud data with improved detection accuracy 输出: 经过压缩的点云数据，具有改进的检测准确性 |
9.5 | [[9.5] 2502.04843 PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression](https://arxiv.org/abs/2502.04843) <br> [{'name': 'Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>scene coordinate regression<br>novel view synthesis | Input: Rendered images and sparse inputs 渲染图像和稀疏输入<br>Step1: Pixel filtering to retain well-rendered pixels 像素过滤以保留渲染良好的像素<br>Step2: Scene Coordinate Regression (SCR) model training based on filtered data 基于过滤数据的场景坐标回归模型训练<br>Step3: Evaluation of pose estimation performance 性能评估 |
9.5 | [[9.5] 2502.04981 OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and Geometric-Aware Gaussian Splatting](https://arxiv.org/abs/2502.04981) <br> [{'name': 'Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang'}] | 3D Reconstruction 三维重建 | v2<br>3D occupancy reconstruction<br>semantic reconstruction<br>Gaussian Splatting | Input: Raw sensor data 原始传感器数据<br>Step1: Extract semantic information from vision-language models 提取语言模型中的语义信息<br>Step2: Construct Semantic and Geometric-Aware Gaussians 构建语义和几何意识高斯<br>Step3: Implement cumulative Gaussian-to-3D voxel splatting 实现累积高斯到3D体素的溅射<br>Output: Semantic 3D occupancy reconstruction 语义3D占用重建 |
9.5 | [[9.5] 2502.05040 GaussRender: Learning 3D Occupancy with Gaussian Rendering](https://arxiv.org/abs/2502.05040) <br> [{'name': 'Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord'}] | 3D Reconstruction 三维重建 | v2<br>3D occupancy<br>Gaussian rendering<br>autonomous driving<br>semantic understanding<br>voxel-based supervision | Input: 3D voxel representations 3D体素表示<br>Step1: Projection to 2D perspectives 投影到2D视图<br>Step2: Introduction of Gaussian splatting 高斯点云引入<br>Step3: Loss integration for training 损失函数集成<br>Output: Enhanced 3D occupancy models 改进的3D占用模型 |
9.5 | [[9.5] 2502.05175 Fillerbuster: Multi-View Scene Completion for Casual Captures](https://arxiv.org/abs/2502.05175) <br> [{'name': 'Ethan Weber, Norman M\\"uller, Yash Kant, Vasu Agrawal, Michael Zollh\\"ofer, Angjoo Kanazawa, Christian Richardt'}] | 3D Reconstruction 三维重建 | v2<br>3D scene completion<br>multi-view synthesis<br>novel view generation | Input: Multi-view casual captures 多视角随意捕捉<br>Step1: Unobserved content recovery 未观察到的内容恢复<br>Step2: Generative model training 生成模型训练<br>Step3: Scene completion and pose prediction 场景补全与姿势预测<br>Output: Complete 3D scene with novel views 输出: 完整的三维场景与新视角 |
9.5 | [[9.5] 2502.05176 AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360{\deg} Unbounded Scene Inpainting](https://arxiv.org/abs/2502.05176) <br> [{'name': 'Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D scene inpainting<br>Gaussian Splatting<br>depth-aware methods<br>multi-view coherence<br>unbounded scenes | Input: Multi-view images, camera parameters, object masks, and reference images  输入: 多视角图像、相机参数、对象掩膜和参考图像<br>Step1: Generate depth-aware unseen masks for occlusion identification 步骤1: 生成深度感知的看不见掩膜以识别遮挡<br>Step2: Apply Adaptive Guided Depth Diffusion for point placement 步骤2: 应用自适应引导深度扩散进行点放置<br>Step3: Employ SDEdit for detail enhancement and coherence 步骤3: 使用SDEdit进行细节增强和一致性<br>Output: High-quality inpainted 3D scenes 输出: 高质量的3D场景修复 |
8.5 | [[8.5] 2502.04361 Predicting 3D Motion from 2D Video for Behavior-Based VR Biometrics](https://arxiv.org/abs/2502.04361) <br> [{'name': 'Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee'}] | 3D Motion Prediction 三维运动预测 | v2<br>3D motion prediction<br>biometric authentication<br>virtual reality<br>2D video | Input: 2D body joint data from video 输入: 来自视频的2D身体关节数据<br>Step1: External video tracking 外部视频追踪<br>Step2: 2D to 3D motion prediction 从2D到3D的运动预测<br>Step3: Authentication model evaluation 认证模型评估<br>Output: Enhanced biometric authentication system 输出: 增强的生物识别认证系统 |
8.5 | [[8.5] 2502.04377 MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction](https://arxiv.org/abs/2502.04377) <br> [{'name': 'Xiaoshuai Hao, Yunfeng Diao, Mengchuan Wei, Yifan Yang, Peng Hao, Rong Yin, Hui Zhang, Weiming Li, Shu Zhao, Yu Liu'}] | Map Construction 地图构建 | v2<br>BEV Feature Fusion<br>Autonomous Driving<br>Map Construction<br>Cross-modal Interaction | Input: Multi-modal data from camera and LiDAR sensors<br>Step1: Cross-modal Interaction Transform (CIT) for semantic alignment<br>Step2: Dual Dynamic Fusion (DDF) for selective information integration<br>Step3: Map construction tasks evaluation<br>Output: Enhanced HD and BEV maps |
8.5 | [[8.5] 2502.04378 DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation](https://arxiv.org/abs/2502.04378) <br> [{'name': "Luciano Baresi, Davide Yi Xian Hu, Muhammad Irfan Mas'udi, Giovanni Quattrocchi"}] | Multi-modal Testing and Image Generation 多模态测试与图像生成 | v2<br>autonomous driving<br>deep learning testing<br>diffusion models | Input: Existing images from datasets 现有数据集中的图像<br>Step1: Image captioning 进行图像描述<br>Step2: Keyword identification 关键词识别<br>Step3: Counterfactual caption generation 生成反事实描述<br>Step4: Image generation using diffusion model 利用扩散模型生成图像<br>Output: Augmented test images 增强的测试图像 |
8.5 | [[8.5] 2502.04478 OneTrack-M: A multitask approach to transformer-based MOT models](https://arxiv.org/abs/2502.04478) <br> [{'name': 'Luiz C. S. de Araujo, Carlos M. S. Figueiredo'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Multi-Object Tracking<br>transformers<br>autonomous vehicles | Input: Video sequences from cameras 视频序列<br>Step1: Data pre-processing 数据预处理<br>Step2: Model architecture design 模型架构设计<br>Step3: Multitask training techniques 多任务训练技术<br>Output: Enhanced tracking and detection performance 改进的跟踪与检测性能 |
8.5 | [[8.5] 2502.04483 Measuring Physical Plausibility of 3D Human Poses Using Physics Simulation](https://arxiv.org/abs/2502.04483) <br> [{'name': 'Nathan Louis, Mahzad Khoshlessan, Jason J. Corso'}] | 3D Reconstruction 三维重建 | v2<br>3D human pose estimation<br>physical plausibility<br>physics simulation<br>3D reconstruction | Input: 3D human poses from estimation models 3D 人类姿势估计模型<br>Step1: Physics simulation setup 物理仿真设置<br>Step2: Metric introduction (CoM distance, Pose Stability Duration) 指标引入（质心距离，姿态稳定时间）<br>Step3: Evaluation against state-of-the-art methods 评估与现有最佳方法的比较<br>Output: Metrics for physical plausibility and stability 普适性的物理合理性和稳定性的指标 |
8.5 | [[8.5] 2502.04566 An Optimized YOLOv5 Based Approach For Real-time Vehicle Detection At Road Intersections Using Fisheye Cameras](https://arxiv.org/abs/2502.04566) <br> [{'name': 'Md. Jahin Alam, Muhammad Zubair Hasan, Md Maisoon Rahman, Md Awsafur Rahman, Najibul Haque Sarker, Shariar Azad, Tasnim Nishat Islam, Bishmoy Paul, Tanvir Anjum, Barproda Halder, Shaikh Anowarul Fattah'}] | Autonomous Systems and Robotics 自主系统与机器人技术 | v2<br>vehicle detection<br>YOLOv5<br>fisheye camera<br>autonomous systems | Input: Fisheye camera images 鱼眼摄像头图像<br>Step1: Data acquisition 数据采集<br>Step2: Image preprocessing 图像预处理<br>Step3: Vehicle detection using modified YOLOv5 基于改进的YOLOv5进行车辆检测<br>Step4: Model training and ensemble 模型训练与集成<br>Output: Real-time vehicle detection results 实时车辆检测结果 |
8.5 | [[8.5] 2502.04615 Neural Clustering for Prefractured Mesh Generation in Real-time Object Destruction](https://arxiv.org/abs/2502.04615) <br> [{'name': 'Seunghwan Kim, Sunha Park, Seungkyu Lee'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>point cloud segmentation<br>real-time object destruction | Input: Point cloud data 点云数据<br>Step1: Clustering point cloud with a neural network 使用神经网络进行点云聚类<br>Step2: Predicting structural weaknesses 预测结构弱点<br>Step3: Generating prefractured meshes 生成预裂网格<br>Output: Ready-to-use prefractured meshes 准备使用的预裂网格 |
8.5 | [[8.5] 2502.05055 Differentiable Mobile Display Photometric Stereo](https://arxiv.org/abs/2502.05055) <br> [{'name': 'Gawoon Ban, Hyeongjun Kim, Seokjun Choi, Seungwoo Yoon, Seung-Hwan Baek'}] | 3D Reconstruction 三维重建 | v2<br>Photometric stereo<br>3D reconstruction<br>Mobile devices<br>Surface normals | Input: Mobile phone display and camera 移动电话显示器和相机<br>Step1: Developing a mobile app 开发移动应用<br>Step2: Capturing HDR images and display patterns 捕获HDR图像和显示模式<br>Step3: Learning display patterns 通过可微学习模式<br>Output: 3D surface normals and albedos 3D表面法线和反射率 |
8.5 | [[8.5] 2502.05091 DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions](https://arxiv.org/abs/2502.05091) <br> [{'name': 'Gorkem Can Ates, Kuang Gong, Wei Shao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D vision-language models<br>medical imaging<br>zero-shot classification<br>efficient computation | Input: 3D medical images 3D医学图像<br>Step1: Decomposed convolution设计 设计分解卷积<br>Step2: Integration into CLIP framework 集成到 CLIP 框架中<br>Step3: Evaluation on CT-RATE dataset 在 CT-RATE 数据集上评估<br>Output: Efficient 3D vision-language model 高效的 3D 视觉-语言模型 |
8.5 | [[8.5] 2502.05153 Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment](https://arxiv.org/abs/2502.05153) <br> [{'name': 'Minh-Quan Le, Gaurav Mittal, Tianjian Meng, A S M Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen'}] | Image Generation 图像生成 | v2<br>Image Generation<br>Visual Question Answering<br>Multimodal learning | Input: Multimodal context (reference image + text guidance) 多模态上下文（参考图像 + 文本指导）<br>Step1: Context description generation 上下文描述生成<br>Step2: Fine-tuning of the diffusion model 调整扩散模型<br>Step3: Image generation 生成图像<br>Output: High-fidelity, diverse images 高保真、多样化图像 |
8.5 | [[8.5] 2502.05178 QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178) <br> [{'name': 'Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Kr\\"ahenb\\"uhl, De-An Huang'}] | Neural Rendering 神经渲染 | v2<br>visual tokenization<br>multimodal understanding<br>image generation<br>reconstruction | Input: Image data 影像数据<br>Step1: Train binary-spherical-quantization-based autoencoder 训练基于二元球面量化的自编码器<br>Step2: Dynamically balance reconstruction and alignment objectives 动态平衡重建与对齐目标<br>Step3: Validate performance on multimodal understanding and image generation 验证在多模态理解与图像生成中的表现<br>Output: Unified model for multimodal tasks 输出：多模态任务的统一模型 |
7.5 | [[7.5] 2502.04475 Augmented Conditioning Is Enough For Effective Training Image Generation](https://arxiv.org/abs/2502.04475) <br> [{'name': 'Jiahui Chen, Amy Zhang, Adriana Romero-Soriano'}] | Image Generation 图像生成 | v2<br>image generation<br>data augmentation<br>classification | Input: Real images and text prompts 真实图像和文本提示<br>Step1: Apply data augmentations 应用数据增强<br>Step2: Condition image generation on augmented data 基于增强数据进行图像生成<br>Step3: Generate synthetic training images 生成合成训练图像<br>Output: Enhanced training datasets 改进的训练数据集 |
7.5 | [[7.5] 2502.04896 Goku: Flow Based Video Generative Foundation Models](https://arxiv.org/abs/2502.04896) <br> [{'name': 'Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>image generation<br>video generation<br>text-to-video tasks | Input: Image and video datasets 图像和视频数据集<br>Step1: Data processing pipeline 数据处理管道<br>Step2: Model architecture optimization 模型架构优化<br>Step3: Training and evaluation 训练与评估<br>Output: High-quality image and video generation 高质量的图像和视频生成 |


## Arxiv 2025-02-07

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.03901 LeAP: Consistent multi-domain 3D labeling using Foundation Models](https://arxiv.org/abs/2502.03901) <br> [{'name': 'Simon Gebraad, Andras Palffy, Holger Caesar'}] | 3D Semantic Understanding 3D语义理解 | v2<br>3D semantic labeling<br>Bayesian update<br>Vision Foundation Models | Input: Unlabeled image-pointcloud pairs 输入: 未标记的图像-点云对<br>Step1: Generate soft 2D labels using Vision Foundation Models 步骤1: 使用视觉基础模型生成软2D标签<br>Step2: Apply Bayesian updating to obtain 3D pseudo-labels 步骤2: 应用贝叶斯更新以获得3D伪标签<br>Step3: Use 3D Consistency Network to improve label quality 步骤3: 使用3D一致性网络提高标签质量<br>Output: High-quality 3D semantic labels 输出: 高质量的3D语义标签 |
9.5 | [[9.5] 2502.04318 sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D Reconstruction from Sparse-Views](https://arxiv.org/abs/2502.04318) <br> [{'name': 'Eyvaz Najafli, Marius K\\"astingsch\\"afer, Sebastian Bernhard, Thomas Brox, Andreas Geiger'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>sparse views<br>latent features | Input: Sparse view images 稀疏视图图像<br>Step1: Generate intermediate virtual views 生成中间虚拟视图<br>Step2: Decode Gaussian primitives 解码高斯原语<br>Step3: Render novel views 渲染新视图<br>Output: 360-degree reconstructed scene 360度重建场景 |
9.0 | [[9.0] 2502.04139 Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation](https://arxiv.org/abs/2502.04139) <br> [{'name': 'Jiahao Lu, Jiacheng Deng, Tianzhu Zhang'}] | 3D Instance Segmentation 3D实例分割 | v2<br>3D instance segmentation<br>transformer-based methods | Input: Scene point cloud input 场景点云输入<br>Step1: Query initialization 查询初始化<br>Step2: Hierarchical query fusion 层次查询融合<br>Step3: Instance segmentation 实例分割<br>Output: Binary foreground masks with semantic labels 输出：带语义标签的二元前景掩码 |
8.5 | [[8.5] 2502.03510 Mapping and Localization Using LiDAR Fiducial Markers](https://arxiv.org/abs/2502.03510) <br> [{'name': 'Yibo Liu'}] | Mapping and Localization 映射与定位 | v2<br>LiDAR<br>fiducial markers<br>mapping<br>localization | Input: LiDAR sensors and fiducial markers<br>Step1: Development of Intensity Image-based LiDAR Fiducial Marker system<br>Step2: Detection of 3D fiducials from intensity images<br>Step3: Algorithm enhancement for 3D map merging and localization<br>Output: Optimized mapping and localization using LFMs |
8.5 | [[8.5] 2502.03628 The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628) <br> [{'name': 'Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, Dimitris N. Metaxas'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>hallucination<br>VISTA<br>multimodal learning | Input: Visual tokens from large Vision-Language Models (LVLMs) 视觉令牌来自大型视觉-语言模型<br>Step1: Analyze token logits ranking 分析令牌的对数排名<br>Step2: Identify visual information loss 识别视觉信息损失<br>Step3: Propose VISTA framework 提出VISTA框架<br>Output: Enhanced decoding with reduced hallucination 输出：减少幻觉的增强解码 |
8.5 | [[8.5] 2502.03639 Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach](https://arxiv.org/abs/2502.03639) <br> [{'name': 'Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, Jian Ren'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Video Generation 视频生成<br>3D Point Regularization 3D点正则化<br>Diffusion Models 扩散模型 | Input: 2D videos with 3D point trajectories 2D视频与3D点轨迹<br>Step1: Data augmentation 数据增强<br>Step2: Model fine-tuning 模型微调<br>Step3: Regularization of shape and motion 形状与运动的正则化<br>Output: Enhanced video quality 改进的视频质量 |
8.5 | [[8.5] 2502.03836 Adapting Human Mesh Recovery with Vision-Language Feedback](https://arxiv.org/abs/2502.03836) <br> [{'name': 'Chongyang Xu, Buzhen Huang, Chengfang Zhang, Ziliang Feng, Yangang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>human mesh recovery<br>vision-language models<br>3D reconstruction<br>diffusion-based framework | Input: Monocular images 单目图像<br>Step1: Initial pose prediction using a regression model 初始姿态预测<br>Step2: 2D keypoints extraction from images 从图像中提取2D关键点<br>Step3: Integration of vision-language descriptions 结合视觉语言描述<br>Step4: Refinement of 3D mesh using diffusion modeling 使用扩散模型优化3D网格<br>Output: Enhanced 3D human mesh 改进的3D人类网格 |
8.5 | [[8.5] 2502.03877 Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks](https://arxiv.org/abs/2502.03877) <br> [{'name': 'Yuhui Jin, Yaqiong Zhang, Zheyuan Xu, Wenqing Zhang, Jingyu Xu'}] | 6D Object Detection and Pose Estimation 6D对象检测与姿态估计 | v2<br>6D object detection<br>pose estimation<br>Hybrid Task Cascade<br>High-Resolution Network | Input: 6D object detection data 6D对象检测数据<br>Step1: Hybrid Task Cascade integration 集成混合任务级联<br>Step2: High-Resolution Network backbone usage 使用高分辨率网络骨干<br>Step3: Advanced post-processing techniques 先进的后处理技术<br>Output: Improved object detection and pose estimation models 改进的对象检测和姿态估计模型 |
8.5 | [[8.5] 2502.04111 Adaptive Margin Contrastive Learning for Ambiguity-aware 3D Semantic Segmentation](https://arxiv.org/abs/2502.04111) <br> [{'name': 'Yang Chen, Yueqi Duan, Runzhong Zhang, Yap-Peng Tan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Semantic Segmentation<br>Point Cloud Processing<br>Contrastive Learning | Input: 3D point cloud 数据集<br>Step1: Ambiguity estimation based on position embeddings 基于位置嵌入的模糊性估计<br>Step2: Development of adaptive margin contrastive learning algorithm 自适应边际对比学习算法开发<br>Step3: Evaluation on large-scale datasets 在大规模数据集上进行评估<br>Output: Improved semantic segmentation results 改进的语义分割结果 |
8.5 | [[8.5] 2502.04293 GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation](https://arxiv.org/abs/2502.04293) <br> [{'name': 'Weihang Li, Hongli Xu, Junwen Huang, Hyunjun Jung, Peter KT Yu, Nassir Navab, Benjamin Busam'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic shape<br>pose estimation | Input: Partial RGB-D observations 具有部分可见性的RGB-D观测<br>Step1: Semantic Shape Reconstruction (SSR) 语义形状重建<br>Step2: Global Context Enhanced (GCE) feature fusion module 全球上下文增强特征融合模块<br>Output: Enhanced object poses 改进的物体姿态 |
8.5 | [[8.5] 2502.04329 SMART: Advancing Scalable Map Priors for Driving Topology Reasoning](https://arxiv.org/abs/2502.04329) <br> [{'name': 'Junjie Ye, David Paz, Hengyuan Zhang, Yuliang Guo, Xinyu Huang, Henrik I. Christensen, Yue Wang, Liu Ren'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous driving<br>lane topology reasoning | Input: Standard-definition (SD) and satellite maps 标准清晰度和卫星地图<br>Step 1: Train map prior model to infer lane graphs 训练地图先验模型以推断车道图<br>Step 2: Integrate model with online topology reasoning models 将模型与在线拓扑推理模型集成<br>Output: Enhanced lane topology understanding 改进的车道拓扑理解 |
7.5 | [[7.5] 2502.03813 Optimized Unet with Attention Mechanism for Multi-Scale Semantic Segmentation](https://arxiv.org/abs/2502.03813) <br> [{'name': 'Xuan Li, Quanchao Lu, Yankaiqi Li, Muqing Li, Yijiashun Qi'}] | Image Generation 图像生成 | v2<br>semantic segmentation<br>attention mechanism<br>autonomous driving | Input: Multi-scale images 多尺度图像<br>Step1: Implement attention mechanism 实施注意力机制<br>Step2: Optimize Unet architecture 优化Unet架构<br>Step3: Evaluate on Cityscapes dataset 在Cityscapes数据集上评估<br>Output: Improved segmentation results 改进的分割结果 |
7.5 | [[7.5] 2502.04244 An object detection approach for lane change and overtake detection from motion profiles](https://arxiv.org/abs/2502.04244) <br> [{'name': 'Andrea Benericetti, Niccol\\`o Bellaccini, Henrique Pi\\~neiro Monteagudo, Matteo Simoncini, Francesco Sambo'}] | Autonomous Driving 自动驾驶 | v2<br>object detection<br>lane change<br>ADAS<br>motion profiles<br>autonomous driving | Input: Motion profile images 运动轮廓图像<br>Step1: Dataset creation 数据集创建<br>Step2: Object detection model development 目标检测模型开发<br>Step3: Performance evaluation 性能评估<br>Output: Detection of lane change and overtake maneuvers 车道变换和超车动作检测 |


## Arxiv 2025-02-06

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.02936 Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud](https://arxiv.org/abs/2502.02936) <br> [{'name': 'Junkun Jiang, Jie Chen, Ho Yin Au, Mingyuan Chen, Wei Xue, Yike Guo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Joint Cloud<br>multi-view motion capture | Input: Multi-view images 多视角图像<br>Step1: Triangulate 2D joints into Joint Cloud 将2D关节三角测量为联合云<br>Step2: Process using JCSAT to explore correlations 使用JCSAT处理以探索相关性<br>Step3: Utilize OTAP for feature selection 使用OTAP进行特征选择<br>Output: 3D motion estimation 3D运动估计 |
9.5 | [[9.5] 2502.03449 Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics](https://arxiv.org/abs/2502.03449) <br> [{'name': 'Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>garment generation<br>multi-view images<br>simulation-ready | Input: In-the-wild image 单张图像<br>Step1: Pre-trained image-to-sewing pattern generation model 预训练的图像到缝制模式生成模型<br>Step2: Multi-view diffusion model for producing images 多视角扩散模型用于生成图像<br>Step3: Refinement using a differentiable garment simulator differentiable garment simulator 进行细化<br>Output: Simulation-ready 3D garment 适合模拟的三维服装 |
8.5 | [[8.5] 2502.02907 PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette Stacking](https://arxiv.org/abs/2502.02907) <br> [{'name': 'Jacopo Villa, Jay W. McMahon, Issa A. D. Nesnas'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D pole estimation<br>silhouette stacking | Input: Silhouette images from multiple camera poses 多个相机视角的轮廓图像<br>Step1: Create a silhouette-stack image 创建轮廓堆叠图像<br>Step2: Apply Discrete Fourier Transform to enhance robustness 应用离散傅里叶变换以增强鲁棒性<br>Step3: Estimate 3D pole orientation using projected-pole measurements 使用投影极坐标测量来估计3D极坐标方向<br>Output: Accurate pole orientation estimation 准确的极坐标方向估计 |
8.5 | [[8.5] 2502.02977 Disentangling CLIP Features for Enhanced Localized Understanding](https://arxiv.org/abs/2502.02977) <br> [{'name': 'Samyak Rawelekar, Yujun Cai, Yiwei Wang, Ming-Hsuan Yang, Narendra Ahuja'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>mutual feature information (MFI)<br>vision-language models (VLM)<br>multi-label recognition (MLR) | Input: CLIP features from vision-language models 视觉语言模型中的CLIP特征<br>Step1: Analyze feature correlation 分析特征相关性<br>Step2: Implement MFI loss 施加MFI损失<br>Step3: Align text and image features 对齐文本和图像特征<br>Output: Improved localized understanding 改进的局部理解 |
8.5 | [[8.5] 2502.03005 Driver Assistance System Based on Multimodal Data Hazard Detection](https://arxiv.org/abs/2502.03005) <br> [{'name': 'Long Zhouxiang, Ovanes Petrosian'}] | Autonomous Driving 自动驾驶 | v2<br>multimodal data<br>hazard detection<br>autonomous driving<br>incident recognition | Input: Multimodal data (video, audio) 输入：多模态数据（视频、音频）<br>Step1: Data integration 数据集成<br>Step2: Attention-based fusion strategy 基于注意力的融合策略<br>Step3: Incident recognition incidents 事件识别<br>Output: Enhanced detection accuracy 改进的检测精度 |
8.5 | [[8.5] 2502.03465 Seeing World Dynamics in a Nutshell](https://arxiv.org/abs/2502.03465) <br> [{'name': 'Qiuhong Shen, Xuanyu Yi, Mingbao Lin, Hanwang Zhang, Shuicheng Yan, Xinchao Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D representation<br>Monocular video<br>Dynamic Gaussian Splatting | Input: Monocular videos 单目视频<br>Step1: Transform videos to dynamic Gaussian representations 将视频转换为动态高斯表示<br>Step2: Introduce STAG representation 引入结构化时空对齐高斯表示<br>Step3: Optimizing for spatial and temporal coherence 进行空间和时间一致性的优化<br>Output: High-fidelity video reconstruction and spatial-temporal modeling 高保真视频重建和时空建模 |
7.5 | [[7.5] 2502.02951 VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA](https://arxiv.org/abs/2502.02951) <br> [{'name': 'Madhuri Latha Madaka, Chakravarthy Bhagvati'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>VQA dataset<br>Hierarchical questions | Input: Visual content and questions 视觉内容和问题<br>Step1: Dataset development 数据集开发<br>Step2: Classification of questions 问题分类<br>Step3: Initial testing on VQA systems 在VQA系统上的初步测试<br>Output: VQA-Levels dataset VQA-Levels数据集 |


## Arxiv 2025-02-05

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.01666 Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding](https://arxiv.org/abs/2502.01666) <br> [{'name': 'Jingming Xia, Guanqun Cao, Guang Ma, Yiben Luo, Qinzhao Li, John Oyekan'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>3D reconstruction<br>generative models<br>autonomous driving | Input: RGB image<br>Step1: Extract latent features using Image Encoder<br>Step2: Extract semantic vector through Image Semantic Encoder<br>Step3: Integrate features within a denoising UNet<br>Step4: Generate final metric depth map<br>Output: Enhanced depth prediction |
9.5 | [[9.5] 2502.01846 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping](https://arxiv.org/abs/2502.01846) <br> [{'name': 'Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Arthur Chen, Srinath Sridhar, Aayush Prakash'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>diffusion models<br>3D generation<br>structured representation | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Spherical mapping to transform data into structured 2D representation 使用球面映射将数据转换为结构化2D表示<br>Step2: Multi-branch network for feature compression 使用多分支网络进行特征压缩<br>Step3: Integration with existing 2D models with zero-shot learning 将其与现有的2D模型进行无缝整合<br>Output: Structured 3D representation ready for generative tasks 输出：准备好用于生成任务的结构化3D表示 |
9.5 | [[9.5] 2502.01855 Learning Fine-to-Coarse Cuboid Shape Abstraction](https://arxiv.org/abs/2502.01855) <br> [{'name': 'Gregor Kobsik, Morten Henkel, Yanjiang He, Victor Czech, Tim Elsner, Isaak Lim, Leif Kobbelt'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>shape abstraction<br>cuboids<br>unsupervised learning<br>structural analysis | Input: Collections of 3D shapes 3D形状集<br>Step1: Initialize with fine reconstruction to capture details 细致重建以捕获细节<br>Step2: Gradually reduce primitives while optimizing loss 渐进减少原始体并优化损失<br>Step3: Evaluate performance on shape benchmarks 在形状基准上评估性能<br>Output: Compact cuboid-based representations 紧凑的立方体表示 |
9.5 | [[9.5] 2502.01856 Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2502.01856) <br> [{'name': 'Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, WonSook Lee'}] | 3D Object Detection 三维物体检测 | v2<br>LiDAR-camera fusion<br>3D object detection<br>autonomous driving | Input: LiDAR and camera data 数据<br>Step1: Spatio-Temporal Feature Aggregation (STFA) module processes input 提取时空特征<br>Step2: Reliability module assigns confidence scores 可靠性模块自信度评分<br>Step3: Confidence-Weighted Mutual Cross-Attention (CW-MCA) module balances information with confidence 用置信度动态平衡信息<br>Output: Enhanced 3D object detection 改进的三维物体检测 |
9.5 | [[9.5] 2502.01896 INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy](https://arxiv.org/abs/2502.01896) <br> [{'name': 'Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi'}] | 3D Perception and Modeling 3D 感知与建模 | v2<br>LiDAR<br>3D perception<br>object detection | Input: Noisy LiDAR data 噪声激光雷达数据<br>Step 1: Meta-learning phase 迁移学习阶段<br>Step 2: Generate robust saliency maps 生成健壮的显著性图<br>Step 3: Adversarial curriculum training 对抗性课程训练<br>Output: Enhanced noise resilience 提升噪声鲁棒性 |
9.5 | [[9.5] 2502.02163 Progressive Correspondence Regenerator for Robust 3D Registration](https://arxiv.org/abs/2502.02163) <br> [{'name': 'Guiyu Zhao, Sheng Ao, Ye Zhang, Kai Xu Yulan Guo'}] | 3D Registration 3D配准 | v2<br>3D registration<br>point cloud<br>outlier removal<br>reconstruction<br>robustness | Input: Point cloud data 点云数据<br>Step1: Prior-guided local grouping using generalized mutual matching 先验引导的局部分组与互匹配<br>Step2: Local correspondence correction using center-aware three-point consistency 局部对应关系修正<br>Step3: Global correspondence refinement using extensive iterations 全局对应关系的细化<br>Output: High-quality point correspondences 高质量的点对应关系 |
9.5 | [[9.5] 2502.02187 ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion](https://arxiv.org/abs/2502.02187) <br> [{'name': 'Nissim Maruani, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun'}] | 3D Generation 三维生成 | v2<br>3D Generation 3D生成<br>Shape Variations 形状变体 | Input: Reference 3D model 参考3D模型<br>Step1: Sparse voxel grid and point sampling 稀疏体素网格和点采样<br>Step2: Multiscale neural architecture training 多尺度神经架构训练<br>Step3: Generate shape variations 生成形状变体<br>Output: High-quality 3D shapes 高质量3D形状 |
9.5 | [[9.5] 2502.02247 Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning](https://arxiv.org/abs/2502.02247) <br> [{'name': 'Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud analysis 3D点云分析<br>domain generalization 域推广<br>rotation robustness 旋转鲁棒性 | Input: 3D point clouds 3D点云<br>Step 1: Identify challenging rotations 识别具有挑战性的旋转<br>Step 2: Construct intricate orientation set 构建复杂方向集<br>Step 3: Utilize contrastive learning against orientations 使用对比学习进行方向建模<br>Output: Generalizable features with rotation consistency 输出: 具有旋转一致性的可泛化特征 |
9.5 | [[9.5] 2502.02283 GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/abs/2502.02283) <br> [{'name': 'Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Structure-from-Motion<br>point clouds<br>novel view synthesis | Input: Sparse SfM point clouds 稀疏结构光点云<br>Step1: Dynamic sampling dynamic sampling 动态采样<br>Step2: Gaussian Process modeling 高斯过程建模<br>Step3: Densification of point clouds 点云稠密化<br>Output: Enhanced 3D Gaussian representation 改进的3D高斯表示 |
9.5 | [[9.5] 2502.02334 Event-aided Semantic Scene Completion](https://arxiv.org/abs/2502.02334) <br> [{'name': 'Shangwei Guo, Hao Shi, Song Wang, Xiaoting Yin, Kailun Yang, Kaiwei Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Semantic Scene Completion<br>3D Reconstruction | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model evaluation 模型评估<br>Output: Enhanced 3D models 改进的三维模型 |
9.5 | [[9.5] 2502.02338 Geometric Neural Process Fields](https://arxiv.org/abs/2502.02338) <br> [{'name': 'Wenzhe Yin, Zehao Xiao, Jiayi Shen, Yunlu Chen, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D scenes<br>probabilistic modeling | Input: Limited context images 限制的上下文图像<br>Step1: Probabilistic modeling 概率建模<br>Step2: Integrate geometric bases 集成几何基底<br>Step3: Hierarchical latent variable design 分层潜变量设计<br>Output: Improved generalization 改进的泛化能力 |
9.5 | [[9.5] 2502.02372 MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning](https://arxiv.org/abs/2502.02372) <br> [{'name': 'Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>avatar generation<br>continual learning | Input: Image data of avatars 头像图像数据<br>Step1: Implement continual learning strategy 进行持续学习策略<br>Step2: Develop Global-Local Joint Storage Module 开发全局-局部联合存储模块<br>Step3: Develop Pose Distillation Module 开发姿态提炼模块<br>Output: Maintainable virtual avatar 可维护虚拟头像 |
9.5 | [[9.5] 2502.02548 Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2502.02548) <br> [{'name': 'Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy'}] | 3D Segmentation 三维分割 | v2<br>3D segmentation<br>open-vocabulary<br>Vision-Language Models | Input: Multi-view images 多视角图像<br>Step1: Data generation 数据生成<br>Step2: Data annotation 数据注释<br>Step3: Training model 训练模型<br>Output: Open-vocabulary segmentation model 开放词汇分割模型 |
9.5 | [[9.5] 2502.02590 Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling](https://arxiv.org/abs/2502.02590) <br> [{'name': 'Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D articulated objects<br>Vision-Language Models<br>3D modeling | Input: 3D meshes 3D 网格<br>Step1: Movable Part Segmentation 可动部分分割<br>Step2: Articulation Estimation 关节估计<br>Step3: Refinement 精化<br>Output: Articulated 3D objects 装配式三维物体 |
9.2 | [[9.2] 2502.01940 Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach](https://arxiv.org/abs/2502.01940) <br> [{'name': 'Mohammed Alsakabi, Aidan Erickson, John M. Dolan, Ozan K. Tonguz'}] | Autonomous Driving 自动驾驶 | v2<br>3D reconstruction<br>autonomous driving<br>depth maps | Input: Images from 4D radar detectors and RGB cameras 4D 雷达探测器和 RGB 摄像头的图像<br>Step1: Integrate radar depth maps and RGB images 集成雷达深度图和 RGB 图像<br>Step2: Apply pixel positional encoding algorithm 应用像素位置信息编码算法<br>Step3: Develop spectrum estimation algorithms 研发光谱估计算法<br>Step4: Train depth map generative models 训练深度图生成模型<br>Output: Enhanced depth maps 改进的深度图 |
9.2 | [[9.2] 2502.02144 DOC-Depth: A novel approach for dense depth ground truth generation](https://arxiv.org/abs/2502.02144) <br> [{'name': 'Simon de Moreau, Mathias Corsia, Hassan Bouchiba, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Fabien Moutarde'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Reconstruction 三维重建<br>Dense Depth Generation 密集深度生成<br>LiDAR 激光雷达 | Input: LiDAR sensor data 利用激光雷达传感器数据<br>Step1: 3D environment reconstruction 3D环境重建<br>Step2: Dynamic object classification 动态对象分类<br>Step3: Dense depth generation 密集深度生成<br>Output: Dense depth annotation output 输出：密集深度标注 |
8.5 | [[8.5] 2502.01814 PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://arxiv.org/abs/2502.01814) <br> [{'name': 'Dazhou Yu, Genpei Zhang, Liang Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>polyhedral representation<br>surface-attributed graph | Input: Polyhedral data 多面体数据<br>Step1: Decompose into local rigid representations 将其分解为局部刚性表示<br>Step2: Hierarchical aggregation of representations 层次聚合表示<br>Output: Global representation of polyhedra 全球多面体表示 |
8.5 | [[8.5] 2502.01894 SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset](https://arxiv.org/abs/2502.01894) <br> [{'name': 'Goodarz Mehr, Azim Eskandarian'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>Synthetic Data Generation 合成数据生成<br>Autonomous Driving 自动驾驶<br>BEV Representation 鸟瞰视图表示 | Input: Multi-sensor data collection 多传感器数据收集<br>Step1: Configuration of synthetic data generation 生成合成数据的配置<br>Step2: Data generation for BEV representation 生成鸟瞰视图表示的数据<br>Step3: Annotation of perception data 性能数据的标注<br>Output: SimBEV dataset with annotated driving scenarios 输出: 包含标注的驾驶场景的SimBEV数据集 |
8.5 | [[8.5] 2502.01949 LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.01949) <br> [{'name': 'Yang Zhou, Zongjin He, Qixuan Li, Chao Wang'}] | 3D Generation 三维生成 | 3D scene generation<br>physically consistent layouts<br>text-guided generation | Input: Text prompt 文本提示<br>Step1: Convert text to scene graph 将文本转换为场景图<br>Step2: Adjust Gaussian densities and layouts 调整高斯密度和布局<br>Step3: Make dynamic camera adjustments 进行动态相机调整<br>Output: 3D compositional scene generation 3D 组合场景生成 |
8.5 | [[8.5] 2502.01961 Hierarchical Consensus Network for Multiview Feature Learning](https://arxiv.org/abs/2502.01961) <br> [{'name': 'Chengwei Xia, Chaoxi Niu, Kun Zhan'}] | Multi-view and Stereo Vision 多视角立体 | v2<br>multiview feature learning<br>hierarchical consensus<br>3D reconstruction | Input: Multi-view images 多视角图像<br>Step1: Learning view-consistency features 学习视图一致性特征<br>Step2: Hierarchical consensus derivation 层次共识推导<br>Step3: Comprehensive feature extraction 综合特征提取<br>Output: Discriminative features 具有区分性的特征 |
8.5 | [[8.5] 2502.02091 Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation](https://arxiv.org/abs/2502.02091) <br> [{'name': 'JooHyun Kwon, Hanbyel Cho, Junmo Kim'}] | Image and Video Generation 图像生成 | v2<br>4D Gaussian Splatting<br>dynamic scene editing<br>computer vision<br>motion artifacts | Input: 4D dynamic scene data 4D动态场景数据<br>Step1: Model static 3D Gaussians 模型静态三维高斯<br>Step2: Implement Hexplane-based deformation field 实现基于Hexplane的变形场<br>Step3: Perform editing on static 3D Gaussians 在静态三维高斯上执行编辑<br>Step4: Apply score distillation for refinement 应用得分蒸馏进行细化<br>Output: Enhanced edited dynamic scenes 改进的编辑动态场景 |
8.5 | [[8.5] 2502.02322 Improving Generalization Ability for 3D Object Detection by Learning Sparsity-invariant Features](https://arxiv.org/abs/2502.02322) <br> [{'name': 'Hsin-Cheng Lu, Chung-Yi Lin, Winston H. Hsu'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection 3D物体检测<br>autonomous driving 自动驾驶<br>generalization 泛化 | Input: Source domain 3D point clouds 源域3D点云<br>Step1: Downsample the point cloud based on confidence scores 根据置信度得分下采样点云<br>Step2: Teacher-student framework to align BEV features 使用师生框架对齐鸟瞰视图特征<br>Step3: Apply FCA and GERA to maintain consistency 使用FCA和GERA保持一致性<br>Output: Domain-agnostic 3D object detector 域无关的3D物体检测器 |
8.5 | [[8.5] 2502.02468 High-Fidelity Human Avatars from Laptop Webcams using Edge Compute](https://arxiv.org/abs/2502.02468) <br> [{'name': 'Akash Haridas, Imran N. Junejo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Morphable Models 3D可变形模型<br>Photo-realistic Rendering 照相真实渲染<br>Avatar Generation 头像生成 | Input: Images from consumer-grade laptop webcams 笔记本电脑网络摄像头拍摄的图像<br>Step1: Shape generation by fitting 3DMM shape parameters 通过拟合3D形状模型参数生成形状<br>Step2: Texture map generation 纹理图生成<br>Step3: Rendering using pre-defined parameters 使用预定义参数进行渲染<br>Output: High-fidelity animatable avatars 高保真可动画化头像 |
8.5 | [[8.5] 2502.02537 Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks](https://arxiv.org/abs/2502.02537) <br> [{'name': 'Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, Fei Miao'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Collaborative Object Detection<br>Uncertainty Quantification<br>Adversarial Attacks<br>Autonomous Driving | Input: Collaborative Object Detection (COD) models 协作目标检测模型<br>Step1: Apply adversarial training adversarially during collaboration 在协作中施加对抗性训练<br>Step2: Provide output uncertainty estimation through learning-based module 提供基于学习的模块输出的不确定性估计<br>Step3: Calibrate uncertainty using conformal prediction 对不确定性进行校准<br>Output: Enhanced object detection accuracy 提高的目标检测准确性 |
7.5 | [[7.5] 2502.01906 Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models](https://arxiv.org/abs/2502.01906) <br> [{'name': 'Chia-Wen Kuo, Sijie Zhu, Fan Chen, Xiaohui Shen, Longyin Wen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>Decomposed Attention<br>cross-modal learning | Input: Visual and textual embeddings 视觉和文本嵌入<br>Step1: Decompose the self-attention mechanism 解构自注意力机制<br>Step2: Optimize visual-to-visual self-attention 视觉-视觉自注意力优化<br>Step3: Merge visual and textual information 视觉与文本信息合并<br>Output: Improved efficiency and performance of LVLMs 提高LVLM效率与性能 |
7.5 | [[7.5] 2502.01969 Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration](https://arxiv.org/abs/2502.01969) <br> [{'name': 'Younan Zhu, Linwei Tao, Minjing Dong, Chang Xu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucination<br>attention calibration | Input: Large Vision-Language Models (LVLMs) 大型视觉语言模型<br>Step1: Bias estimation from input image 输入图像的偏差估计<br>Step2: Uniform Attention Calibration (UAC) application 应用统一注意力校准<br>Step3: Dynamic Attention Calibration (DAC) implementation 实现动态注意力校准<br>Output: Reduced object hallucination 减少物体幻觉 |


## Arxiv 2025-02-05

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.01814 PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://arxiv.org/abs/2502.01814) <br> [{'name': 'Dazhou Yu, Genpei Zhang, Liang Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>polyhedral representation | Input: 3D polyhedral objects 3D 多面体对象<br>Step1: Surface-attributed graph construction 表面属性图构建<br>Step2: Local rigid representation learning 局部刚性表示学习<br>Step3: Hierarchical aggregation of representations 表示的分层聚合<br>Output: Global representation of polyhedra 全球多面体表示 |
9.5 | [[9.5] 2502.01846 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping](https://arxiv.org/abs/2502.01846) <br> [{'name': 'Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Arthur Chen, Srinath Sridhar, Aayush Prakash'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>UV Mapping<br>image-based generation<br>3D reconstruction 3D重建 | Input: 3D Gaussian Splatting (3DGS) data 3D高斯点云数据<br>Step1: Spherical mapping to create a structured 2D representation 使用球面映射创建结构化的2D表示<br>Step2: Compression of heterogeneous features into a shared feature space 将异构特征压缩到共享特征空间<br>Step3: Integration with pre-trained 2D generative models 与预训练的2D生成模型集成<br>Output: Structured 2D UV Gaussian Splatting representation 结构化的2D UV高斯点云表示 |
9.5 | [[9.5] 2502.01856 Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2502.01856) <br> [{'name': 'Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, WonSook Lee'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection<br>LiDAR-camera fusion<br>autonomous driving | Input: Sensor data from LiDAR and camera LiDAR和摄像头的传感器数据<br>Step1: Integration of spatial and semantic information 空间和语义信息的集成<br>Step2: Implementation of Reliability module to assess confidence 实现可靠性模块以评估置信度<br>Step3: Use of CW-MCA for dynamic weighting of modalities 使用CW-MCA对模态进行动态加权<br>Output: Robust 3D object detection results 稳健的3D目标检测结果 |
9.5 | [[9.5] 2502.01940 Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach](https://arxiv.org/abs/2502.01940) <br> [{'name': 'Mohammed Alsakabi, Aidan Erickson, John M. Dolan, Ozan K. Tonguz'}] | Depth Estimation 深度估计 | v2<br>Depth Estimation 深度估计<br>Autonomous Vehicles 自动驾驶<br>Radar-RGB Integration 雷达- RGB集成 | Input: Radar depth maps and RGB images 雷达深度图和RGB图像<br>Step1: Pixel positional encoding 像素位置编码<br>Step2: Transformation to Spatial Spectrum 转换为空间谱<br>Step3: Generating denser depth maps 生成更密集的深度图<br>Output: Enhanced depth maps 改进的深度图 |
9.5 | [[9.5] 2502.02144 DOC-Depth: A novel approach for dense depth ground truth generation](https://arxiv.org/abs/2502.02144) <br> [{'name': 'Simon de Moreau, Mathias Corsia, Hassan Bouchiba, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Fabien Moutarde'}] | Depth Estimation 深度估计 | v2<br>depth estimation 深度估计<br>LiDAR<br>3D reconstruction 三维重建 | Input: LiDAR measurements LiDAR测量<br>Step1: Data aggregation 数据聚合<br>Step2: Dynamic object classification 动态物体分类<br>Step3: Dense depth generation 密集深度生成<br>Output: Fully-dense depth annotations 完全密集的深度注解 |
9.5 | [[9.5] 2502.02163 Progressive Correspondence Regenerator for Robust 3D Registration](https://arxiv.org/abs/2502.02163) <br> [{'name': 'Guiyu Zhao, Sheng Ao, Ye Zhang, Kai Xu Yulan Guo'}] | 3D Registration 3D 注册 | v2<br>3D registration<br>point cloud registration | Input: Point clouds from different perspectives 从不同视角获得点云<br>Step1: Prior-guided local grouping prior引导局部分组<br>Step2: Generalized mutual matching 广义互匹配<br>Step3: Center-aware three-point consistency center-aware三点一致性<br>Step4: Global correspondence refinement 全局对应关系精炼<br>Output: High-quality correspondences 高质量对应关系 |
9.5 | [[9.5] 2502.02187 ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion](https://arxiv.org/abs/2502.02187) <br> [{'name': 'Nissim Maruani, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun'}] | 3D Generation 三维生成 | v2<br>3D Generation<br>shape variations<br>multiscale neural architecture<br>interactive generation | Input: A single reference 3D model 单一参考3D模型<br>Step1: Shape variations generation 形状变体生成<br>Step2: Multiscale diffusion sampling 多尺度扩散采样<br>Step3: Interactive editing 交互式编辑<br>Output: High-quality 3D shape variants 高质量3D形状变体 |
9.5 | [[9.5] 2502.02247 Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning](https://arxiv.org/abs/2502.02247) <br> [{'name': 'Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud<br>domain generalization<br>rotation robustness | Input: Point clouds with variable orientations 变量方向的点云<br>Step1: Identify challenging rotations 识别具有挑战性的旋转<br>Step2: Construct intricate orientation set 构建复杂方向集<br>Step3: Apply contrastive learning using intricate samples 使用复杂样本进行对比学习<br>Output: Enhanced orientation-aware 3D representations 改进的方向感知3D表示 |
9.5 | [[9.5] 2502.02283 GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/abs/2502.02283) <br> [{'name': 'Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Processes<br>novel view synthesis | Input: Sparse SfM point clouds 稀疏的结构光点云<br>Step1: Develop MOGP model 开发多输出高斯过程模型<br>Step2: Adaptive sampling and filtering strategy 自适应采样和过滤策略<br>Step3: Densify the point clouds 使点云密集化<br>Output: High-quality 3D Gaussians 高质量的3D高斯 |
9.5 | [[9.5] 2502.02322 Improving Generalization Ability for 3D Object Detection by Learning Sparsity-invariant Features](https://arxiv.org/abs/2502.02322) <br> [{'name': 'Hsin-Cheng Lu, Chung-Yi Lin, Winston H. Hsu'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>autonomous driving<br>domain generalization | Input: LiDAR point clouds from various domains 各种域的LiDAR点云<br>Step1: Data subsampling based on confidence scores 根据置信度评分进行数据子采样<br>Step2: Teacher-student framework implementation 教师-学生框架实施<br>Step3: Feature alignment between domains 域间特征对齐<br>Output: Generalized 3D object detector 具备良好泛化能力的3D物体检测器 |
9.5 | [[9.5] 2502.02334 Event-aided Semantic Scene Completion](https://arxiv.org/abs/2502.02334) <br> [{'name': 'Shangwei Guo, Hao Shi, Song Wang, Xiaoting Yin, Kailun Yang, Kaiwei Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic scene completion<br>autonomous driving<br>event cameras | Input: Event and RGB images 输入：事件图像和RGB图像<br>Step1: Data integration 数据集成<br>Step2: Event-aided Lifting Module (ELM) 事件辅助提升模块开发<br>Step3: 3D scene reconstruction 三维场景重建<br>Output: Enhanced 3D semantic occupancy models 输出：改进的3D语义占用模型 |
9.5 | [[9.5] 2502.02338 Geometric Neural Process Fields](https://arxiv.org/abs/2502.02338) <br> [{'name': 'Wenzhe Yin, Zehao Xiao, Jiayi Shen, Yunlu Chen, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>Geometric Neural Process Fields<br>3D reconstruction | Input: Limited context observations 有限上下文观察<br>Step 1: Formulate NeF generalization as a probabilistic problem 将NeF泛化表述为一个概率问题<br>Step 2: Design geometric bases to encode structural information 设计几何基以编码结构信息<br>Step 3: Develop a hierarchical latent variable model for parameterization 建立分层潜变量模型以进行参数化<br>Output: Improved generalization for novel scenes and signals 改进的新场景和信号的泛化能力 |
9.5 | [[9.5] 2502.02548 Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2502.02548) <br> [{'name': 'Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy'}] | 3D Segmentation 三维分割 | v2<br>3D segmentation 3D分割<br>open-vocabulary 开放词汇 | Input: 3D scene datasets 3D场景数据集<br>Step1: Data generation data generation 数据生成<br>Step2: Model training 模型训练<br>Step3: Segmentation validation 分割验证<br>Output: Open-vocabulary 3D segmentation results 开放词汇3D分割结果 |
9.5 | [[9.5] 2502.02590 Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling](https://arxiv.org/abs/2502.02590) <br> [{'name': 'Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D modeling<br>articulated objects 3D建模<br>可动物体 | Input: 3D mesh 输入: 3D网格<br>Step1: Movable Part Segmentation 可移动部分分割<br>Step2: Articulation Estimation and Refinement 动作估计与精细化<br>Output: Articulated 3D object 输出: 可动的3D物体 |
9.0 | [[9.0] 2502.01666 Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding](https://arxiv.org/abs/2502.01666) <br> [{'name': 'Jingming Xia, Guanqun Cao, Guang Ma, Yiben Luo, Qinzhao Li, John Oyekan'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation 单目深度估计<br>Autonomous Driving 自动驾驶<br>3D Reconstruction 三维重建 | Input: Single RGB image 单个RGB图像<br>Step1: Image-based semantic embedding image-based using SeeCoder 图像语义嵌入<br>Step2: Integration of features via denoising UNet 特征集成通过去噪UNet<br>Step3: Depth map generation 深度图生成<br>Output: Enhanced depth map 改进的深度图 |
9.0 | [[9.0] 2502.01855 Learning Fine-to-Coarse Cuboid Shape Abstraction](https://arxiv.org/abs/2502.01855) <br> [{'name': 'Gregor Kobsik, Morten Henkel, Yanjiang He, Victor Czech, Tim Elsner, Isaak Lim, Leif Kobbelt'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape abstraction<br>unsupervised learning<br>cuboids | Input: Collections of 3D shapes 三维形状集合<br>Step1: Initial fine reconstruction 初始化细致重建<br>Step2: Apply fine-to-coarse abstraction fine-to-coarse abstraction<br>Step3: Optimize reconstruction and volume preservation 优化重建与体积保持<br>Output: Cuboid-based structural abstraction cuboid 基于的结构抽象 |
8.5 | [[8.5] 2502.01894 SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset](https://arxiv.org/abs/2502.01894) <br> [{'name': 'Goodarz Mehr, Azim Eskandarian'}] | Autonomous Driving 自动驾驶 | v2<br>BEV perception<br>synthetic data generation<br>autonomous driving | Input: Multi-sensor data 多传感器数据<br>Step1: Data generation 生成数据<br>Step2: Ground truth capture 捕获真实数据<br>Step3: Dataset creation 创建数据集<br>Output: Comprehensive BEV dataset 完整的鸟瞩图数据集 |
8.5 | [[8.5] 2502.01896 INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy](https://arxiv.org/abs/2502.01896) <br> [{'name': 'Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi'}] | 3D Point Cloud Processing 点云处理 | v2<br>LiDAR<br>adversarial training<br>3D perception | Input: Noisy LiDAR data 噪声LiDAR数据<br>Step1: Prepare saliency maps 准备显著性图<br>Step2: Apply adversarial curriculum training 应用对抗课程训练<br>Step3: Train student network 训练学生网络<br>Output: Robust deep learning model 稳健的深度学习模型 |
8.5 | [[8.5] 2502.01949 LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.01949) <br> [{'name': 'Yang Zhou, Zongjin He, Qixuan Li, Chao Wang'}] | 3D Generation 三维生成 | 3D scene generation<br>3D Gaussian Splatting<br>physics-guided generation | Input: Text prompt 文本提示<br>Step1: Convert text to scene graph 将文本转换为场景图<br>Step2: Adjust density and layout 调整密度和布局<br>Step3: Dynamic camera adjustments 动态相机调整<br>Output: Compositional 3D scenes 组合三维场景 |
8.5 | [[8.5] 2502.01961 Hierarchical Consensus Network for Multiview Feature Learning](https://arxiv.org/abs/2502.01961) <br> [{'name': 'Chengwei Xia, Chaoxi Niu, Kun Zhan'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>Multiview Learning 多视角学习<br>Consensus Learning 共识学习<br>Feature Integration 特征整合 | Input: Multi-view data 多视角数据<br>Step1: Learn distinct and common information 学习独特和共同信息<br>Step2: Derive consensus indices 生成共识指标<br>Step3: Perform hierarchical consensus learning 进行分层共识学习<br>Output: Comprehensive and discriminative features 详尽和有辨识度的特征 |
8.5 | [[8.5] 2502.01969 Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration](https://arxiv.org/abs/2502.01969) <br> [{'name': 'Younan Zhu, Linwei Tao, Minjing Dong, Chang Xu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucination | Input: LVLMs with visual tokens 视觉语言模型与视觉标记<br>Step1: Analyze attention biases 分析注意力偏差<br>Step2: Implement UAC for calibration 实施均匀注意力校准<br>Step3: Develop DAC for dynamic adjustment 开发动态注意力校准模块<br>Output: Improved alignment and reduced hallucination 输出: 改进的对齐和减少的幻觉 |
8.5 | [[8.5] 2502.02171 DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging](https://arxiv.org/abs/2502.02171) <br> [{'name': 'Mohamed Youssef, Jian Peng, Oliver Bimber'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>remote sensing<br>vegetation analysis | Input: Aerial images from drones 通过无人机获取航空图像<br>Step1: Synthetic-aperture imaging 合成孔径成像<br>Step2: Use 3D convolutional neural networks to reduce out-of-focus signals 使用3D卷积神经网络减少模糊信号<br>Step3: Combine multiple reflectance stacks from various spectral channels 结合来自不同光谱通道的多重反射堆栈<br>Output: Volumetric representations of vegetation 体积植被表示 |
8.5 | [[8.5] 2502.02372 MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning](https://arxiv.org/abs/2502.02372) <br> [{'name': 'Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D rendering<br>continual learning | Input: Limited training data 对应的有限训练数据<br>Step1: Employ NeRF for 3D rendering 使用NeRF进行3D渲染<br>Step2: Implement a Global-Local Joint Storage Module 实现全局-局部联合存储模块<br>Step3: Utilize a Pose Distillation Module 使用姿态蒸馏模块<br>Output: Maintainable virtual avatars 可维护的虚拟 avatar |
8.5 | [[8.5] 2502.02468 High-Fidelity Human Avatars from Laptop Webcams using Edge Compute](https://arxiv.org/abs/2502.02468) <br> [{'name': 'Akash Haridas Imran N. Junejo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>avatar generation<br>differentiable rendering | Input: Consumer-grade laptop webcam images 使用普通笔记本电脑网络摄像头的图像<br>Step1: Shape generation using 3D morphable models 使用3D可变形模型生成形状<br>Step2: Landmark detection using optimization 标记检测使用优化<br>Step3: Texture generation with GANs 使用GAN生成纹理<br>Step4: Differentiable rendering to create avatars 使用可微渲染创建虚拟形象<br>Output: High-fidelity human avatars 高保真度人类虚拟形象 |
8.5 | [[8.5] 2502.02525 Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation](https://arxiv.org/abs/2502.02525) <br> [{'name': 'Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian'}] | Object Pose Estimation 物体姿态估计 | v2<br>9-DoF object pose estimation<br>domain generalization<br>robotic grasping | Input: Rendered synthetic data 渲染合成数据<br>Step1: Model training 模型训练<br>Step2: Pose estimation 估计姿态<br>Step3: Real-time performance optimization 实时性能优化<br>Output: Estimated 9-DoF object poses 估计的9自由度物体姿态 |
8.5 | [[8.5] 2502.02537 Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks](https://arxiv.org/abs/2502.02537) <br> [{'name': 'Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, Fei Miao'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Collaborative Object Detection<br>Uncertainty Quantification<br>Adversarial Robustness<br>Autonomous Vehicles | Input: Collaborative object detection models 协作目标检测模型<br>Step1: Adversarial training for robustness 对抗训练以增强鲁棒性<br>Step2: Uncertainty quantification estimation 不确定性量化估计<br>Step3: Calibration of uncertainty using conformal prediction 使用保形预测进行不确定性校准<br>Output: Enhanced object detection accuracy 改进的目标检测准确性 |
8.0 | [[8.0] 2502.01890 Geometric Framework for 3D Cell Segmentation Correction](https://arxiv.org/abs/2502.01890) <br> [{'name': 'Peter Chen, Bryan Chang, Olivia Annette Creasey, Julie Beth Sneddon, Yining Liu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Segmentation 3D分割<br>Geometric Framework 几何框架 | Input: 2D cell segmentation results 2D细胞分割结果<br>Step1: Extract geometric features 提取几何特征<br>Step2: Train binary classifier 训练二元分类器<br>Step3: Correct segmentation errors 修正分割错误<br>Output: Accurate 3D cell body reconstruction 精确的3D细胞体重建 |
8.0 | [[8.0] 2502.01906 Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models](https://arxiv.org/abs/2502.01906) <br> [{'name': 'Chia-Wen Kuo, Sijie Zhu, Fan Chen, Xiaohui Shen, Longyin Wen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Decomposed Attention<br>Computational Efficiency | Input: Visual and textual embeddings 视觉和文本嵌入<br>Step1: Decompose the attention mechanism 分解注意力机制<br>Step2: Optimize visual-to-visual self-attention 优化视觉间自注意力<br>Step3: Debias positional encodings 去偏差位置编码<br>Output: Enhanced processing of visual and textual embeddings 改进的视觉和文本嵌入处理 |
7.5 | [[7.5] 2502.02225 Exploring the latent space of diffusion models directly through singular value decomposition](https://arxiv.org/abs/2502.02225) <br> [{'name': 'Li Wang, Boyan Gao, Yanran Li, Zhao Wang, Xiaosong Yang, David A. Clifton, Jun Xiao'}] | Image Generation 图像生成 | v2<br>diffusion models<br>image editing<br>latent space<br>Singular Value Decomposition<br>image generation | Input: Latent space of diffusion models 扩散模型的潜在空间<br>Step1: Investigate latent space using Singular Value Decomposition (SVD) 通过奇异值分解（SVD）研究潜在空间<br>Step2: Discover properties of latent space 发现潜在空间的属性<br>Step3: Propose image editing framework based on properties 提出基于属性的图像编辑框架<br>Output: Enhanced image editing capabilities 改进的图像编辑能力 |


## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯点云辐射场<br>novel view synthesis 新视图合成 | Input: Posed 2D image data 2D图像数据<br>Step1: Extract per-image 2D segmentation masks 提取每帧的2D分割掩码<br>Step2: 2D-to-3D lifting to assign unique object IDs 在3D中分配唯一对象ID的2D到3D提升流程<br>Step3: Incremental merging of object fragments into coherent objects 将对象片段合并成一致的对象<br>Output: High-quality 3D object segments 高质量的3D对象片段 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Shape Generation 3D形状生成 | v2<br>3D reconstruction<br>shape generation<br>semantic input | Input: Semantic descriptions 语义描述<br>Step1: Distill 3D geometry from 2D diffusion models 从2D扩散模型提取3D几何<br>Step2: Refine textures using image and video generation models 使用图像和视频生成模型细化纹理<br>Step3: Represent the refined 3D model with neural implicit representations 使用神经隐式表示来表示细化的3D模型<br>Output: Fabricable high-quality meshes 可制造的高质量网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR-camera calibration<br>3D reconstruction<br>autonomous driving | Input: LiDAR and camera data 激光雷达和相机数据<br>Step1: Environment interpretation 环境解读<br>Step2: Data fusion 数据融合<br>Step3: Dual-path correspondence matching 双色通道对应匹配<br>Step4: Spatial-temporal optimization 空间-时间优化<br>Output: Accurate extrinsic calibration 精准的外部标定 |
9.5 | [[9.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>generative models<br>dynamic avatars | Input: Monocular video 单目视频<br>Step1: Generative prior usage 生成优先级使用<br>Step2: Dual-Space Optimization 双空间优化<br>Step3: View selection strategy 视图选择策略<br>Step4: Pose feature injection 姿势特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人形象 |
9.5 | [[9.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Few-Shot NeRF<br>3D Reconstruction<br>Neural Rendering | Input: Limited input views 有限的输入视角<br>Step1: Frequency control frequency control<br>Step2: Curriculum training curriculum training<br>Step3: Scene reconstruction scene reconstruction<br>Output: Accurate 3D representations 准确的三维表示 |
9.2 | [[9.2] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>hazard detection<br>vision-language model<br>autonomous driving | Input: Multimodal data fusion 多模态数据融合<br>Step1: Semantic and visual inputs integration 语义和视觉输入集成<br>Step2: Supervised fine-tuning of vision-language models 有监督微调视觉语言模型<br>Step3: Hazard detection and edge case evaluation 危险检测和边缘案例评估<br>Output: Enhanced situational awareness 改进的情境意识 |
9.2 | [[9.2] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Object Detection 3D对象检测 | v2<br>3D object detection 3D对象检测<br>monocular vision 单目视觉<br>depth estimation 深度估计 | Input: Monocular images 单目图像<br>Step1: Depth estimation using Vision Transformer 步骤1：使用视觉Transformer进行深度估计<br>Step2: Feature extraction with Hierarchical Feature Fusion 步骤2：利用层次特征融合提取特征<br>Step3: Object detection using DETR architecture 步骤3：使用DETR架构进行对象检测<br>Output: 3D bounding boxes for detected objects 输出：检测到对象的3D边界框 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 目标检测 | v2<br>4D Radar<br>3D object detection<br>energy efficiency<br>autonomous driving | Input: 4D Radar point clouds 4D雷达点云<br>Step1: Convert RTNH to SNN architecture 将RTNH转换为SNN架构<br>Step2: Implement biological top-down inference (BTI) 实现生物学自上而下推理(BTI)<br>Step3: Model evaluation and comparison 模型评估与比较<br>Output: Energy-efficient 3D object detection model 能源高效的3D目标检测模型 |
8.5 | [[8.5] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D Scene Question Answering<br>multimodal models | Input: 3D scene representation and query 3D场景表示和查询<br>Step1: Systematic literature review 系统文献综述<br>Step2: Dataset analysis 数据集分析<br>Step3: Methodology evaluation 方法评估<br>Output: Comprehensive insights and challenges on 3D SQA 对3D SQA的综合见解和挑战 |
8.5 | [[8.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Video Generation 视频生成 | v2<br>video generation<br>interpolation<br>extrapolation<br>latent flow matching | Input: Video frames 视频帧<br>Step1: Model latent flow 模型潜在流<br>Step2: Polynomial projection 多项式投影<br>Step3: Generate time-dependent frames 生成时间相关帧<br>Output: Video with interpolation and extrapolation 带插值和外推的视频 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | v2<br>3D generation<br>compositional scenes<br>large language models | Input: Complex scene descriptions 复杂场景描述<br>Step1: Semantic parsing and relationship extraction 语义解析和关系提取<br>Step2: Scene graph generation 场景图生成<br>Step3: 2D and 3D asset generation 2D和3D资产生成<br>Step4: Layout prediction and planning 布局预测与规划<br>Output: High-quality 3D compositional scenes 高质量三维组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>Vision-Language Models<br>Autonomous Driving | Input: Visual Question Answering task in autonomous driving 视觉问答任务之于自动驾驶<br>Step1: Integrate Vision-Language Models with continual learning 结合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放与知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用特定任务的投影层正则化<br>Output: Enhanced VQA performance in autonomous driving environments 改进的自动驾驶环境中的视觉问答性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning in Scenes 三维场景推理 | v2<br>3D reasoning<br>visual question answering<br>hypothetical reasoning | Input: Context change descriptions 上下文变化描述<br>Step1: Dataset construction 数据集构建<br>Step2: Model evaluation 模型评估<br>Output: Performance analysis 性能分析 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Semantic Segmentation 三维语义分割 | v2<br>3D semantic segmentation<br>domain adaptation<br>pseudo labels<br>autonomous driving | Input: 3D point cloud and SAM masks 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 类别标签确定（使用投票法）<br>Step2: Application of filtering constraints to unreliable labels 对不可靠标签应用过滤约束<br>Step3: Geometry-Aware Progressive Propagation (GAPP) for label propagation 到所有3D点进行标签传播（GAPP方法）<br>Output: Enhanced pseudo-labels and improved segmentation performance 输出：改进的伪标签和增强的分割性能 |
8.5 | [[8.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>image generation<br>video generation<br>state-space models<br>transformer models | Input: Images and video sequences 图像和视频序列<br>Step1: Develop SSM-Transformer hybrid model 开发SSM-Transformer混合模型<br>Step2: Efficient processing of visual sequences 高效处理视觉序列<br>Step3: Generate images and videos 生成图像和视频<br>Output: High-quality images and dynamic videos 高质量图像和动态视频 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>6D pose estimation<br>bin-picking<br>zero-shot learning<br>robotic manipulation | Input: RGB-D image and CAD model 输入: RGB-D图像和CAD模型<br>Step1: Object detection 物体检测<br>Step2: Point cloud extraction 点云提取<br>Step3: Position-Aware Correspondence learning 位置感知对应学习<br>Step4: Pose estimation 位置估计<br>Output: 6D pose predictions 输出: 6D姿态预测 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | v2<br>differentiable rendering<br>volumetric meshes<br>real-time rendering | Input: Volumetric mesh representations 体积网格表示<br>Step1: Mesh parameterization 网格参数化<br>Step2: Differentiable ray tracing 可微光线追踪<br>Step3: Rendering and evaluation 渲染与评估<br>Output: Real-time rendering results 实时渲染结果 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Systems and Robotics 自动驾驶机器人系统 | v2<br>road segmentation<br>autonomous vehicles<br>image registration<br>deep learning | Input: Roadside camera images 道路监控摄像头图像<br>Step1: Automatic data collection 自动数据收集<br>Step2: Semi-automatic annotation method 开发半自动注释方法<br>Step3: Image registration to correct labels 图像配准以修正标签<br>Output: Enhanced road segmentation models 改进的道路分割模型 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>Visual Inertial Odometry<br>Initialization<br>Feature Matching<br>AR<br>VR | Input: Visual Inertial Odometry (VIO) data 视觉惯性里程计数据<br>Step1: Initialization using gyroscope and visual measurements 初始化算法<br>Step2: Hybrid feature matching using optical flow and descriptor methods 特征匹配<br>Step3: Evaluation on benchmarks and practical applications 验证和实际应用<br>Output: Enhanced VIO performance 改进的VIO性能 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Autonomous Driving 自动驾驶 | v2<br>3D multi-object tracking<br>4D Radar | Input: 4D Radar data 4D雷达数据<br>Step1: Object detection using Bayesian approximation 基于贝叶斯近似进行目标检测<br>Step2: Motion prediction with transformer network 使用变换器网络进行运动预测<br>Step3: Two-stage data association integrating Doppler measurements 两阶段数据关联，整合多普勒测量<br>Output: Accurate 3D MOT results 准确的3D多目标跟踪结果 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉基础 | v2<br>3D visual grounding<br>Large Language Model<br>3D reconstruction<br>vision-language model | Input: Referring utterances and 3D scene scans 参考话语和三维场景扫描<br>Step1: Parse utterance into symbolic expression 将话语解析为符号表达式<br>Step2: Generate spatial relation features 生成空间关系特征<br>Step3: Use VLM to process visual information 使用视觉语言模型处理视觉信息<br>Output: Identified target object 确定目标对象 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | v2<br>Generative Adversarial Networks<br>Data Augmentation<br>Image Generation | Input: Limited training data 有限训练数据<br>Step 1: Estimate covariance matrices 估计协方差矩阵<br>Step 2: Identify semantic transformation directions 确定语义转换方向<br>Step 3: Apply adversarial semantic augmentation 应用对抗性语义增强<br>Output: Improved generation quality 改进的生成质量 |
7.5 | [[7.5] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>knowledge forgetting<br>general attributes | Input: Pretrained Vision-Language Models (VLMs) 预训练视觉语言模型<br>Step1: Generating General Attribute Descriptions 生成通用属性描述<br>Step2: Establishing Vision-GA-Class Associations 建立视觉-通用属性-类关联<br>Step3: Tuning Visual Encoder 调整视觉编码器<br>Output: Enhanced Adaptation with Reduced Knowledge Forgetting 改进的适应性，减少知识遗忘 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | v2<br>Diffusion Model<br>Image Generation<br>Video Generation | Input: Diffusion Model (DM) diffusion模型<br>Step1: Analyze variance and bias variance和偏差分析<br>Step2: Develop Recursive Likelihood Ratio optimizer 开发递归似然比优化器<br>Step3: Validate on image and video tasks 在图像和视频任务上验证<br>Output: Fine-tuned model 改进的模型 |
7.0 | [[7.0] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>inductive biases<br>generalization | Input: Visual and textual stimuli 视觉和文本刺激<br>Step1: Inductive bias analysis 偏置分析<br>Step2: Experimental paradigm application 实验范式应用<br>Step3: Data collection and evaluation 数据收集与评估<br>Output: Insights on model generalization 关于模型泛化的见解 |
6.5 | [[6.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal learning<br>Large Language Models<br>parameter-efficient learning<br>Vision-Language Models | Input: Vision-language models 视觉-语言模型<br>Step1: Categorize and review VLLMs 对VLLMs进行分类和审查<br>Step2: Discuss training paradigms 讨论训练范式<br>Step3: Summarize benchmarks 总结基准测试<br>Output: Comprehensive survey report 综合调查报告 |


## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建 | 3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯喷溅辐射场 | Input: 2D segmentation masks 2D分割掩码<br>Step1: Feature integration 特征集成<br>Step2: 3D Gaussian lifting 3D高斯提升<br>Step3: Segmentation application 分割应用<br>Output: 3D segmented assets 3D分割资产 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Generation 三维生成 | 3D reconstruction<br>shape generation<br>semantics | Input: Multi-view semantics 多视角语义<br>Step1: Semantic input analysis 语义输入分析<br>Step2: Geometry and appearance distillation from 2D models 从2D模型提取几何与外观<br>Step3: Image restoration and detail enhancement 图像修复与细节增强<br>Step4: Shape reconstruction using neural SDF representation 使用神经签名距离场重建形状<br>Output: Complex detailed 3D meshes 复杂细节的三维网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | LiDAR-camera calibration<br>3D reconstruction<br>data fusion | Input: LiDAR and camera data LiDAR和相机数据<br>Step1: Environmental interpretation 环境解释<br>Step2: Dual-path correspondence matching 双路径对应匹配<br>Step3: Spatial-temporal optimization 空间时间优化<br>Output: Precise extrinsic calibration 精确的外部标定 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 三维物体检测 | 3D object detection<br>neural networks<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Process high-density point clouds 处理高密度点云<br>Step2: Implement spiking neural network architecture 实现脉冲神经网络架构<br>Step3: Apply biological top-down inference (BTI) 应用生物学的自上而下推理法<br>Output: Efficient 3D object detection results 高效的三维物体检测结果 |
8.5 | [[8.5] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Driving 自动驾驶 | hazard detection<br>autonomous driving<br>multimodal data fusion | Input: Multimodal data 输入: 多模态数据<br>Step1: Data integration 数据集成<br>Step2: Hazard detection 危险检测<br>Step3: Spatial localization 空间定位<br>Output: Enhanced hazard prediction 改进的危险预测 |
8.5 | [[8.5] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Reconstruction 三维重建 | 3D object detection<br>depth estimation | Input: Monocular images 单目图像<br>Step1: Feature extraction using Vision Transformer 基于视觉变换器的特征提取<br>Step2: Depth estimation using a relative depth model 使用相对深度模型进行深度估计<br>Step3: Object detection using DETR architecture 使用DETR架构进行物体检测<br>Output: Enhanced 3D object detection capabilities 改进的3D物体检测能力 |
8.5 | [[8.5] 2502.00528 Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings](https://arxiv.org/abs/2502.00528) <br> [{'name': 'Zachary Huemann, Samuel Church, Joshua D. Warner, Daniel Tran, Xin Tie, Alan B McMillan, Junjie Hu, Steve Y. Cho, Meghan Lubner, Tyler J. Bradshaw'}] | VLM & VLA 视觉语言模型 | 3D vision-language model<br>PET/CT<br>visual grounding | Input: PET/CT reports and images PET/CT 报告和图像<br>Step1: Automation of weak labeling pipeline 弱标记生成管道自动化<br>Step2: Data extraction from reports 报告中数据提取<br>Step3: Training of ConTEXTual Net 3D 训练 ConTEXTual Net 3D<br>Output: 3D visual grounding model 3D 视觉定位模型 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | text-to-3D generation<br>compositional scenes<br>physics-guided generation | Input: Complex scene descriptions 复杂场景描述<br>Step1: Scene graph generation 场景图生成<br>Step2: Asset creation using multimodal agents 使用多模态代理进行资产创建<br>Step3: Layout prediction with physical model 使用物理模型进行布局预测<br>Output: Compositional scenes with physical rationality 具有物理合理性的组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | Vision-Language Models<br>Visual Question Answering<br>autonomous driving<br>continual learning | Input: Visual Question Answering tasks in autonomous driving 在自动驾驶中的视觉问答任务<br>Step1: Integrate Vision-Language Models with continual learning 整合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放和知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用任务特定投影层正则化<br>Output: Improved VQA system performance 改进的视觉问答系统性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning 3D推理 | 3D reasoning<br>Visual Question Answering<br>scene understanding | Input: Context changes and indoor scene descriptions 上下文变化和室内场景描述<br>Step1: Benchmark formulation 基准测试制定<br>Step2: Model evaluation models performance evaluation 模型性能评估<br>Output: Hypothetical reasoning capabilities 设想推理能力 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Reconstruction and Modeling 三维重建 | 3D semantic segmentation<br>domain adaptation<br>pseudo-labels<br>autonomous driving | Input: 3D point cloud and SAM masks 输入: 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 步骤1: 使用投票法确定类别标签<br>Step2: Unreliable mask label filtering using constraints 步骤2: 使用约束过滤不可靠的掩码标签<br>Step3: Geometry-Aware Progressive Propagation (GAPP) to propagate mask labels 步骤3: 使用几何感知逐步传播来传递掩码标签<br>Output: Enhanced pseudo-labels with improved quality 输出: 质量提升的增强伪标签 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶 | 6D pose estimation<br>bin-picking<br>robotic manipulation<br>zero-shot learning | Input: Scene instances and CAD models 场景实例与CAD模型<br>Step1: Feature extraction 特征提取<br>Step2: Position-aware correspondence learning 基于位置的对应学习<br>Step3: Pose estimation 位置估计<br>Output: Accurate 6D poses 准确的6D姿势 |
8.5 | [[8.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction 三维重建 | 3D human reconstruction<br>photorealistic rendering | Input: Monocular video 单目视频<br>Step1: Dual-Space Optimization 双空间优化<br>Step2: Score Distillation Sampling (SDS) 评分蒸馏采样<br>Step3: View Selection_strategy 视图选择策略<br>Step4: Pose Feature Injection 姿态特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人类虚拟形象 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | differentiable rendering<br>ray tracing<br>computer vision | Input: Scene representations 场景表示<br>Step1: Implement volumetric mesh ray tracing 实现体积网格光线追踪<br>Step2: Develop a novel scene representation 发展新场景表示<br>Step3: Evaluate rendering speed and quality 评估渲染速度和质量<br>Output: Real-time rendering model 实时渲染模型 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Driving 自动驾驶 | road segmentation<br>deep learning<br>autonomous vehicles<br>data annotation | Input: Roadside camera feeds 路边摄像头视频<br>Step1: Manual labeling of one frame 手动标注一帧<br>Step2: Transfer labels to other frames 转移标签到其他帧<br>Step3: Compensate for camera movements 使用频域图像配准补偿相机位移<br>Output: Semi-automatically labeled road data 半自动标注的道路数据 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Visual Odometry 视觉里程计 | Visual Inertial Odometry<br>Structure from Motion<br>Augmented Reality<br>Virtual Reality | Input: Visual inertial measurements 视觉惯性测量<br>Step1: Robust initialization initialization 稳健初始化<br>Step2: Feature matching 特征匹配<br>Step3: State estimation 状态估计<br>Output: Accurate visual inertial odometry result 精确的视觉惯性里程计结果 |
8.5 | [[8.5] 2502.01356 Quasi-Conformal Convolution : A Learnable Convolution for Deep Learning on Riemann Surfaces](https://arxiv.org/abs/2502.01356) <br> [{'name': 'Han Zhang, Tsz Lok Ip, Lok Ming Lui'}] | 3D Reconstruction and Modeling 3D重建 | 3D facial analysis<br>Riemann surfaces | Input: Geometric data and Riemann surfaces 几何数据和黎曼曲面<br>Step1: Define quasi-conformal mappings 定义准保形映射<br>Step2: Develop Quasi-Conformal Convolution operators 开发准保形卷积算子<br>Step3: Implement Quasi-Conformal Convolutional Neural Network (QCCNN) 实现准保形卷积神经网络<br>Output: Adaptive convolution for geometric data 自适应卷积用于几何数据 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Robotic Perception 机器人感知 | 3D multi-object tracking<br>Bayesian approximation<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Motion prediction using transformer-based network 使用基于变换器的网络进行运动预测<br>Step2: Bayesian approximation for detection and prediction 步骤 2: 检测和预测中的贝叶斯近似<br>Step3: Two-stage data association leveraging Doppler measurements 基于多普勒测量的两阶段数据关联<br>Output: Enhanced multi-object tracking performance 提升的多目标跟踪性能 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉定位 | 3D visual grounding<br>weakly supervised learning | Input: 3D visual information and language 3D视觉信息与语言<br>Step1: Code generation using LLM 通过LLM生成代码<br>Step2: Spatial relationship computation 空间关系计算<br>Step3: Quality evaluation and optimization 质量评估和优化<br>Output: Efficient grounding results 高效的定位结果 |
8.5 | [[8.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction 三维重建 | Few-Shot NeRFs 少样本神经辐射场<br>3D Reconstruction 三维重建 | Input: Scene images 场景图像<br>Step1: Curriculum training curriculum training 课程训练<br>Step2: Feature parameterization 特征参数化<br>Step3: Scene complexity increment 增加场景复杂性<br>Output: High-quality reconstruction 高质量重建 |
8.0 | [[8.0] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | 3D Reconstruction and Modeling 3D重建与建模 | 3D scene question answering<br>multimodal modelling<br>datasets | Input: 3D scene data 3D场景数据<br>Step1: Systematic review of datasets 数据集的系统评审<br>Step2: Analysis of methodologies 方法论分析<br>Step3: Evaluation of metrics 评估指标<br>Output: Comprehensive understanding of 3D SQA 3D场景问答的综合理解 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | Generative Adversarial Networks<br>data augmentation<br>image synthesis<br>semantic features | Input: Limited image datasets 有限图像数据集<br>Step1: Estimate covariance matrices 估计协方差矩阵<br>Step2: Identify meaningful transformation directions 识别有意义的转化方向<br>Step3: Apply transformations to semantic features 对语义特征应用转化<br>Output: Enhanced synthetic images 增强合成图像 |
7.5 | [[7.5] 2502.00333 BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution](https://arxiv.org/abs/2502.00333) <br> [{'name': 'Kai Liu, Kaicheng Yang, Zheng Chen, Zhiteng Li, Yong Guo, Wenbo Li, Linghe Kong, Yulun Zhang'}] | Image Generation 图像生成 | super-resolution<br>diffusion model<br>binarization<br>model compression | Input: Diffusion model for super-resolution 超分辨率扩散模型<br>Step1: Binarization of model models 模型的二值化<br>Step2: One-step distillation into extreme compression 一步蒸馏以实现极端压缩<br>Step3: Integration of sparse and low rank matrix branches 结合稀疏和低秩矩阵分支<br>Output: Compressed and accelerated super-resolution model 压缩和加速的超分辨率模型 |
7.5 | [[7.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Image and Video Generation 图像生成 | video generation<br>interpolation<br>extrapolation | Input: Video frames 视频帧<br>Step1: Hypothesis generation 假设生成<br>Step2: Optimal projection approximation 最优投影近似<br>Step3: Interpolation and extrapolation 插值和外推<br>Output: Time-dependent video frames 时间依赖视频帧 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | Diffusion Model<br>image generation<br>video generation | Input: Probabilistic diffusion model 概率扩散模型<br>Step1: Pre-training on unlabeled data 在无标签数据上进行预训练<br>Step2: Recursive Likelihood Ratio optimizer proposal 提出递归似然比优化器<br>Step3: Implementation of zero-order gradient estimation 零阶梯度估计的实施<br>Output: Aligned diffusion models 对齐的扩散模型 |
7.5 | [[7.5] 2502.00662 Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation](https://arxiv.org/abs/2502.00662) <br> [{'name': 'Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki'}] | VLM & VLA 视觉语言模型与对齐 | vision-language models<br>out-of-distribution detection<br>few-shot learning | Input: ID image and text prototypes 输入: ID图像和文本原型<br>Step1: Theoretical analysis 理论分析<br>Step2: Incorporation of image prototypes 图像原型的整合<br>Step3: Development of biased prompts generation (BPG) module 偏差提示生成(BPG)模块的开发<br>Step4: Implementation of image-text consistency (ITC) module 图像文本一致性(ITC)模块的实施<br>Output: Enhanced VLM-based OOD detection performance 输出: 改进的基于VLM的OOD检测性能 |
7.5 | [[7.5] 2502.00711 VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework](https://arxiv.org/abs/2502.00711) <br> [{'name': 'Chunbai Zhang, Chao Wang, Yang Zhou, Yan Peng'}] | Vision-Language Models (VLMs) 视觉语言模型 | visual reasoning<br>evidence-based reasoning<br>VLM | Input: Visual information (images/videos) 输入: 视觉信息（图像/视频）<br>Step1: Extract fine-grained visual knowledge from visual relationships 第一步: 从视觉关系中提取细粒度视觉知识<br>Step2: Paraphrase questions with underspecification using extracted knowledge 第二步: 利用提取的知识对欠规范的问题进行改写<br>Step3: Employ Chain-of-Evidence prompting for interpretable reasoning 第三步: 使用证据链提示进行可解释推理<br>Output: Enhanced visual reasoning capabilities 输出: 改进的视觉推理能力 |
7.5 | [[7.5] 2502.00719 Vision and Language Reference Prompt into SAM for Few-shot Segmentation](https://arxiv.org/abs/2502.00719) <br> [{'name': 'Kosuke Sakurai, Ryotaro Shimizu, Masayuki Goto'}] | VLM & VLA 视觉语言模型与对齐 | few-shot segmentation<br>vision-language model | Input: Annotated reference images and text labels 参考图像和文本标签<br>Step1: Input visual and semantic reference信息输入视觉和语义参考<br>Step2: Integrate prompt embeddings into SAM 将提示嵌入集成到SAM<br>Step3: Few-shot segmentation via VLP-SAM 通过VLP-SAM进行少样本分割<br>Output: High-performance segmentation results 高性能的分割结果 |
7.5 | [[7.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image Generation 图像生成 | image generation<br>video generation | Input: Visual sequences 视觉序列<br>Step1: Model development 模型开发<br>Step2: Integration of SSM and Transformers SSM与变换器的整合<br>Step3: Evaluation of generated outputs 生成结果的评估<br>Output: Generated images and videos 生成的图像和视频 |
7.5 | [[7.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | VLM & VLA 视觉语言模型与对齐 | Vision-Language<br>Large Language Models<br>parameter efficiency | Step1: Introduce architecture of LLMs 介绍LLM架构<br>Step2: Discuss parameter-efficient learning methods 讨论参数效率学习方法<br>Step3: Present taxonomy of modality integrators 提出模态集成器分类<br>Step4: Review training paradigms and efficiency considerations 回顾训练范式及效率考虑<br>Step5: Compare experimental results of representative models 比较代表模型的实验结果 |
7.5 | [[7.5] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>inductive biases<br>generalization | Input: Stimuli presented in vision and text 视觉和文本中呈现的刺激<br>Step1: Conduct experiments 进行实验<br>Step2: Analyze generalization across models 分析模型间的概括性<br>Output: Insights on inductive biases regarding shape and color 对形状和颜色的归纳偏见的见解 |
5.0 | [[5.0] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>continual adaptation<br>attribute descriptions | Input: Visual features and class text visuals 视觉特征和类别文本<br>Step1: Generate general attribute descriptions 生成一般属性描述<br>Step2: Design anchor-based embedding filter 设计基于锚点的嵌入过滤器<br>Step3: Tune visual encoder 调整视觉编码器<br>Output: Robust vision-GA-class associations 稳健的视觉-一般属性-类别关联 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent representation<br>3D高斯渲染<br>视角依赖表示 | input: images 图片<br>extend the 3D Gaussian Splatting model 扩展3D高斯渲染模型<br>introduce an additional symmetric matrix 引入额外的对称矩阵<br>achieve view-dependent opacity representation 实现视角依赖的透明度表示<br>output: improved 3D scene reconstruction 输出：改进的3D场景重建 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction 三维重建 | 3D reconstruction<br>3D Gaussian Splatting<br>endoscopic SLAM<br>depth reconstruction<br>三维重建<br>3D高斯斑点<br>内窥镜SLAM<br>深度重建 | input: endoscopic image sequences 内窥镜图像序列<br>Step 1: tracking using Gaussian Splatting 使用高斯斑点的跟踪<br>Step 2: mapping and bundle adjustment 映射与束调整<br>Step 3: surface normal-aware reconstruction 结合表面法向量进行重构<br>output: accurate 3D reconstruction and real-time tracking 输出: 精确的3D重建与实时跟踪 |
8.5 | [[8.5] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction  三维重建 | Point Cloud Completion<br>3D Shape Completion<br>Knowledge Distillation<br>Points Completion<br>点云补全<br>3D形状补全<br>知识蒸馏<br>点补全 | input: incomplete point cloud  有缺失的点云<br>step1: apply autoencoder to encode the point cloud  应用自编码器对点云进行编码<br>step2: use knowledge distillation for completion  使用知识蒸馏进行补全<br>step3: output: completed 3D shape  输出：完整的3D形状 |
8.5 | [[8.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>Gaussian Splatting<br>3D高斯喷溅<br>高斯喷溅 | Input: 2D images 2D图像<br>Ray-tracing mechanism 射线追踪机制<br>Intersection computation 交点计算<br>Ray-tracing algorithms construction 射线追踪算法构建<br>Final 3D object with lighting and shadows 最终带有光影效果的三维物体 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>3D reconstruction<br>实时渲染<br>3D高斯分喷<br>三维重建 | input: 3D key points (输入：3D关键点)<br>Step 1: Create a joint-driven 3D Gaussian representation (步骤1：创建联合驱动的3D高斯表示)<br>Step 2: Implement differentiable spatial transformations (步骤2：实现可微分的空间变换)<br>Step 3: Apply real-time shadow simulation method (步骤3：应用实时阴影模拟方法)<br>output: High-fidelity hand images (输出：高保真的手部图像) |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>物体生成<br>3D高斯 | input: 3D assets 3D资产<br>extract: physical properties 提取物理属性<br>generate: physics-based dynamics 生成基于物理的动态<br>output: dynamic scene 输出动态场景 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自动驾驶 | LiDAR<br>loop closure detection<br>graph attention networks<br>place recognition<br>semanitic registration<br>激光雷达<br>回环闭合检测<br>图注意力网络<br>地点识别<br>语义注册 | input: semantic graphs 语义图<br>step1: encode semantic graphs using graph attention networks 使用图注意力网络编码语义图<br>step2: compare graph vectors to identify loop closure 比较图向量以识别回环闭合<br>step3: estimate 6 DoF pose constraint using semantic registration 使用语义注册估计6自由度位姿约束<br>output: loop closure detection results 回环闭合检测结果 |
7.5 | [[7.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | Autonomous Driving<br>Neuromorphic Vision<br>Real-time Navigation<br>Autonomous Systems<br>自驾驶<br>神经形态视觉<br>实时导航<br>自主系统 | Input: Human speech commands 人类语音指令<br>Step 1: Translate speech into planning commands 将语音翻译成规划指令<br>Step 2: Execute commands using neuromorphic vision 执行命令使用神经形态视觉<br>Step 3: Navigate and avoid obstacles in real-time 实时导航和避免障碍<br>Output: Autonomous drone navigation output 自主无人机导航输出 |
7.5 | [[7.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | video generation<br>text-to-video models<br>视频生成<br>文本到视频模型 | input: diffusion model inputs 输入：扩散模型输入<br>step1: align video frames with text prompts 步骤1：将视频帧与文本提示对齐<br>step2: utilize a beam search strategy to optimize output 使用束搜索策略优化输出<br>step3: compute metrics for perceptual quality evaluation 计算感知质量评估的指标<br>output: high-quality, aligned video generation 输出：高质量、对齐的视频生成 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自动驾驶 | Semantic Segmentation<br>LiDAR Imaging<br>Autonomous Driving<br>合成分割<br>LiDAR成像<br>自动驾驶 | input: LiDAR data 输入: LiDAR 数据<br>step1: generate synthetic dataset 生成合成数据集<br>step2: utilize CARLA simulator 使用 CARLA 模拟器<br>step3: train segmentation algorithms 训练分割算法<br>output: improved segmentation performance 输出: 改进的分割性能 |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | personalized portrait generation<br>identity preservation<br>view-consistent reconstruction<br>个性化肖像生成<br>身份保留<br>视角一致重建 | input: reference images 参考图像<br>step1: Lighting-Aware Stitching 光照感知拼接<br>step2: View-Consistent Adaptation 视角一致自适应<br>step3: ControlNet-like supervision 控制网络样监督<br>output: personalized portraits 个性化肖像 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving (自动驾驶) | Monocular Positioning<br>Extended Kalman Filter<br>Deep Learning<br>Single-shot<br>单目定位<br>扩展卡尔曼滤波<br>深度学习<br>单次 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integrate APR and RPR using EKF 通过扩展卡尔曼滤波整合APR和RPR<br>output: accurate positioning results 精确定位结果 |
6.0 | [[6.0] 2501.19331v1 Consistent Video Colorization via Palette Guidance](http://arxiv.org/abs/2501.19331v1) | Video Generation 视频生成 | Video Colorization<br>Stable Video Diffusion<br>Palette Guidance<br>视频上色<br>稳定视频扩散<br>调色板引导 | input: video sequences 视频序列<br>step 1: design palette-based color guider 设计调色板引导器<br>step 2: utilize Stable Video Diffusion as base model 利用稳定视频扩散作为基础模型<br>step 3: generate vivid colors using color context 根据颜色上下文生成生动的颜色<br>output: colorized video sequences 上色的视频序列 |
5.5 | [[5.5] 2501.18865v1 REG: Rectified Gradient Guidance for Conditional Diffusion Models](http://arxiv.org/abs/2501.18865v1) | Image Generation 图像生成 | conditional generation<br>diffusion models<br>conditional generation 条件生成<br>扩散模型 | input: guidance techniques 指导技术<br>step1: replace the scaled marginal distribution target 替换缩放的边际分布目标<br>step2: implement rectified gradient guidance 实施矩形梯度指导<br>step3: conduct experiments on image generation tasks 进行图像生成任务的实验<br>output: improved image generation results 改进的图像生成结果 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 三维生成 | 3D Gaussian Splatting<br>Ray Tracing<br>3D高斯点云<br>光线追踪 | input: 2D images 2D图像<br>process: Gaussian Splatting 高斯点云渲染<br>process: ray tracing based on Gaussian primitives 基于高斯原始体的光线追踪<br>output: 3D objects with light and shadow effects 输出具有光影效果的3D物体 |
9.0 | [[9.0] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent rendering<br>3D高斯点云<br>视角依赖的渲染 | input: 3D scene reconstruction from images 3D场景重建从图像中提取<br>step 1: extend 3D Gaussian Splatting model 扩展3D高斯点云模型<br>step 2: introduce symmetric matrix to enhance opacity representation 引入对称矩阵以增强不透明性表示<br>step 3: optimize suppression of Gaussians based on viewer perspective 根据观察者视角优化高斯的抑制<br>output: improved representation of view-dependent reflections and specular highlights 输出：改进视角依赖的反射和镜面高光的表示 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction  三维重建 | 3D Gaussian Splatting<br>SLAM<br>endoscopic reconstruction<br>depth reconstruction<br>3D 高斯点<br>SLAM<br>内窥镜重建<br>深度重建 | input: endoscopic images 内窥镜图像<br>step1: surface normal-aware tracking 表面法线感知跟踪<br>step2: accurate mapping 精确地图构建<br>step3: bundle adjustment 捆绑调整<br>output: geometrically accurate 3D reconstruction 准确的三维重建 |
8.5 | [[8.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | Text-to-video<br>Diffusion models<br>Video generation<br>评分调整<br>文本转视频<br>扩散模型<br>视频生成<br>奖励校准 | input: video generation prompts 视频生成提示<br>step1: employ diffusion latent beam search 使用扩散潜在光束搜索<br>step2: maximize alignment reward 最大化对齐奖励<br>step3: improve perceptual quality 提升感知质量<br>output: high-quality video optimized for natural movement 输出：高质量视频，优化自然运动 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>animatable hand avatar<br>3D高斯喷涂<br>可动画手部化身 | input: 3D key points 3D关键点<br>Jointly 3D Gaussian Splatting (3DGS) joint-driven representation 联合3D高斯喷涂（3DGS）驱动表示<br>apply spatial transformations based on 3D key points 基于3D关键点应用空间变换<br>real-time rendering and shadow simulation 实时渲染和阴影模拟<br>output: animatable high-fidelity hand images 输出：可动画的高保真手部图像 |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>3D生成<br>3D高斯 | input: user-specified prompts 用户指定的提示<br>step1: define a scene according to user prompts 根据用户提示定义场景<br>step2: estimate material weighting factors using a pretrained video diffusion model 使用预训练的视频扩散模型估计材料权重因子<br>step3: represent each 3D asset as a collection of constitutive 3D Gaussians 将每个3D资产表示为一组组成的3D高斯分布<br>output: a physics-based 3D dynamic scene 输出：基于物理的3D动态场景 |
8.0 | [[8.0] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction三维重建 | Point Cloud Completion<br>Multi-view Distillation<br>3D Shape Recovery<br>点云补全<br>多视图蒸馏<br>3D形状恢复 | input: incomplete point cloud 输入: 不完整的点云<br>step1: apply autoencoder architecture 应用自编码器架构<br>step2: use knowledge distillation strategy to enhance completion 使用知识蒸馏策略以增强完成度<br>step3: output: completed point cloud 输出: 完整的点云 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自主驾驶 | Loop Closure Detection<br>Semantic Graphs<br>Graph Attention Networks<br>闭环检测<br>语义图<br>图注意力网络 | input: point cloud 输入: 点云<br>step1: encode semantic graphs using graph attention networks 步骤1: 使用图注意力网络编码语义图<br>step2: generate graph vectors through self-attention mechanisms 步骤2: 通过自注意力机制生成图向量<br>step3: compare graph vectors to detect loop closure 步骤3: 比较图向量以检测闭环<br>output: loop closure candidates 输出: 闭环候选 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自主驾驶 | Semantic segmentation<br>LiDAR imaging<br>autonomous driving<br>合成分割<br>LiDAR成像<br>自主驾驶 | input: LiDAR images (输入: LiDAR图像)<br>modify CARLA simulator (修改CARLA模拟器)<br>generate SynthmanticLiDAR dataset (生成SynthmanticLiDAR数据集)<br>evaluate with transfer learning (使用迁移学习进行评估)<br>output: improved semantic segmentation performance (输出: 改进的语义分割性能) |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | Personalized Portrait Generation<br>3D-aware relighting<br>个性化肖像生成<br>具3D感知的重光照 | Input: reference portrait images 参考肖像图像<br>Step 1: Lighting-Aware Stitching 具光照感知的拼接<br>Step 2: View-Consistent Adaptation 具视图一致的适配<br>Output: personalized portraits with identity preservation 具有身份保留的个性化肖像 |
7.0 | [[7.0] 2501.19243v1 Accelerating Diffusion Transformer via Error-Optimized Cache](http://arxiv.org/abs/2501.19243v1) | Image Generation 图像生成 | Image Generation<br>Diffusion Transformer<br>ImageNet Dataset<br>图像生成<br>扩散变换器<br>ImageNet数据集 | input: Diffusion Transformer features (扩散变换器特征)<br>extract caching differences (提取缓存差异)<br>optimize cache based on errors (基于错误优化缓存)<br>output: improved generated images (输出: 改进的生成图像) |
6.5 | [[6.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | autonomous driving<br>natural language processing<br>neuroscience<br>autonomous navigation<br>自主驾驶<br>自然语言处理<br>神经科学<br>自主导航 | input: human speech and dynamic environment 输入：人类语言和动态环境<br>step1: translate human speech into planning commands 步骤1：将人类语言翻译为规划命令<br>step2: navigate and avoid obstacles using neuromorphic vision 步骤2：利用神经形态视觉导航并避免障碍物<br>output: real-time autonomous navigation output 实时自主导航结果 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving 自主驾驶 | monocular positioning<br>extended kalman filter<br>variational bayesian inference<br>单目定位<br>扩展卡尔曼滤波<br>变分贝叶斯推理 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integration with Extended Kalman Filter (EKF) 通过扩展卡尔曼滤波整合<br>output: accurate positional predictions 准确的位置信息预测 |


## Arxiv 2025-01-30

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
8.5 | [[8.5] 2501.18594v1 Foundational Models for 3D Point Clouds: A Survey and Outlook](http://arxiv.org/abs/2501.18594v1) | 3D reconstruction 3D重建 | 3D point clouds<br>foundational models<br>3D视觉理解<br>基础模型<br>3D点云 | input: 3D point clouds 3D点云<br>step1: review of foundational models FMs 基础模型的回顾<br>step2: categorize use of FMs in 3D tasks 分类基础模型在3D任务中的应用<br>step3: summarize state-of-the-art methods 总结最新的方法<br>output: comprehensive overview of FMs for 3D understanding 输出：基础模型在3D理解中的综合概述 |
8.5 | [[8.5] 2501.18162v1 IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain](http://arxiv.org/abs/2501.18162v1) | Autonomous Driving 自动驾驶 | 3D object detection<br>autonomous driving<br>3D对象检测<br>自动驾驶 | input: roadside data and vehicle-side data<br>In-Domain Query Interaction module learns content and depth information<br>Cross-Domain Query Enhancement decouples queries into semantic and geometry parts<br>outputs enhanced object queries |
8.5 | [[8.5] 2501.18110v1 Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR Mapping Systems](http://arxiv.org/abs/2501.18110v1) | 3D reconstruction 三维重建 | 3D Mapping<br>3D Reconstruction<br>Lifelong Mapping<br>激光雷达<br>三维映射<br>三维重建<br>终身映射 | Input: Hand-held and robot-mounted LiDAR maps 输入：手持和机器人安装的激光雷达地图<br>Dynamic point removal algorithm 动态点去除算法<br>Multi-session map alignment using feature descriptor matching and fine registration 多会话地图对齐，使用特征描述符匹配和精细配准<br>Map change detection to identify changes between aligned maps 地图变化检测以识别对齐地图之间的变化<br>Map version control for maintaining current environmental state and querying changes 地图版本控制，用于维护当前环境状态和查询变化 |
8.0 | [[8.0] 2501.18595v1 ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer](http://arxiv.org/abs/2501.18595v1) | Mesh Reconstruction 网格重建 | Mesh Reconstruction<br>3D reconstruction<br>网格重建<br>三维重建 | input: limited set of images 限制的图像集<br>step1: optimize mesh geometry 优化网格几何形状<br>step2: refine mesh with spatially adaptive resolution 使用空间自适应分辨率细化网格<br>step3: reconstruct high-resolution textures 重新构建高分辨率纹理<br>output: textured mesh with detailed appearance 带有详细外观的纹理网格 |
7.5 | [[7.5] 2501.18590v1 DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models](http://arxiv.org/abs/2501.18590v1) | Rendering Techniques 渲染技术 | Inverse Rendering<br>Forward Rendering<br>Video Diffusion Models<br>Inverse渲染<br>正向渲染<br>视频扩散模型 | input: real-world videos, 真实世界视频<br>step1: estimate G-buffers using inverse rendering model, 使用逆向渲染模型估计G-buffer<br>step2: generate photorealistic images from G-buffers, 从G-buffer生成照片级真实图像<br>output: relit images, material edited images, realistic object insertions, 重新照明图像，材料编辑图像，逼真的物体插入 |
7.5 | [[7.5] 2501.18315v1 Surface Defect Identification using Bayesian Filtering on a 3D Mesh](http://arxiv.org/abs/2501.18315v1) | Mesh Reconstruction 网格重建 | 3D Mesh<br>Mesh Reconstruction<br>3D网格<br>网格重建 | input: CAD model and point cloud data 输入：CAD模型和点云数据<br>transform CAD model into polygonal mesh 将CAD模型转换为多边形网格<br>apply weighted least squares algorithm 应用加权最小二乘算法<br>estimate state based on point cloud measurements 根据点云测量估计状态<br>output: high-precision defect identification 输出：高精度缺陷识别 |
7.5 | [[7.5] 2501.17636v2 Efficient Interactive 3D Multi-Object Removal](http://arxiv.org/abs/2501.17636v2) | 3D reconstruction 三维重建 | 3D scene understanding<br>multi-object removal<br>3D场景理解<br>多对象移除 | input: selected areas and objects for removal 选定的移除区域和对象<br>step1: mask matching and refinement mask 匹配和细化掩码步骤<br>step2: homography-based warping 同伦变换基础的扭曲<br>step3: inpainting process 修复过程<br>output: modified 3D scene 修改后的3D场景 |
7.0 | [[7.0] 2501.18246v1 Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation](http://arxiv.org/abs/2501.18246v1) | 3D reconstruction  三维重建 | point cloud segmentation<br>outdoor point clouds<br>semantic segmentation<br>point cloud<br>关键点云分割<br>户外点云<br>语义分割<br>点云 | input: outdoor point clouds 户外点云<br>compute Digital Terrain Models (DTMs) 计算数字地形模型<br>employ RandLA-Net for segmentation 使用 RandLA-Net 进行分割<br>evaluate performance on datasets 评估在数据集上的表现<br>integrate relative elevation features 集成相对高程特征 |
6.5 | [[6.5] 2501.18494v1 Runway vs. Taxiway: Challenges in Automated Line Identification and Notation Approaches](http://arxiv.org/abs/2501.18494v1) | Autonomous Driving 自动驾驶 | Automated line identification 自动化线识别<br>Convolutional Neural Network 卷积神经网络<br>runway markings 跑道标记<br>autonomous systems 自动化系统<br>labeling algorithms 标记算法 | input: runway and taxiway images 跑道和滑行道图像<br>Step 1: color threshold adjustment 颜色阈值调整<br>Step 2: refine region of interest selection 精细化感兴趣区域选择<br>Step 3: integrate CNN classification 集成CNN分类<br>output: improved marking identification 改进的标记识别 |


## Newly Found Papers on ...
(Older entries get replaced automatically when the script runs again.)