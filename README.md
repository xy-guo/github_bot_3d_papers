# Daily Updates on 3D-Related Papers

This repository automatically fetches new or updated arXiv papers in the [cs.CV] category every day, checks if they are relevant to "3D reconstruction" or "3D generation" via ChatGPT, and lists them below.

## How It Works
1. A GitHub Actions workflow runs daily at 09:00 UTC.  
2. It uses the script [fetch_cv_3d_papers.py](fetch_cv_3d_papers.py) to:  
   - Retrieve the latest arXiv papers in cs.CV.  
   - Use ChatGPT to filter out those related to 3D reconstruction/generation.  
   - Update this README.md with the new findings.  
   - Send an email via 163 Mail if any relevant papers are found.  

# Paper List
## Arxiv 2025-02-18

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.10674 Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition](https://arxiv.org/abs/2502.10674) <br> [{'name': 'Khanh Nguyen, Ghulam Mubashar Hassan, Ajmal Mian'}] | 3D Object Recognition 3D物体识别 | v2<br>3D object recognition 3D物体识别<br>point clouds 点云<br>occlusion-aware遮挡感知 | Input: Synthetic 3D models from ShapeNetCore 3D模型<br>Step1: Generate partial point clouds from 3D models 从3D模型生成部分点云<br>Step2: Implement occlusion-aware pretraining 进行遮挡感知预训练<br>Step3: Evaluate recognition performance 评估识别性能<br>Output: Improved recognition accuracy 提高识别准确性 |
9.5 | [[9.5] 2502.10704 Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy](https://arxiv.org/abs/2502.10704) <br> [{'name': 'Mingyang Zhao, Gaofeng Meng, Dong-Ming Yan'}] | Point Cloud Processing 点云处理 | v2<br>non-rigid registration<br>point cloud alignment<br>occlusion handling | Input: Point cloud data 点云数据<br>Step1: Identify occluded regions 确定遮挡区域<br>Step2: Apply maximum correntropy criterion 采用最大相关熵准则<br>Step3: Optimize deformation field 优化变形场<br>Output: Accurately aligned point clouds 准确对齐的点云 |
9.5 | [[9.5] 2502.10827 E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting](https://arxiv.org/abs/2502.10827) <br> [{'name': 'Sohaib Zahid, Viktor Rudnev, Eddy Ilg, Vladislav Golyanik'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>novel view synthesis<br>event cameras<br>3D rendering<br>Gaussian splatting | Input: Event camera data 事件相机数据<br>Step1: Data processing 数据处理<br>Step2: 3D Gaussian representation construction 3D高斯表示构建<br>Step3: Novel view synthesis 利用生成新的视角<br>Output: High-quality rendered scenes 高质量渲染场景 |
9.5 | [[9.5] 2502.10842 Mobile Robotic Multi-View Photometric Stereo](https://arxiv.org/abs/2502.10842) <br> [{'name': 'Suryansh Kumar'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Multi-View Photometric Stereo<br>3D acquisition<br>Mobile Robotics | Input: Multi-view images 多视角图像<br>Step1: Supervised learning setup for predicting surface normals, object depth, and uncertainty 监督学习设置以预测表面法线、物体深度和不确定性<br>Step2: Solve MVPS-driven optimization problem to refine depth maps 解决基于MVPS的优化问题以细化深度图<br>Step3: Fuse refined depth maps while tracking camera pose 融合精细化深度图并跟踪相机位姿<br>Output: Globally consistent 3D geometry 具有全局一致性的3D几何体 |
9.5 | [[9.5] 2502.10982 TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://arxiv.org/abs/2502.10982) <br> [{'name': 'Yunfei Liu, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li'}] | 3D Reconstruction 三维重建 | v2<br>3D facial reconstruction<br>expression capture<br>neural renderer | Input: A single in-the-wild image 一张单一的野外图像<br>Step1: Extract hybrid facial parameters 提取混合面部参数<br>Step2: Design multi-scale tokenizer 设计多尺度标记器<br>Step3: Implement token-guided neural renderer 实现标记引导的神经渲染器<br>Step4: Train with token cycle loss 采用标记周期损失进行训练<br>Output: High-fidelity facial expressions output 高保真的面部表情输出 |
9.5 | [[9.5] 2502.10988 OMG: Opacity Matters in Material Modeling with Gaussian Splatting](https://arxiv.org/abs/2502.10988) <br> [{'name': 'Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie'}] | Neural Rendering 神经渲染 | v2<br>neural rendering<br>3D Gaussian Splatting<br>material modeling<br>opacity | Input: Images 图像<br>Step1: Inverse rendering process 逆向渲染过程<br>Step2: Opacity modeling 透明度建模<br>Step3: Algorithm integration 集成算法<br>Output: Improved material properties 改进的材料属性 |
9.5 | [[9.5] 2502.11390 MARS: Mesh AutoRegressive Model for 3D Shape Detailization](https://arxiv.org/abs/2502.11390) <br> [{'name': 'Jingnan Gao, Weizhe Liu, Weixuan Sun, Senbo Wang, Xibin Song, Taizhang Shang, Shenzhou Chen, Hongdong Li, Xiaokang Yang, Yichao Yan, Pan Ji'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape detailization<br>Generative Adversarial Networks (GANs)<br>geometry-consistency<br>MARS<br>autoregressive model | Input: Coarse mesh shapes 低质量网格形状<br>Step1: Tokenization of meshes 网格的标记化<br>Step2: Geometry-consistency supervision geometry-consistency 监督<br>Step3: Autoregressive detailization 自回归细节化<br>Output: Detailed meshes 细化的网格 |
9.5 | [[9.5] 2502.11618 Real-time Neural Rendering of LiDAR Point Clouds](https://arxiv.org/abs/2502.11618) <br> [{'name': 'Joni Vanherck, Brent Zoomers, Tom Mertens, Lode Jorissen, Nick Michiels'}] | Neural Rendering 神经渲染 | v2<br>Neural Rendering<br>LiDAR Point Clouds<br>Real-time Rendering | Input: LiDAR point clouds LiDAR点云<br>Step1: Point cloud projection 点云投影<br>Step2: Depth-based filtering based on heuristics 基于启发式的深度过滤<br>Step3: Final image reconstruction using U-Net 使用U-Net进行最终图像重建<br>Output: Photorealistic images of LiDAR scans LiDAR扫描的照片真实图像 |
9.5 | [[9.5] 2502.11777 Deep Neural Networks for Accurate Depth Estimation with Latent Space Features](https://arxiv.org/abs/2502.11777) <br> [{'name': 'Siddiqui Muhammad Yasir, Hyunsik Ahn'}] | Depth Estimation 深度估计 | v2<br>depth estimation<br>3D scene reconstruction | Input: RGB image to depth image mapping<br>Step1: Feature extraction using latent space<br>Step2: Dual encoder-decoder architecture<br>Step3: Introduce a novel loss function<br>Output: Enhanced depth maps with improved boundaries |
9.5 | [[9.5] 2502.11801 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency](https://arxiv.org/abs/2502.11801) <br> [{'name': 'Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang'}] | 3D Inpainting 3D修复 | v2<br>3D Gaussian Inpainting<br>Neural Radiance Field<br>multi-view consistency<br>3D reconstruction<br>computer vision | Input: Multi-view images 多视角图像<br>Step1: Infer Depth-Guided Inpainting Masks 深度引导的修复掩码推断<br>Step2: Update inpainting mask based on background pixels 更新修复掩码基于背景像素<br>Step3: Perform 3D inpainting with cross-view consistency 在视图间一致性下进行3D修复<br>Output: High-fidelity 3D inpainting results 高保真3D修复结果 |
9.5 | [[9.5] 2502.12135 MagicArticulate: Make Your 3D Models Articulation-Ready](https://arxiv.org/abs/2502.12135) <br> [{'name': 'Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, Guosheng Lin'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D models<br>articulation<br>skeleton generation<br>skinning weights | Input: Static 3D models 静态3D模型<br>Step1: Dataset creation 数据集合成<br>Step2: Skeleton generation 骨架生成<br>Step3: Skinning weight prediction 皮肤权重预测<br>Output: Articulation-ready 3D models 准备好的关节动作3D模型 |
9.5 | [[9.5] 2502.12138 FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views](https://arxiv.org/abs/2502.12138) <br> [{'name': 'Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>camera pose estimation<br>novel view synthesis | Input: Uncalibrated sparse-view images 未标定稀疏视图<br>Step1: Camera pose estimation 摄像机姿态估计<br>Step2: Geometry and appearance estimation 几何体和外观估计<br>Step3: Novel-view synthesis 新视图合成<br>Output: High-quality 3D geometry 高质量三维几何体 |
9.2 | [[9.2] 2502.10492 Multi-view 3D surface reconstruction from SAR images by inverse rendering](https://arxiv.org/abs/2502.10492) <br> [{'name': 'Emile Barbier--Renard (IDS, IMAGES), Florence Tupin (IMAGES, IDS), Nicolas Trouv\\\'e (LabHC), Lo\\"ic Denis (LabHC)'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction<br>SAR Imaging<br>Inverse Rendering<br>Deep Learning | Input: SAR images from radar sensors 合成孔径雷达图像<br>Step1: Develop a differentiable rendering model 开发可微分的渲染模型<br>Step2: Implement a coarse-to-fine MLP strategy 实施精细训练的多层感知器策略<br>Step3: Train the model on synthetic datasets 在合成数据集上训练模型<br>Output: 3D surface reconstruction results 3D表面重建结果 |
9.2 | [[9.2] 2502.10606 HIPPo: Harnessing Image-to-3D Priors for Model-free Zero-shot 6D Pose Estimation](https://arxiv.org/abs/2502.10606) <br> [{'name': 'Yibo Liu, Zhaodong Jiang, Binbin Xu, Guile Wu, Yuan Ren, Tongtong Cao, Bingbing Liu, Rui Heng Yang, Amir Rasouli, Jinjun Shan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>6D pose estimation<br>image-to-3D<br>Diffusion Models | Input: Images and scenes from robotics applications<br>Step1: Utilize image-to-3D priors to generate initial meshes<br>Step2: Estimate the 6D pose of observed objects<br>Step3: Continuously refine the mesh and pose estimation based on new observations<br>Output: Enhanced 3D mesh and accurate 6D pose estimation |
8.7 | [[8.7] 2502.11663 MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction](https://arxiv.org/abs/2502.11663) <br> [{'name': 'Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>video generation<br>mask reconstruction | Input: Video sequences 视频序列<br>Step1: Video mask reconstruction 视频掩码重建<br>Step2: Diffusion Transformer training 扩散变换器训练<br>Step3: Model evaluation 模型评估<br>Output: Generalizable driving world model 通用驾驶世界模型 |
8.7 | [[8.7] 2502.12080 HumanGif: Single-View Human Diffusion with Generative Prior](https://arxiv.org/abs/2502.12080) <br> [{'name': 'Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human reconstruction<br>novel view synthesis<br>human avatars | Input: Single-view image 单视图图像<br>Step1: Integrate generative priors from diffusion models 从扩散模型中集成生成先验<br>Step2: Implement Human NeRF module 引入Human NeRF模块<br>Step3: Optimize with image-level loss 使用图像级损失进行优化<br>Output: Novel view and pose consistent human avatars 输出: 新视图和姿态一致的人类头像 |
8.5 | [[8.5] 2502.10498 The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2502.10498) <br> [{'name': 'Sifan Tu, Xin Zhou, Dingkang Liang, Xingyu Jiang, Yumeng Zhang, Xiaofan Li, Xiang Bai'}] | Autonomous Driving 自动驾驶 | v2<br>Driving World Model<br>autonomous driving<br>scene prediction<br>3D perception | Step1: Literature review and categorization of DWM approaches 进行文献回顾并对DWM方法进行分类<br>Step2: Analysis of existing methodologies and datasets 对现有方法和数据集进行分析<br>Step3: Discussion on limitations and future directions 讨论局限性和未来方向 |
8.5 | [[8.5] 2502.10603 Adaptive Neural Networks for Intelligent Data-Driven Development](https://arxiv.org/abs/2502.10603) <br> [{'name': 'Youssef Shoeb, Azarm Nowzad, Hanno Gottschalk'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>adaptive neural networks<br>autonomous driving<br>out-of-distribution learning | Input: Autonomous driving environments 自动驾驶环境<br>Step1: Data collection 数据收集<br>Step2: Dynamic integration of new object classes 新对象类别的动态集成<br>Step3: Continuous learning 模型的持续学习<br>Output: Adaptive perception system 自适应感知系统 |
8.5 | [[8.5] 2502.10720 NPSim: Nighttime Photorealistic Simulation From Daytime Images With Monocular Inverse Rendering and Ray Tracing](https://arxiv.org/abs/2502.10720) <br> [{'name': 'Shutong Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>mesh reconstruction<br>autonomous driving<br>nighttime simulation | Input: Daytime images and semantic labels 白天图像和语义标签<br>Step1: Mesh reconstruction 网格重建<br>Step2: Relighting 重光照<br>Step3: Nighttime image simulation 夜间图像仿真<br>Output: Realistic nighttime images 真实的夜间图像 |
8.5 | [[8.5] 2502.10724 Semantics-aware Test-time Adaptation for 3D Human Pose Estimation](https://arxiv.org/abs/2502.10724) <br> [{'name': 'Qiuxia Lin, Rongyu Chen, Kerui Gu, Angela Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Human Pose Estimation<br>Test-time adaptation<br>Semantics-aware motion prior | Input: Video sequences containing human poses 视频 sequences containing human poses<br>Step1: Identify semantics from video using language models 使用语言模型识别视频中的语义<br>Step2: Integrate motion prior with semantic information 将运动先验与语义信息整合<br>Step3: Adapt 3D pose predictions during test-time adaptation (TTA) 在测试时间适应中调整3D姿势预测<br>Output: Refined 3D pose estimations 提炼的3D姿势估计 |
8.5 | [[8.5] 2502.11287 MC-BEVRO: Multi-Camera Bird Eye View Road Occupancy Detection for Traffic Monitoring](https://arxiv.org/abs/2502.11287) <br> [{'name': 'Arpitsinh Vaghela, Duo Lu, Aayush Atul Verma, Bharatesh Chakravarthi, Hua Wei, Yezhou Yang'}] | 3D Perception 3D感知 | v2<br>3D perception 3D感知<br>traffic monitoring 交通监测<br>multi-camera 多摄像头<br>occupancy detection 占用检测 | Input: Multi-camera images 多摄像头图像<br>Step1: Data acquisition 数据收集<br>Step2: Background integration 背景集成<br>Step3: Late and early fusion 方法的融合<br>Output: BEV occupancy map BEV占用图 |
8.5 | [[8.5] 2502.11307 Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection](https://arxiv.org/abs/2502.11307) <br> [{'name': 'Jiaxiang Wang, Haote Xu, Xiaolu Chen, Haodi Xu, Yue Huang, Xinghao Ding, Xiaotong Tu'}] | 3D Anomaly Detection 3D异常检测 | v2<br>anomaly detection<br>3D point cloud<br>Point-Language model | Input: 3D point clouds 3D点云<br>Step1: Dual-prompt learning 双提示学习<br>Step2: Dynamic prompt creation 动态提示创建<br>Step3: Anomaly detection 异常检测<br>Output: Enhanced anomaly detection performance 改进的异常检测性能 |
8.5 | [[8.5] 2502.11586 Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis from Japanese Haiku](https://arxiv.org/abs/2502.11586) <br> [{'name': 'Chunan Yu, Yidong Han, Chaotao Ding, Ying Zang, Lanyun Zhu, Xinhao Chen, Zejian Li, Renjun Xu, Tianrun Chen'}] | 3D Scene Generation 三维场景生成 | v2<br>3D scene synthesis<br>Japanese Haiku | Input: Japanese Haiku 日本俳句<br>Step1: Literary analysis 文学分析<br>Step2: Spatial representation 空间表现<br>Step3: 3D scene synthesis 三维场景合成<br>Output: Navigable 3D scenes 可导航三维场景 |
8.5 | [[8.5] 2502.11642 GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with Pose Guidance from Text](https://arxiv.org/abs/2502.11642) <br> [{'name': 'Gyumin Shim, Sangmin Lee, Jaegul Choo'}] | Image Generation 图像生成 | v2<br>3D human models<br>Gaussian Splatting<br>text-to-3D generation<br>animation | Input: Textual descriptions 文本描述<br>Step1: Data integration 数据集成<br>Step2: Model optimization 模型优化<br>Step3: Animation generation 动画生成<br>Output: Animatable 3D avatars 可动画的三维头像 |
8.5 | [[8.5] 2502.11697 MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow](https://arxiv.org/abs/2502.11697) <br> [{'name': 'Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang'}] | Image and Video Generation 图像生成 | v2<br>4D generation<br>multiview diffusion models<br>autonomous systems | Input: Monocular videos 单目视频<br>Step1: Generate multiview images using multiview diffusion models 利用多视角扩散模型生成多视角图像<br>Step2: Associate pixels using token flow technique 使用令牌流技术关联像素<br>Step3: Refine the coarse 4D field 细化粗糙的4D场<br>Output: High-quality 4D field 高质量4D场 |
8.5 | [[8.5] 2502.11710 The Worse The Better: Content-Aware Viewpoint Generation Network for Projection-related Point Cloud Quality Assessment](https://arxiv.org/abs/2502.11710) <br> [{'name': 'Zhiyong Su, Bingxu Xie, Zheng Li, Jincan Wu, Weiqing Li'}] | Point Cloud Processing 点云处理 | v2<br>Point Cloud Quality Assessment 点云质量评估<br>Content-Aware Viewpoint Generation 内容感知视点生成<br>Geometric Features 几何特征 | Input: Degraded point clouds 退化点云<br>Step1: Extract multi-scale geometric and texture features 提取多尺度几何和纹理特征<br>Step2: Refine features per viewpoint 针对每个视点进行特征优化<br>Step3: Generate optimized viewpoints 生成优化视角<br>Output: Optimized viewpoints for projection-related PCQA 用于投影相关PCQA的优化视角 |
8.5 | [[8.5] 2502.11726 No-reference geometry quality assessment for colorless point clouds via list-wise rank learning](https://arxiv.org/abs/2502.11726) <br> [{'name': 'Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su'}] | Geometry Quality Assessment 几何质量评估 | v2<br>geometry quality assessment<br>point clouds<br>3D reconstruction | Input: Colorless point clouds 颜色点云<br>Step1: Construct LRL dataset 生成 LRL 数据集<br>Step2: Design GQANet to extract geometric features 设计 GQANet 提取几何特征<br>Step3: Use LRLNet for ranking the quality of point clouds 使用 LRLNet 对点云品质进行排序<br>Output: Predicted geometry quality index 预测的几何质量指数 |
8.5 | [[8.5] 2502.11742 Range and Bird's Eye View Fused Cross-Modal Visual Place Recognition](https://arxiv.org/abs/2502.11742) <br> [{'name': 'Jianyi Peng, Fan Lu, Bin Li, Yuan Huang, Sanqing Qu, Guang Chen'}] | Visual Place Recognition 视觉地点识别 | v2<br>Visual Place Recognition<br>Cross-modal<br>RGB images<br>LiDAR<br>Bird's Eye View | Input: RGB images and LiDAR point clouds<br>Step1: Initial retrieval using global descriptor similarity<br>Step2: Re-ranking based on Bird's Eye View (BEV) images<br>Output: Improved Visual Place Recognition results |
8.5 | [[8.5] 2502.11864 Does Knowledge About Perceptual Uncertainty Help an Agent in Automated Driving?](https://arxiv.org/abs/2502.11864) <br> [{'name': 'Natalie Grabowsky, Annika M\\"utze, Joshua Wendland, Nils Jansen, Matthias Rottmann'}] | Autonomous Driving 自动驾驶 | v2<br>Perceptual Uncertainty<br>Reinforcement Learning<br>Automated Driving | Input: Perturbed observation space 观察空间<br>Step1: Introduce uncertainty 引入不确定性<br>Step2: Inform agent of uncertainty 通知代理不确定性<br>Step3: Reward agent for navigating safely 奖励代理安全导航<br>Output: Adjusted behavior with uncertainty 根据不确定性调整行为 |
8.5 | [[8.5] 2502.11971 Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance](https://arxiv.org/abs/2502.11971) <br> [{'name': 'Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, Jian Yang'}] | Autonomous Systems and Robotics 自动驾驶系统与机器人 | v2<br>6DoF pose tracking<br>augmented reality<br>contour-based methods<br>object tracking<br>intelligent manufacturing | Input: 6DoF object poses 6DoF 物体姿态<br>Step1: Robust contour-based tracking 方法 提出了一种基于轮廓的跟踪<br>Step2: CPU-only strategy for symmetric objects 针对对称物体的CPU仅策略<br>Step3: Unified energy function formulation 统一能量函数的表述<br>Output: Accurate tracking and assembly guidance 精确的跟踪和装配指导 |
8.5 | [[8.5] 2502.12151 VoLUT: Efficient Volumetric streaming enhanced by LUT-based super-resolution](https://arxiv.org/abs/2502.12151) <br> [{'name': 'Chendong Wang, Anlan Zhang, Yifan Yang, Lili Qiu, Yuqing Yang, Xinyang Jiang, Feng Qian, Suman Banerjee'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D volumetric video<br>super-resolution<br>bandwidth reduction<br>lookup tables (LUTs) | Input: Low-resolution volumetric data 低分辨率体积数据<br>Step1: Downsampling data to reduce bandwidth 数据下采样以减少带宽<br>Step2: Applying super-resolution algorithm to upscale data 应用超分辨率算法对数据进行上采样<br>Step3: Utilizing lookup tables (LUTs) for efficient processing 使用查找表 (LUTs) 进行高效处理<br>Output: Enhanced volumetric video for streaming 改进的体积视频用于流传输 |
7.8 | [[7.8] 2502.10444 A Survey of Representation Learning, Optimization Strategies, and Applications for Omnidirectional Vision](https://arxiv.org/abs/2502.10444) <br> [{'name': 'Hao Ai, Zidong Cao, Lin Wang'}] | 3D Geometry and Motion Estimation 3D几何与运动估计 | v2<br>Omnidirectional vision<br>Deep learning<br>3D geometry<br>Autonomous driving | Input: Omnidirectional images 全景图像<br>Step 1: Literature review 文献综述<br>Step 2: Challenges and complexities analysis 挑战与复杂性分析<br>Step 3: Taxonomy development 分类法开发<br>Objective: Summarize DL methods for omnidirectional vision 总结全景视觉的深度学习方法 |
7.5 | [[7.5] 2502.12095 Descriminative-Generative Custom Tokens for Vision-Language Models](https://arxiv.org/abs/2502.12095) <br> [{'name': 'Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, Stefano Soatto'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>image retrieval<br>custom tokens | Input: Concept images and text 描述概念的图像和文本<br>Step1: Learn custom tokens 学习自定义token<br>Step2: Align text and image features 对齐文本和图像特征<br>Step3: Use in VLMs 应用于视觉语言模型<br>Output: Improved query performance 改进的查询性能 |


## Arxiv 2025-02-17

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.2 | [[9.2] 2502.09672 IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter](https://arxiv.org/abs/2502.09672) <br> [{'name': 'Xiaohong Liu, Xulong Zhao, Gang Liu, Zili Wu, Tao Wang, Lei Meng, Yuhan Wang'}] | 3D Multi-Object Tracking 3D多目标跟踪 | v2<br>3D Multi-Object Tracking<br>Interacting Multiple Model filter<br>3D point clouds | Input: 3D point clouds and images 3D点云和图像<br>Step1: Damping Window mechanism for trajectory management 轨迹管理的阻尼窗口机制<br>Step2: Interacting Multiple Model filter for dynamic tracking 动态跟踪的交互多个模型滤波器<br>Step3: Distance-Based Score Enhancement for detection scores 检测分数的基于距离的增强<br>Output: Enhanced 3D multi-object tracking system 改进的3D多目标跟踪系统 |
9.0 | [[9.0] 2502.09980 V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models](https://arxiv.org/abs/2502.09980) <br> [{'name': 'Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen'}] | Autonomous Driving 自动驾驶 | v2<br>Autonomous Driving<br>Cooperative Perception<br>Large Language Models | Input: Perception information from multiple CAVs 从多个CAV获取感知信息<br>Step1: Data integration 数据集成<br>Step2: LLM-based fusion 方法：基于LLM的特征融合<br>Step3: Question answering 问题回答<br>Output: Driving-related answers 驾驶相关答案 |
8.5 | [[8.5] 2502.09652 GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing](https://arxiv.org/abs/2502.09652) <br> [{'name': 'Lei (Rachel),  Chen, Juheon Lee, Juan Carlos Catana, Tsegai Yhdego, Nathan Moroney, Mohammad Amin Nabian, Hui Wang, Jun Zeng'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D printing 3D 打印<br>shape deviation 形状偏差<br>additive manufacturing 增材制造 | Input: Point cloud data 点云数据<br>Step1: Integrate positional factors 集成位置因素<br>Step2: Develop compensation algorithms 开发补偿算法<br>Step3: Validate and refine with experimental data 验证和完善实验数据<br>Output: Enhanced shape accuracy 改进的形状精度 |
8.5 | [[8.5] 2502.09669 Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation](https://arxiv.org/abs/2502.09669) <br> [{'name': 'Maizhe Yang, Kaiyuan Tang, Chaoli Wang'}] | Volumetric Reconstruction 体积重建 | v2<br>implicit neural representation<br>volumetric data<br>meta-learning<br>3D reconstruction<br>volume rendering | Input: Volumetric dataset 体积数据集<br>Step1: Meta-pretraining on subsampled data 亚采样数据上的元预训练<br>Step2: Volume-specific finetuning on complete data 对完整数据的卷特定微调<br>Output: Adapted implicit neural representations (INRs) 调整后的隐式神经表征 |
8.5 | [[8.5] 2502.09795 Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions](https://arxiv.org/abs/2502.09795) <br> [{'name': 'Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis'}] | Autonomous Systems and Robotics 自动驾驶机器人 | v2<br>Map-based Localization<br>Mars<br>image registration<br>deep learning | Input: Onboard images and reference map<br>Step1: Development of Geo-LoFTR model<br>Step2: Incorporation of geometric context<br>Step3: Simulation of Martian terrain<br>Output: Enhanced localization accuracy |
8.5 | [[8.5] 2502.10028 ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation](https://arxiv.org/abs/2502.10028) <br> [{'name': 'Yuxin He, Qiang Nie'}] | 3D Flow and Action Prediction 3D流和动作预测 | v2<br>3D flow<br>action prediction<br>robotic manipulation | Input: Language instructions and video data 语言指令和视频数据<br>Step 1: 3D flow prediction 3D流预测<br>Step 2: Model training using causal transformer 使用因果变换器训练模型<br>Output: Fine-grained action predictions and future image generation 输出: 精细的动作预测和未来图像生成 |
8.5 | [[8.5] 2502.10059 RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control](https://arxiv.org/abs/2502.10059) <br> [{'name': 'Teng Li, Guangcong Zheng, Rui Jiang,  Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>image-to-video generation<br>3D scene reconstruction<br>camera control<br>depth estimation | Input: Monocular images 单目图像<br>Step1: Depth estimation 深度估计<br>Step2: 3D scene reconstruction 3D场景重建<br>Step3: Camera trajectory scaling 相机轨迹缩放<br>Output: Interactive video generation 交互式视频生成 |
8.5 | [[8.5] 2502.10127 Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation](https://arxiv.org/abs/2502.10127) <br> [{'name': 'Gamal Elghazaly, Raphael Frank'}] | Autonomous Driving 自动驾驶 | v2<br>Collaboration<br>HD maps<br>V2X<br>Scene Graph Generation | Input: Front-facing camera images 前视相机图像<br>Step1: Extract lane centerlines from images 从图像中提取车道中心线<br>Step2: Represent lane centerlines as directed graphs 将车道中心线表示为有向图<br>Step3: Transmit data to the cloud via V2X 通过V2X将数据传输到云端<br>Output: Generated localized HD map 生成的局部高清地图 |
8.5 | [[8.5] 2502.10377 ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences](https://arxiv.org/abs/2502.10377) <br> [{'name': 'Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni'}] | 3D Generation 三维生成 | v2<br>3D reconstruction<br>style transfer<br>multi-view consistency | Input: Multi-view images 多视角图像<br>Step1: Style transfer to a single view using semantic attention mechanism 在单视图上使用语义注意机制进行风格转移<br>Step2: Lift stylization to additional views using warp-and-refine network 通过变换和细化网络将风格提升到其他视图<br>Output: Consistent stylized results across multiple views 在多个视图中获得一致的风格化结果 |
8.5 | [[8.5] 2502.10392 Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding](https://arxiv.org/abs/2502.10392) <br> [{'name': 'Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu'}] | 3D Visual Grounding 3D视觉定位 | v2<br>3D visual grounding 3D视觉定位<br>sparse convolution 稀疏卷积<br>text features 文本特征 | Input: 3D scene representation and text features 3D场景表示和文本特征<br>Step1: Text-guided pruning to sparsify the 3D voxel features 文本引导的修剪以减少3D体素特征<br>Step2: Completion-based addition to address over-pruned areas 基于补全的添加以解决过度修剪区域<br>Output: Efficiently fused features for 3D visual grounding 高效融合的特征用于3D视觉定位 |
8.0 | [[8.0] 2502.10273 Probing Perceptual Constancy in Large Vision Language Models](https://arxiv.org/abs/2502.10273) <br> [{'name': 'Haoran Sun, Suyang Yu, Yijiang Li, Qingying Gao, Haiyun Lyu, Hokin Deng, Dezhi Luo'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>perceptual constancy<br>vision-language models<br>VLMs<br>cognitive tasks | Input: Vision-Language Models (VLMs) 视觉语言模型<br>Step1: Evaluation using cognitive experiments 使用认知实验进行评估<br>Step2: Testing across dimensions of perceptual constancy 在感知恒常性的各个维度进行测试<br>Step3: Analysis of model variability in performance 对模型性能的变异性进行分析<br>Output: Insights into perceptual constancy capabilities of VLMs 输出: 对VLMs感知恒常性能力的洞察 |
7.5 | [[7.5] 2502.09818 On the robustness of multimodal language model towards distractions](https://arxiv.org/abs/2502.09818) <br> [{'name': 'Ming Liu, Hao Chen, Jindong Wang, Wensheng Zhang'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models (VLMs) 视觉语言模型<br>Robustness of Models 模型鲁棒性 | Input: Vision-language models (VLMs) 视觉语言模型<br>Step1: Develop a benchmark 数据集开发<br>Step2: Introduce distractions in visual and textual inputs 输入中引入干扰<br>Step3: Evaluate model robustness 评估模型鲁棒性<br>Output: Insights on VLM performance 视觉语言模型性能洞察 |


## Arxiv 2025-02-14

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.08902 CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics for Metric 3D Shape Recovery](https://arxiv.org/abs/2502.08902) <br> [{'name': 'Chenghao Zhang, Lubin Fan, Shen Cao, Bojian Wu, Jieping Ye'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape recovery<br>depth estimation<br>camera calibration | Input: Single image 单幅图像<br>Step1: Depth estimation 深度估计<br>Step2: Camera intrinsics estimation 相机内参估计<br>Step3: Collaborative optimization 协同优化<br>Output: Metric 3D shape metric 3D 形状 |
9.5 | [[9.5] 2502.09111 DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance Prior](https://arxiv.org/abs/2502.09111) <br> [{'name': 'Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang'}] | SLAM 同时定位与地图构建 | v2<br>SLAM<br>Neural Radiance Fields<br>3D Reconstruction<br>Gaussian Splatting | Input: RGB-D stream of frames RGB-D帧流<br>Step1: Camera pose and neural radiance fields optimization 相机位姿和神经辐射场优化<br>Step2: Initialize Gaussian primitives using implicit radiance fields based on sampled points 使用样本点的隐式辐射场初始化高斯原语<br>Step3: Implement local loop closure detection and bundle optimization 进行局部闭环检测和捆绑优化<br>Output: Enhanced Gaussian maps with improved tracking and mapping performance 输出：具有改进跟踪和映射性能的增强高斯地图 |
9.5 | [[9.5] 2502.09274 FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation](https://arxiv.org/abs/2502.09274) <br> [{'name': 'Bin Yang, Alexandru Paul Condurache'}] | 3D Scene Understanding 3D场景理解 | v2<br>3D scene understanding<br>LiDAR<br>semantic segmentation<br>autonomous driving | Input: LiDAR point clouds LiDAR点云<br>Step1: Redesign data representation 重新设计数据表示<br>Step2: Implement data augmentation 实施数据增强<br>Step3: Apply post-processing methods 应用后处理方法<br>Output: Enhanced semantic segmentation performance 提升的语义分割性能 |
9.5 | [[9.5] 2502.09278 ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View Gaussian Optimization](https://arxiv.org/abs/2502.09278) <br> [{'name': 'Onat \\c{S}ahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>image-to-3D<br>mesh generation | Input: Multi-view images 多视角图像<br>Step1: Generate multi-view prior images 生成多视角先验图像<br>Step2: Use score distillation sampling (SDS) to guide view generation 使用得分蒸馏采样引导视图生成<br>Step3: Optimize rough shape and fine details 优化粗形状和细节<br>Output: View-consistent 3D mesh 视图一致的三维网格 |
9.5 | [[9.5] 2502.09425 A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone Scans with Deep Learning Based Methods Using Geometry and Morphometry Criteria](https://arxiv.org/abs/2502.09425) <br> [{'name': "\\'Alvaro Heredia-Lid\\'on, Alejandro Mo\\~nux-Bernal, Alejandro Gonz\\'alez, Luis M. Echeverry-Quiceno, Max Rubert, Aroa Casado, Mar\\'ia Esther Esteban, Mireia Andreu-Montoriol, Susanna Gallardo, Cristina Ruffo, Neus Mart\\'inez-Abad\\'ias, Xavier Sevillano"}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D facial reconstruction<br>morphometric analysis<br>deep learning | Input: Smartphone-based 3D scans and deep learning models 智能手机3D扫描与深度学习模型<br>Step1: Data acquisition 数据采集<br>Step2: Morphometric shape analysis morphometric形状分析<br>Step3: Comparison with ground truth 比较真实模型<br>Output: Evaluation of global and local shape differences 输出：全球和局部形状差异的评估 |
9.5 | [[9.5] 2502.09563 Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction](https://arxiv.org/abs/2502.09563) <br> [{'name': 'Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec'}] | 3D Reconstruction 三维重建 | v2<br>3D Reconstruction 三维重建<br>Camera Calibration 相机校准<br>Gaussian Splatting 高斯点云 | Input: Wide-angle images 广角图像<br>Step1: Optimize camera parameters 优化相机参数<br>Step2: Model lens distortion 建模镜头畸变<br>Step3: Use Gaussian representations 使用高斯表示<br>Step4: Resample with cubemap strategy 使用立方映射策略<br>Output: Accurate 3D scene reconstruction 准确的三维场景重建 |
9.5 | [[9.5] 2502.09613 Latent Radiance Fields with 3D-aware 2D Representations](https://arxiv.org/abs/2502.09613) <br> [{'name': 'Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>latent representations<br>photorealistic rendering | Input: 2D latent representations 2D 潜在表示<br>Step1: Enhance 3D consistency with correspondence-aware autoencoding 方法1: 使用对应感知自编码增强3D一致性<br>Step2: Lift 3D-aware representations into 3D space 方法2: 将3D感知表示提升至3D空间<br>Step3: Align VAE-Radiance Fields for image decoding 方法3: 对齐VAE-放射场以进行图像解码<br>Output: Photorealistic 3D reconstruction output 照片真实的3D重建输出 |
9.5 | [[9.5] 2502.09615 RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets](https://arxiv.org/abs/2502.09615) <br> [{'name': 'Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D assets<br>autoregressive modeling<br>automatic rigging | Input: 3D asset shapes 3D资产形状<br>Step1: Joint probabilistic generation 关节概率生成<br>Step2: Skeleton topology prediction 骨架拓扑预测<br>Step3: Skinning weights assignment 绑定权重分配<br>Output: Rigged 3D asset 装配好的3D资产 |
9.5 | [[9.5] 2502.09623 Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF Architectures](https://arxiv.org/abs/2502.09623) <br> [{'name': 'Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D representation<br>Graph Meta-Networks | Input: Neural Radiance Fields (NeRFs) 神经辐射场<br>Step1: Train a Graph Meta-Network 训练图元网络<br>Step2: Apply contrastive learning 施加对比学习<br>Step3: Perform classification and retrieval tasks 执行分类和检索任务<br>Output: Architecture-agnostic representations 架构无关表示 |
8.8 | [[8.8] 2502.09620 Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620) <br> [{'name': 'Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D LMMs<br>Encoder-free architectures 3D LMMs<br>3D understanding 3D 理解 | Input: 3D point clouds 3D 点云<br>Step1: Semantic Encoding in pre-training 阶段的语义编码<br>Step2: Hierarchical Geometry Aggregation in tuning 调优中的层次几何聚合<br>Output: Encoder-free 3D LMM 编码器自由 3D LMM |
8.5 | [[8.5] 2502.08884 ShapeLib: designing a library of procedural 3D shape abstractions with Large Language Models](https://arxiv.org/abs/2502.08884) <br> [{'name': 'R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape representation<br>procedural modeling<br>Large Language Models | Input: Design intent (text descriptions, seed shapes) 设计意图（文本描述，种子形状）<br>Step1: Library interface design 库接口设计<br>Step2: Function application proposing 函数应用提出<br>Step3: Function implementation formulation 函数实现制定<br>Step4: Geometric validation of functions 几何验证函数<br>Output: Library of procedural shape functions 程序化形状函数库 |
8.5 | [[8.5] 2502.08974 Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning](https://arxiv.org/abs/2502.08974) <br> [{'name': 'Yiming Yang, Yueru Luo, Bingkun He, Erlong Li, Zhipeng Cao, Chao Zheng, Shuqi Mei, Zhen Li'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>lane topology<br>autonomous driving<br>topology reasoning | Input: Perspective views (PV) from cameras<br>Step1: Extract lane topology sequences from PV<br>Step2: Implement dual-decoder architecture for segment and topology decoding<br>Step3: Utilize randomized order prompt-to-sequence learning<br>Output: Enhanced lane topology sequences for autonomous driving |
8.5 | [[8.5] 2502.08977 Text-driven 3D Human Generation via Contrastive Preference Optimization](https://arxiv.org/abs/2502.08977) <br> [{'name': 'Pengfei Zhou, Xukun Shen, Yong Hu'}] | 3D Generation 三维生成 | v2<br>3D human generation<br>text-driven<br>contrastive preferences | Input: Textual descriptions 文本描述<br>Step1: Preference optimization module 偏好优化模块<br>Step2: Integration of multiple preference models 多个偏好模型的集成<br>Step3: Negation preference module 引入否定偏好模块<br>Output: Enhanced 3D human models 改进的三维人类模型 |
8.5 | [[8.5] 2502.09039 Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting](https://arxiv.org/abs/2502.09039) <br> [{'name': 'Lingting Zhu, Guying Lin, Jinnan Chen, Xinjie Zhang, Zhenchao Jin, Zhao Wang, Lequan Yu'}] | 3D Reconstruction and Modeling 三维重建 | Gaussian Splatting<br>3D reconstruction<br>image representation | Input: Large images 大图像<br>Step1: Gaussian point fitting 高斯点拟合<br>Step2: Optimization strategy 优化策略<br>Step3: Level-of-Gaussian reconstruction 高斯层次重建<br>Output: High-quality image representations 高质量图像表示 |
8.5 | [[8.5] 2502.09057 Vision-Language In-Context Learning Driven Few-Shot Visual Inspection Model](https://arxiv.org/abs/2502.09057) <br> [{'name': 'Shiryu Ueno, Yoshikazu Hayashi, Shunsuke Nakatsuka, Yusei Yamada, Hiroaki Aizawa, Kunihito Kato'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Inspection<br>Vision-Language Model<br>In-Context Learning | Input: Few-shot images of products 产品的少量图像<br>Step1: Construct dataset 创建数据集<br>Step2: Fine-tune VLM for inspection 对VLM进行微调以进行检查<br>Step3: Perform visual inspection using In-Context Learning 使用In-Context Learning进行视觉检查<br>Output: Inspection results and defective location detection 检查结果及缺陷位置检测 |
8.5 | [[8.5] 2502.09080 BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization](https://arxiv.org/abs/2502.09080) <br> [{'name': 'Qiwei Wang, Shaoxun Wu, Yujiao Shi'}] | Cross-View Localization 跨视角定位 | v2<br>3D Gaussian primitives<br>cross-view localization<br>autonomous driving | Input: Ground image and satellite image 地面图像与卫星图像<br>Step1: Generate 3D Gaussian primitives 生成三维高斯原语<br>Step2: Synthesize BEV feature map 合成鸟瞩视图特征图<br>Step3: Conduct pose estimation 进行姿态估计<br>Output: Location probability map of the query image 查询图像的位置信息图 |
8.5 | [[8.5] 2502.09528 SteROI-D: System Design and Mapping for Stereo Depth Inference on Regions of Interest](https://arxiv.org/abs/2502.09528) <br> [{'name': 'Jack Erhardt, Ziang Li, Reid Pinkham, Andrew Berkovich, Zhengya Zhang'}] | Multi-view Stereo 多视角立体 | v2<br>Stereo Depth<br>Region of Interest<br>Energy Efficiency<br>AR/VR<br>Dynamic ROIs | Input: Stereo images 立体图像<br>Step1: ROI identification ROI识别<br>Step2: Depth estimation depth estimation<br>Step3: Energy optimization 能耗优化<br>Output: Efficient depth maps 高效深度图 |
8.5 | [[8.5] 2502.09617 LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh](https://arxiv.org/abs/2502.09617) <br> [{'name': 'Jing Wen, Alexander G. Schwing, Shenlong Wang'}] | Neural Rendering 神经渲染 | v2<br>3D reconstruction<br>human rendering<br>computational efficiency | Input: Sparse source images稀疏源图像<br>Step1: Iterative feedback update iterative feedback update<br>Step2: Coupled multi-resolution Gaussians-on-Mesh representation耦合多分辨率高斯-网格表示<br>Output: Animatable human representation可动画人类表示 |
7.5 | [[7.5] 2502.09075 PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration](https://arxiv.org/abs/2502.09075) <br> [{'name': 'Jinhui Guo, Lubin Fan, Bojian Wu, Jiaqi Gu, Shen Cao, Jieping Ye'}] | Camera Calibration 相机校准 | v2<br>PTZ calibration<br>camera parameters<br>3D information | Input: Reference images 参考图像<br>Step1: Image selection 图像选择<br>Step2: Apply PTZ-IBA algorithm 应用PTZ增量束调整算法<br>Step3: Parameter optimization 参数优化<br>Output: Calibrated camera parameters 校准的相机参数 |
7.5 | [[7.5] 2502.09088 Unsupervised Anomaly Detection on Implicit Shape representations for Sarcopenia Detection](https://arxiv.org/abs/2502.09088) <br> [{'name': 'Louise Piecuch (MD), Jeremie Huet (MD), Antoine Frouin (PT), Antoine Nordez (MD), Anne-Sophie Boureau (MD), Diana Mateus'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anomaly detection<br>implicit neural representation<br>sarcopenia | Input: Muscle shape data 肌肉形状数据<br>Step1: Model normal muscle shapes using implicit neural representation (INR) 使用隐式神经表征建模正常肌肉形状<br>Step2: Employ unsupervised anomaly detection based on reconstruction error 使用基于重建误差的无监督异常检测<br>Step3: Classify and separate normal and sarcopenic muscles from learned representations 对学习的表示进行分类和分离正常与肌肉萎缩肌肉<br>Output: Anomaly detection results for sarcopenic and non-sarcopenic muscles 输出：肌肉萎缩及非肌肉萎缩的异常检测结果 |


## Arxiv 2025-02-13

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.07822 PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation](https://arxiv.org/abs/2502.07822) <br> [{'name': 'Ao Liang, Haiyang Hua, Jian Fang, Wenyu Chen, Huaici Zhao'}] | 3D Object Detection 三维物体检测 | v2<br>3D object detection<br>Point Dilation Mechanism<br>autonomous driving | Input: Point cloud data 点云数据<br>Step1: Efficient feature encoding using PointNet-style backbone 使用PointNet风格的骨干网进行高效特征编码<br>Step2: Point Dilation Mechanism (PDM) to expand feature space 使用点膨胀机制（PDM）扩展特征空间<br>Step3: Hybrid detection head for joint learning 设计混合检测头进行联合学习<br>Output: Enhanced 3D object detection results 改进的三维物体检测结果 |
9.5 | [[9.5] 2502.07840 TranSplat: Surface Embedding-guided 3D Gaussian Splatting for Transparent Object Manipulation](https://arxiv.org/abs/2502.07840) <br> [{'name': 'Jeongyun Kim, Jeongho Noh, Dong-Guw Lee, Ayoung Kim'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>transparent object manipulation<br>depth completion<br>latent diffusion model<br>robotics | Input: RGB images and surface embeddings RGB图像和表面嵌入<br>Step1: Generate surface embeddings using a latent diffusion model 使用潜在扩散模型生成表面嵌入<br>Step2: Jointly optimize Gaussian splatting with RGB images and surface embeddings 与RGB图像和表面嵌入共同优化高斯点云<br>Step3: Render depth for object manipulation 渲染深度以进行物体操作<br>Output: Accurate depth completion for transparent objects 为透明物体提供准确的深度完成 |
9.5 | [[9.5] 2502.07869 EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera](https://arxiv.org/abs/2502.07869) <br> [{'name': 'Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Alain Pagani, Didier Stricker, Christian Theobalt, Vladislav Golyanik'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D human motion capture<br>event cameras<br>egocentric vision | Input: Monocular event camera with fisheye lens 单眼事件相机与鱼眼镜头<br>Step1: Data acquisition from event camera 数据采集<br>Step2: Integration of RGB and event data RGB与事件数据集成<br>Step3: Algorithm development for pose estimation 算法开发以估计姿势<br>Step4: Real-time processing and 3D reconstruction 实时处理与三维重建<br>Output: Accurate 3D human motion capture 精确的三维人类运动捕捉 |
9.5 | [[9.5] 2502.08169 CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus](https://arxiv.org/abs/2502.08169) <br> [{'name': 'Yunjiang Xu, Lingzhi Li, Jin Wang, Benyuan Yang, Zhiwen Wu, Xinhong Chen, Jianping Wang'}] | 3D Object Detection 三维物体检测 | v2<br>3D detection 三维检测<br>autonomous driving 自动驾驶<br>collaborative perception 协同感知 | Input: Sensor data from LiDAR and cameras 传感器数据来自LiDAR和相机<br>Step1: Evaluate dynamic feature trust modulus (DFTM) 评估动态特征信任模数 (DFTM)<br>Step2: Implement multi-scale fusion method 实现多尺度融合方法<br>Step3: Validate performance through extensive experiments 通过广泛实验验证性能<br>Output: Enhanced robustness in 3D object detection 提高三维物体检测的鲁棒性 |
9.5 | [[9.5] 2502.08285 Fully-Geometric Cross-Attention for Point Cloud Registration](https://arxiv.org/abs/2502.08285) <br> [{'name': 'Weijie Wang, Guofeng Mei, Jian Zhang, Nicu Sebe, Bruno Lepri, Fabio Poiesi'}] | 3D Reconstruction 三维重建 | v2<br>Point Cloud Registration 点云配准<br>Geometric Attention 几何注意力<br>Transformer Network 变换网络 | Input: Point clouds 输入: 点云<br>Step1: Cross-attention mechanism development 步骤1: 交叉注意力机制开发<br>Step2: Integration of Gromov-Wasserstein distance into attention 步骤2: 将Gromov-Wasserstein距离集成到注意力机制中<br>Step3: Point feature aggregation through self-attention 步骤3: 通过自注意力聚合点特征<br>Output: Enhanced point cloud registration results 输出: 改进的点云配准结果 |
9.5 | [[9.5] 2502.08352 Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images with Depth and Normal Supervision](https://arxiv.org/abs/2502.08352) <br> [{'name': 'Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>satellite imagery<br>neural networks | Input: Multi-view satellite images 多视角卫星图像<br>Step1: Incorporate explicit depth guidance 引入显式深度指导<br>Step2: Apply surface normal consistency constraints 应用表面法线一致性约束<br>Step3: Utilize a multi-resolution hash grid for efficient reconstruction 使用多分辨率哈希网格进行高效重建<br>Output: Accurate 3D models from satellite images 从卫星图像获得精准的三维模型 |
8.5 | [[8.5] 2502.07829 Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing](https://arxiv.org/abs/2502.07829) <br> [{'name': 'Sihao Wu, Xiaonan Si, Chi Xing, Jianhong Wang, Gaojie Jin, Guangliang Cheng, Lijun Zhang, Xiaowei Huang'}] | Image Generation 图像生成 | v2<br>diffusion models<br>image generation<br>preference alignment<br>autonomous driving | Input: Integration of preference alignment with diffusion models 偏好对齐与扩散模型的结合<br>Step1: Systematic review of optimization techniques 对优化技术进行系统回顾<br>Step2: Exploration of applications across various fields 在多个领域探索应用<br>Step3: Discussion of challenges in preference alignment 讨论偏好对齐中的挑战<br>Output: Insights for future innovation 未来创新的洞察 |
8.5 | [[8.5] 2502.08377 Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling Dynamic-Static Features](https://arxiv.org/abs/2502.08377) <br> [{'name': 'Liying Yang, Chen Liu, Zhenwei Zhu, Ajian Liu, Hui Ma, Jian Nong, Yanyan Liang'}] | 3D Generation 三维生成 | v2<br>4D generation<br>dynamic-static features<br>computer vision | Input: Video frames 视频帧<br>Step1: Feature extraction 特征提取<br>Step2: Dynamic-static feature decoupling 动态静态特征解耦<br>Step3: Temporal-spatial similarity fusion 在时间-空间上选择相似特征<br>Output: 4D content generation 4D内容生成 |
8.5 | [[8.5] 2502.08639 CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation](https://arxiv.org/abs/2502.08639) <br> [{'name': 'Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai'}] | Image and Video Generation 图像生成 | v2<br>3D-aware<br>text-to-video generation<br>depth maps<br>camera trajectories | Input: User-defined scene parameters 用户定义的场景参数<br>Step1: Interactive workflow for 3D control 3D控制的交互工作流程<br>Step2: Condition signal construction 条件信号构建<br>Step3: Text-to-video generation from control signals 基于控制信号的文本生成视频<br>Output: Generated controllable video 输出: 生成的可控视频 |
8.0 | [[8.0] 2502.08374 AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception](https://arxiv.org/abs/2502.08374) <br> [{'name': 'Yuanhao Huang, Qinfan Zhang, Jiandong Xing, Mengyue Cheng, Haiyang Yu, Yilong Ren, Xiao Xiong'}] | Autonomous Driving 自动驾驶 | v2<br>adversarial attack<br>autonomous driving<br>information swapping | Input: Autonomous vehicle images 自动驾驶车辆图像<br>Step1: Information swapping 信息交换<br>Step2: Adversarial sample generation 对抗样本生成<br>Step3: Evaluation on datasets 在数据集上评估<br>Output: Robust adversarial samples 稳健的对抗样本 |
7.5 | [[7.5] 2502.08646 Poly-Autoregressive Prediction for Modeling Interactions](https://arxiv.org/abs/2502.08646) <br> [{'name': 'Neerja Thakkar, Tara Sadjadpour, Jathushan Rajasegaran, Shiry Ginosar, Jitendra Malik'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous vehicles<br>trajectory prediction<br>multi-agent interactions<br>behavior forecasting | Input: Ego agent's state history and states of other interacting agents 自我代理的状态历史和其他交互代理的状态<br>Step1: Model behavior as a sequence of tokens 将行为建模为状态序列<br>Step2: Use a transformer for prediction 使用变压器进行预测<br>Step3: Apply to different prediction tasks 应用到不同的预测任务<br>Output: Predicted future behavior of the ego agent 输出自我代理的未来行为预测 |
6.5 | [[6.5] 2502.07838 NanoVLMs: How small can we go and still make coherent Vision Language Models?](https://arxiv.org/abs/2502.07838) <br> [{'name': 'Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>lightweight models | Input: Image-text pairs 图像-文本对<br>Step1: Dataset creation 数据集创建<br>Step2: Model training 模型训练<br>Step3: Evaluation using creative scoring 通过创意评分进行评估<br>Output: Lightweight vision-language models 轻量级视觉语言模型 |


## Arxiv 2025-02-12

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.07140 Few-Shot Multi-Human Neural Rendering Using Geometry Constraints](https://arxiv.org/abs/2502.07140) <br> [{'name': 'Qian li, Victoria Fern\\`andez Abrevaya, Franck Multon, Adnane Boukhayma'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-human scenes<br>neural rendering | Input: Sparse multi-view images 稀疏多视角图像<br>Step1: Geometry constraints using SMPL meshes 使用SMPL网格的几何约束<br>Step2: Regularize signed distances for optimization 通过正则化签名距离进行优化<br>Step3: Apply ray and saturation regularization 应用射线和饱和度正则化<br>Output: Accurate multi-human 3D reconstructions and renderings 准确的多人的三维重建和渲染 |
9.5 | [[9.5] 2502.07278 Articulate That Object Part (ATOP): 3D Part Articulation from Text and Motion Personalization](https://arxiv.org/abs/2502.07278) <br> [{'name': 'Aditya Vora, Sauradip Nag, Hao Zhang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D articulation<br>motion personalization<br>video diffusion | Input: Segmented mesh and text prompt 输入：分割网格和文本提示<br>Step1: Few-shot finetuning for category-specific motion generation 第一步：针对特定类别的运动生成进行少量样本微调<br>Step2: Multi-view rendering to generate personalized motion video 第二步：多视角渲染生成个性化运动视频<br>Step3: Differentiable rendering for transferring motion to the 3D object 第三步：可微渲染将运动转移到三维对象<br>Output: Articulated 3D object with realistic motion 输出：具有真实运动的关节三维对象 |
9.5 | [[9.5] 2502.07289 Learning Inverse Laplacian Pyramid for Progressive Depth Completion](https://arxiv.org/abs/2502.07289) <br> [{'name': 'Kun Wang, Zhiqiang Yan, Junkai Fan, Jun Li, Jian Yang'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D reconstruction<br>state-of-the-art | Input: Sparse depth measurements and corresponding color image  稀疏深度测量和相应的彩色图像<br>Step 1: Initial low-resolution depth prediction  初步低分辨率深度预测<br>Step 2: Multi-path feature extraction via MFP module 通过MFP模块进行多路径特征提取<br>Step 3: Depth map refinement through upsampling and selective filtering 通过上采样和选择性过滤进行深度图优化<br>Output: Dense depth map  稠密深度图 |
9.5 | [[9.5] 2502.07309 Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2502.07309) <br> [{'name': 'Xiang Li, Pengfei Li, Yupeng Zheng, Wei Sun, Yan Wang, Yilun Chen'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D occupancy modeling 3D占用建模<br>autonomous driving 自动驾驶<br>scene understanding 场景理解 | Input: Multi-view images 多视角图像<br>Step1: Self-supervised pre-training with 2D labels 自监督预训练以2D标签<br>Step2: Fully-supervised fine-tuning with 3D occupancy labels 全监督微调以3D占用标签<br>Step3: State-conditioned forecasting module for future occupancy 未来占用状态条件预测模块<br>Output: 3D occupancy predictions 3D占用预测 |
9.5 | [[9.5] 2502.07403 Extended monocular 3D imaging](https://arxiv.org/abs/2502.07403) <br> [{'name': 'Zicheng Shen, Feng Zhao, Yibo Ni, Yuanmu Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D imaging 3D成像<br>monocular vision 单目视觉<br>depth estimation 深度估计<br>material identification 材料识别 | Input: Monocular camera with diffractive-refractive hybrid lens 使用具备衍射-折射混合透镜的单目相机<br>Step1: Multi-stage fusion of depth cues 深度线索的多级融合<br>Step2: Snapshot acquisition of 3D point cloud 3D点云的快照获取<br>Step3: Accurate 3D reconstruction 精确的3D重建<br>Output: Enhanced 3D imaging capabilities 改进的3D成像能力 |
9.5 | [[9.5] 2502.07505 Efficient Continuous Group Convolutions for Local SE(3) Equivariance in 3D Point Clouds](https://arxiv.org/abs/2502.07505) <br> [{'name': 'Lisa Weijler, Pedro Hermosilla'}] | Point Cloud Processing 点云处理 | v2<br>3D point clouds 3D点云<br>equivariance 等变性 | Input: 3D point clouds 3D点云<br>Step1: Define Local Reference Frame (LRF) 定义局部参考框架<br>Step2: Implement continuous SE(3) equivariant convolution 实现连续SE(3)等变卷积<br>Step3: Train the model with stochastically sampled frames 用随机采样的框架训练模型<br>Output: Local rotation equivariant features 输出局部旋转等变特征 |
9.5 | [[9.5] 2502.07615 Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors](https://arxiv.org/abs/2502.07615) <br> [{'name': 'Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Siyu Zhu, Zhihao Li, Xun Cao, Yao Yao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>mesh reconstruction<br>geometry reconstruction | Input: 3D Gaussian Splatting images 3D高斯点云图像<br>Step1: Incorporate pre-trained matching prior 引入预训练匹配先验<br>Step2: Implement Flow Distillation Sampling 算法流蒸馏抽样<br>Step3: Target unobserved views 目标未观察视图<br>Output: Enhanced geometric reconstruction 改进的几何重建 |
9.5 | [[9.5] 2502.07685 Matrix3D: Large Photogrammetry Model All-in-One](https://arxiv.org/abs/2502.07685) <br> [{'name': 'Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, Shiwei Li'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>photogrammetry<br>depth estimation<br>pose estimation<br>novel view synthesis | Input: Multi-modal data (images, camera parameters, depth maps) 图像、相机参数和深度图的多模态数据<br>Step 1: Masked input learning 掩码输入学习<br>Step 2: Pose estimation 位置估计<br>Step 3: Depth prediction 深度预测<br>Step 4: Novel view synthesis 新视图合成<br>Output: Comprehensive 3D model 综合三维模型 |
9.0 | [[9.0] 2502.07030 PrismAvatar: Real-time animated 3D neural head avatars on edge devices](https://arxiv.org/abs/2502.07030) <br> [{'name': 'Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D avatar<br>neural rendering<br>real-time animation<br>head modeling | Input: Series of matted images of a head 头部图像序列<br>Step1: Data acquisition and tracking 数据采集与跟踪<br>Step2: Train hybrid mesh-volumetric model 训练混合网格-体积模型<br>Step3: Distillation into rigged mesh and neural textures 蒸馏成具有骨架的网格和神经纹理<br>Output: Real-time animated 3D head avatar 实时动画3D头像 |
8.5 | [[8.5] 2502.06843 Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation](https://arxiv.org/abs/2502.06843) <br> [{'name': 'Namhee Kim, Woojin Park'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>large language models<br>computer vision | Input: Visual inputs and scenarios 视觉输入与场景<br>Step1: Feature extraction using YOLOv4 and ViT 使用YOLOv4和ViT进行特征提取<br>Step2: Integration with LLM for reasoning 与LLM结合进行推理<br>Step3: Generation of situation descriptions and responses 生成情境描述和适当反应<br>Output: Improved autonomous driving assistance system 改进的自动驾驶辅助系统 |
8.5 | [[8.5] 2502.06957 GAS: Generative Avatar Synthesis from a Single Image](https://arxiv.org/abs/2502.06957) <br> [{'name': 'Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>avatar generation<br>3D reconstruction<br>diffusion models | Input: A single image 单幅图像<br>Step1: 3D human reconstruction 人体三维重建<br>Step2: Dense driving signal generation 生成密集驱动信号<br>Step3: Video diffusion model application 应用视频扩散模型<br>Output: View-consistent and temporally coherent avatars 输出：视图一致且时间连贯的头像 |
8.5 | [[8.5] 2502.07001 From Image to Video: An Empirical Study of Diffusion Representations](https://arxiv.org/abs/2502.07001) <br> [{'name': "Pedro V\\'elez, Luisa F. Polan\\'ia, Yi Yang, Chuhan Zhang, Rishab Kabra, Anurag Arnab, Mehdi S. M. Sajjadi"}] | Image and Video Generation 图像生成与视频生成 | v2<br>diffusion models<br>video synthesis<br>image generation<br>depth estimation | Input: Video and image diffusion models 视频与图像扩散模型<br>Step1: Model architecture comparison 模型架构比较<br>Step2: Performance analysis of latent representations 潜在表示性能分析<br>Step3: Feature extraction and qualitative analysis 特征提取与定性分析<br>Output: Insights into representations and performance 表示与性能的见解 |
8.5 | [[8.5] 2502.07007 Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC](https://arxiv.org/abs/2502.07007) <br> [{'name': 'Siwei Meng, Yawei Luo, Ping Liu'}] | Image and Video Generation 图像生成与视频生成 | v2<br>3D generation<br>physics priors<br>AI-generated content<br>physical realism | Input: Generative models 生成模型<br>Step1: Review of physics-aware methods 物理感知方法的回顾<br>Step2: Categorization of generation techniques 生成技术的分类<br>Step3: Comparative analysis 比较分析<br>Output: Insights for future research 未来研究的洞见 |
8.5 | [[8.5] 2502.07120 Is Long Range Sequential Modeling Necessary For Colorectal Tumor Segmentation?](https://arxiv.org/abs/2502.07120) <br> [{'name': 'Abhishek Srivastava, Koushik Biswas, Gorkem Durak, Gulsah Ozden, Mustafa Adli, Ulas Bagci'}] | 3D Segmentation and Reconstruction 3D分割与重建 | v2<br>3D segmentation<br>tumor segmentation<br>colorectal cancer | Input: 3D medical images 3D医学影像<br>Step 1: Evaluate long-range and local token modeling mechanisms 评估长范围和局部标记建模机制<br>Step 2: Propose MambaOutUNet for tumor segmentation 提出MambaOutUNet用于肿瘤分割<br>Step 3: Analyze performance on the CTS-204 dataset 在CTS-204数据集上分析性能<br>Output: Comparative results on tumor segmentation techniques 输出：肿瘤分割技术的比较结果 |
8.5 | [[8.5] 2502.07145 Mesh2SSM++: A Probabilistic Framework for Unsupervised Learning of Statistical Shape Model of Anatomies from Surface Meshes](https://arxiv.org/abs/2502.07145) <br> [{'name': 'Krithika Iyer, Mokshagna Sai Teja Karanam, Shireen Elhabian'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Statistical Shape Modeling<br>Surface Meshes<br>Unsupervised Learning | Input: Surface meshes 表面网格<br>Step1: Estimate correspondences from meshes 估计来自网格的对应关系<br>Step2: Develop probabilistic shape model 开发概率形状模型<br>Step3: Evaluate model performance 评估模型性能<br>Output: Statistical shape model 统计形状模型 |
8.5 | [[8.5] 2502.07194 Dense Object Detection Based on De-homogenized Queries](https://arxiv.org/abs/2502.07194) <br> [{'name': 'Yueming Huang, Chenrui Ma, Hao Zhou, Hao Wu, Guowu Yuan'}] | Autonomous Driving 自动驾驶 | v2<br>dense object detection<br>autonomous driving<br>DETR<br>deep learning<br>computer vision | Input: Dense object detection scenario 密集目标检测场景<br>Step1: Identify issues with existing NMS methods 识别现有NMS方法的问题<br>Step2: Propose differentiated encoding for queries 提出差异化编码以应对查询<br>Step3: Implement joint loss for better query initialization 实施联合损失以更好地初始化查询<br>Output: Enhanced dense object detection framework 改进的密集目标检测框架 |
8.5 | [[8.5] 2502.07372 USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging under Multiple Adverse Weather Conditions](https://arxiv.org/abs/2502.07372) <br> [{'name': 'Yuxu Lu, Ai Chen, Dong Yang, Ryan Wen Liu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>autonomous driving<br>image restoration | Input: Degraded images 退化图像<br>Step1: Feature extraction 特征提取<br>Step2: Scene restoration 场景恢复<br>Step3: Edge feature extraction 边缘特征提取<br>Output: Enhanced image quality 改进的图像质量 |
8.5 | [[8.5] 2502.07417 Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized Attention Vision Transformer for Autonomous Driving](https://arxiv.org/abs/2502.07417) <br> [{'name': 'Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Wen-Kai Kuo, Jun-Wei Hsieh'}] | Autonomous Driving 自动驾驶 | v2<br>Object Detection 目标检测<br>Autonomous Driving 自动驾驶<br>Vision Transformer 视觉变换器 | Input: Driving scene images 驾驶场景图像<br>Step1: Analyze backbone architectures 分析主干架构<br>Step2: Develop reparameterized attention vision transformer 开发重参数化注意力视觉变换器<br>Step3: Integrate multi-scale feature extraction 集成多尺度特征提取<br>Step4: Model evaluation 模型评估<br>Output: High-performance object detection model 高性能目标检测模型 |
8.5 | [[8.5] 2502.07486 Automated Road Extraction and Centreline Fitting in LiDAR Point Clouds](https://arxiv.org/abs/2502.07486) <br> [{'name': 'Xinyu Wang, Muhammad Ibrahim, Atif Mansoor, Hasnein Tareque, Ajmal Mian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>road extraction<br>3D point clouds<br>LiDAR | Input: 3D LiDAR point clouds 3D LiDAR点云<br>Step 1: Statistical outlier removal 统计离群值去除<br>Step 2: Density-based clustering 基于密度的聚类<br>Step 3: Ground point filtering using grid-based segmentation 使用基于网格的分割进行地面点过滤<br>Step 4: 2D projection and skeletonization 2D投影和骨架化<br>Step 5: Back-projection onto 3D point cloud 反投影到3D点云<br>Output: Refined road points and centreline 提炼的道路点和中心线 |
8.5 | [[8.5] 2502.07631 Divide and Merge: Motion and Semantic Learning in End-to-End Autonomous Driving](https://arxiv.org/abs/2502.07631) <br> [{'name': 'Yinzhe Shen, \\"Omer \\c{S}ahin Ta\\c{s}, Kaiwen Wang, Royden Wagner, Christoph Stiller'}] | Autonomous Driving 自动驾驶 | v2<br>autonomous driving<br>motion learning<br>semantic learning | Input: Camera data 摄像头数据<br>Step1: Motion and semantic task separation 任务分离<br>Step2: Neural-Bayes motion decoder 运动解码器<br>Step3: Interactive semantic decoder 交互式语义解码器<br>Output: Improved detection and tracking 改进的检测与跟踪 |
8.5 | [[8.5] 2502.07680 Multiview Point Cloud Registration Based on Minimum Potential Energy for Free-Form Blade Measurement](https://arxiv.org/abs/2502.07680) <br> [{'name': 'Zijie Wu, Yaonan Wang, Yang Mo, Qing Zhu, He Xie, Haotian Wu, Mingtao Feng, Ajmal Mian'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>point cloud registration<br>noise resistance<br>industrial measurement | Input: Point cloud data 点云数据<br>Step1: Definition of objective function 目标函数定义<br>Step2: Global optimization procedure 全局优化过程<br>Step3: Fine registration using trimmed ICP 精细配准，使用修剪的ICP算法<br>Output: Registered point clouds 注册后的点云 |
8.5 | [[8.5] 2502.07785 Pippo: High-Resolution Multi-View Humans from a Single Image](https://arxiv.org/abs/2502.07785) <br> [{'name': 'Yash Kant, Ethan Weber, Jin Kyu Kim, Rawal Khirodkar, Su Zhaoen, Julieta Martinez, Igor Gilitschenski, Shunsuke Saito, Timur Bagautdinov'}] | 3D Generation 三维生成 | v2<br>3D consistency<br>multi-view generation<br>video generation | Input: Single image of a person 一个人的单张图像<br>Step1: Pre-training on human images 人体图像的预训练<br>Step2: Multi-view mid-training 多视角中期训练<br>Step3: Post-training with pixel-aligned controls 像素对齐控制的后期训练<br>Output: 1K resolution multi-view consistent images 1K分辨率的多视角一致图像 |
8.0 | [[8.0] 2502.07508 Enhance-A-Video: Better Generated Video for Free](https://arxiv.org/abs/2502.07508) <br> [{'name': 'Yang Luo, Xuanlei Zhao, Mengzhao Chen, Kaipeng Zhang, Wenqi Shao, Kai Wang, Zhangyang Wang, Yang You'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>temporal consistency<br>DiT-based models | Input: DiT-based video generation models  基于DiT的视频生成模型<br>Step1: Analyze temporal attention analysis 时序注意力分析<br>Step2: Introduce cross-frame intensity parameters 引入跨帧强度参数<br>Step3: Enhance video quality through adjusted dependencies 调整依赖关系以增强视频质量<br>Output: Enhanced video generation quality 提升的视频生成质量 |
8.0 | [[8.0] 2502.07564 An Elliptic Curve Based Solution to the Perspective-Three-Point Problem](https://arxiv.org/abs/2502.07564) <br> [{'name': 'Michael Q. Rieck'}] | Computer Vision and Pose Estimation 计算机视觉与位姿估计 | v2<br>P3P<br>camera pose<br>elliptic curves | Input: Control points 控制点<br>Step1: Determine directions of lines 计算直线方向<br>Step2: Develop P3P solver 开发P3P求解器<br>Step3: Compare with linear solvers 与线性求解器比较<br>Output: Accurate camera poses 准确的相机位姿 |
7.5 | [[7.5] 2502.07306 TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/abs/2502.07306) <br> [{'name': 'Navid Rajabi, Jana Kosecka'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Navigation<br>modular approach<br>navigation instruction | Input: Navigation instruction and environment map 导航指令和环境地图<br>Step1: Extract landmarks using LLM 提取地标<br>Step2: Retrieve top-k locations using shortest path algorithm 检索前k个位置，使用最短路径算法<br>Step3: Compute alignment score with dynamic programming 使用动态规划计算对齐评分<br>Output: Evaluate path fidelity using nDTW metric 输出：使用nDTW指标评估路径可信度 |
7.5 | [[7.5] 2502.07617 Scaling Pre-training to One Hundred Billion Data for Vision Language Models](https://arxiv.org/abs/2502.07617) <br> [{'name': 'Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, Xiaohua Zhai'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>cultural diversity<br>multilinguality | Input: 100 billion image-text pairs 1000亿图像-文本对<br>Step1: Empirical investigation 实证研究<br>Step2: Performance analysis 性能分析<br>Step3: Cultural diversity assessment 文化多样性评估<br>Output: Insights on VLM performance 视觉语言模型性能见解 |
7.5 | [[7.5] 2502.07701 Magic 1-For-1: Generating One Minute Video Clips within One Minute](https://arxiv.org/abs/2502.07701) <br> [{'name': 'Hongwei Yi, Shitong Shao, Tian Ye, Jiantong Zhao, Qingyu Yin, Michael Lingelbach, Li Yuan, Yonghong Tian, Enze Xie, Daquan Zhou'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>diffusion models<br>text-to-image<br>image-to-video | Input: Text and video data 文本和视频数据<br>Step1: Task factorization 任务分解<br>Step2: Generative prior injection 生成先验注入<br>Step3: Model optimization 模型优化<br>Output: Efficient video clips 生成高效视频片段 |
7.5 | [[7.5] 2502.07737 Next Block Prediction: Video Generation via Semi-Autoregressive Modeling](https://arxiv.org/abs/2502.07737) <br> [{'name': 'Shuhuai Ren, Shuming Ma, Xu Sun, Furu Wei'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>semi-autoregressive modeling | Input: Video data 视频数据<br>Step1: Block decomposition 块分解<br>Step2: Semi-autoregressive generation 半自回归生成<br>Step3: Bidirectional attention application 双向注意力应用<br>Output: Generated video frames 生成的视频帧 |


## Arxiv 2025-02-11

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.05222 VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic Resolution Management via Q-Learning](https://arxiv.org/abs/2502.05222) <br> [{'name': 'Jayram Palamadai, William Yu'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D volumetric reconstruction 3D体积重建<br>dynamic resolution management 动态分辨率管理<br>photorealistic rendering 照相真实渲染 | Input: 2D photographs 二维照片<br>Step1: Image conversion to PlenOctree data structure 图像转换为PlenOctree数据结构<br>Step2: Dynamic resolution management using QuiQ 动态分辨率管理使用QuiQ<br>Step3: Synthesizing novel viewpoints using differentiable rendering 合成新视角使用可微渲染<br>Output: Interactive 3D volumetric images 互动三维体积图像 |
9.5 | [[9.5] 2502.05378 NextBestPath: Efficient 3D Mapping of Unseen Environments](https://arxiv.org/abs/2502.05378) <br> [{'name': "Shiyao Li, Antoine Gu\\'edon, Cl\\'ementin Boittiaux, Shizhe Chen, Vincent Lepetit"}] | 3D Mapping and Reconstruction 3D映射与重建 | v2<br>3D mapping<br>active mapping<br>robotics | Input: Unseen indoor environments 未知室内环境<br>Step1: Create and benchmark a new dataset (AiMDoom) 创建并基准新的数据集 (AiMDoom)<br>Step2: Develop the next-best-path method (NBP) 开发下一最佳路径方法 (NBP)<br>Step3: Plan and optimize trajectory for active mapping 规划和优化主动映射的轨迹<br>Output: Efficiently reconstructed 3D models 有效重建的三维模型 |
9.5 | [[9.5] 2502.05859 SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion](https://arxiv.org/abs/2502.05859) <br> [{'name': 'Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng'}] | Depth Estimation 深度估计 | v2<br>panorama depth estimation<br>3D reconstruction<br>autonomous driving | Input: Panorama images 全景图像<br>Step1: Feature extraction 特征提取<br>Step2: Feature fusion 特征融合<br>Step3: Depth estimation 深度估计<br>Output: Depth map and point cloud 深度图和点云 |
9.5 | [[9.5] 2502.05874 MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation](https://arxiv.org/abs/2502.05874) <br> [{'name': 'Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Xiangde Liu, Guangyao Zhai'}] | 3D Generation 三维生成 | v2<br>3D scene generation<br>geometry control<br>mixed-modality graph | Input: Mixed-Modality Graph combining textual and visual modalities<br>Step1: Process user inputs involving text, image, or both<br>Step2: Visual enhancement module constructs visual representations<br>Step3: Relation predictor infers relationships between nodes<br>Output: Generated 3D indoor scenes with controllable geometry |
9.5 | [[9.5] 2502.06336 DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation](https://arxiv.org/abs/2502.06336) <br> [{'name': 'Sara Monji-Azad, Marvin Kinz, Siddharth Kothari, Robin Khanna, Amrei Carla Mihan, David Maennel, Claudia Scherl, Juergen Hesser'}] | Point Cloud Processing 点云处理 | v2<br>3D reconstruction<br>point cloud registration<br>Transformers | Input: Source and target point clouds 源点云和目标点云<br>Step1: Feature descriptor design 特征描述符设计<br>Step2: Learning displacement vector fields 学习位移向量场<br>Output: Enhanced point cloud registration 改进的点云配准 |
9.5 | [[9.5] 2502.06338 Zero-shot Depth Completion via Test-time Alignment with Affine-invariant Depth Prior](https://arxiv.org/abs/2502.06338) <br> [{'name': 'Lee Hyoseok, Kyeong Seon Kim, Kwon Byung-Ki, Tae-Hyun Oh'}] | Depth Estimation 深度估计 | v2<br>depth completion<br>3D reconstruction<br>zero-shot learning | Input: Sparse depth measurements and RGB images 输入：稀疏深度测量与RGB图像<br>Step1: Alignment of depth prior with sparse measurements 步骤1：将深度先验与稀疏测量对齐<br>Step2: Optimization loop at test-time to enforce constraints 步骤2：在测试时进行优化循环以强制约束<br>Step3: Depth map completion based on aligned prior 步骤3：基于对齐的先验完成深度图<br>Output: Complete dense depth map 输出：完整的密集深度图 |
9.5 | [[9.5] 2502.06367 FOCUS - Multi-View Foot Reconstruction From Synthetically Trained Dense Correspondences](https://arxiv.org/abs/2502.06367) <br> [{'name': 'Oliver Boyne, Roberto Cipolla'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>multi-view reconstruction<br>foot model<br>structure-from-motion<br>dense correspondences | Input: Multi-view RGB images 多视角RGB图像<br>Step1: Dataset extension 数据集扩展<br>Step2: Dense correspondence prediction 密集对应关系预测<br>Step3: 3D surface reconstruction via SfM and optimization 通过SfM和优化进行3D表面重建<br>Output: 3D mesh model 输出: 3D网格模型 |
9.5 | [[9.5] 2502.06608 TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](https://arxiv.org/abs/2502.06608) <br> [{'name': 'Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao'}] | 3D Generation 三维生成 | v2<br>3D Generation<br>Shape Diffusion<br>High-Fidelity 3D Models | Input: Images 输入: 图像<br>Step1: Data processing 数据处理<br>Step2: Shape generation 形状生成<br>Step3: Model evaluation 模型评估<br>Output: High-fidelity 3D meshes 输出: 高保真3D网格 |
9.5 | [[9.5] 2502.06682 Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene](https://arxiv.org/abs/2502.06682) <br> [{'name': 'Tai-Yu Pan, Sooyoung Jeon, Mengdi Fan, Jinsu Yoo, Zhenyang Feng, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, Wei-Lun Chao'}] | 3D Generation 三维生成 | v2<br>3D generation<br>collaborative perception<br>autonomous driving<br>point cloud generation | Input: Ego-car sensory data 车载传感器数据<br>Step 1: Data integration 数据集成<br>Step 2: Conditioned diffusion model training 条件扩散模型训练<br>Step 3: Generate realistic point clouds 生成真实的点云<br>Output: Collaborative perception data 协同感知数据 |
9.2 | [[9.2] 2502.05769 Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform](https://arxiv.org/abs/2502.05769) <br> [{'name': 'Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li'}] | 3D Modeling 三维建模 | v2<br>3D modeling<br>Gaussian Splatting<br>urban digital twin<br>GIS integration<br>Large Language Models | Input: Building's address, postal code, or geographic coordinates<br>Step1: Integrate with Google Maps Platform APIs<br>Step2: Perform Gaussian Splatting-based mesh extraction<br>Step3: Retrieve 3D models and visual descriptions<br>Output: Digital twin of the building with 3D models and layers of data |
8.5 | [[8.5] 2502.05409 Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment](https://arxiv.org/abs/2502.05409) <br> [{'name': 'Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder'}] | 3D Simulation and Modeling 三维仿真与建模 | v2<br>3D simulation<br>pose estimation<br>UAV<br>Gaussian splatting | Input: Monocular images from UAV 无人机采集的单目图像<br>Step1: Data integration and simulation 数据集成与仿真<br>Step2: Deep pose estimation algorithm development 深度姿态估计算法开发<br>Step3: Indoor testing and validation 室内测试与验证<br>Output: Accurate pose estimation for UAV relative to the vessel 输出：无人机相对于船只的准确姿态估计 |
8.5 | [[8.5] 2502.05779 A 3D Multimodal Feature for Infrastructure Anomaly Detection](https://arxiv.org/abs/2502.05779) <br> [{'name': 'Yixiong Jing, Wei Lin, Brian Sheil, Sinan Acikgoz'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>anomaly detection<br>point clouds<br>crack detection | Input: Point clouds and multimodal features 点云和多模态特征<br>Step1: Feature extraction 特征提取<br>Step2: Integration with PatchCore algorithm 集成至PatchCore算法<br>Step3: Evaluation with statistical methods 使用统计方法进行评估<br>Output: Enhanced defect detection results 改进的缺陷检测结果 |
8.5 | [[8.5] 2502.05964 Revisiting Gradient-based Uncertainty for Monocular Depth Estimation](https://arxiv.org/abs/2502.05964) <br> [{'name': 'Julia Hornauer, Amir El-Ghoussani, Vasileios Belagiannis'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation 单目深度估计<br>Uncertainty Estimation 不确定性估计 | Input: Monocular images 单目图像<br>Step1: Gradient extraction using auxiliary loss 梯度提取与辅助损失<br>Step2: Uncertainty score calculation 不确定性评分计算<br>Output: Depth predictions and uncertainty scores 深度预测与不确定性评分 |
8.5 | [[8.5] 2502.06019 Noise is an Efficient Learner for Zero-Shot Vision-Language Models](https://arxiv.org/abs/2502.06019) <br> [{'name': 'Raza Imam, Asif Hanif, Jian Zhang, Khaled Waleed Dawoud, Yova Kementchedjhieva, Mohammad Yaqub'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>noise adaptation<br>test-time adaptation | Input: Visual representations 视觉表征<br>Step1: Test-time adaptation 测试时适应<br>Step2: Learnable noise optimization 可学习噪声优化<br>Step3: Inter-view representation alignment 视图间表征对齐<br>Output: Enhanced VLM performance 改进的视觉语言模型性能 |
8.5 | [[8.5] 2502.06219 Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing](https://arxiv.org/abs/2502.06219) <br> [{'name': 'Sicen Guo, Tianyou Wen, Chuang-Wei Liu, Qijun Chen, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>RGB-D driving scene parsing<br>Heterogeneous Feature Integration Transformer<br>Vision Foundation Models | Input: RGB and depth data RGB和深度数据<br>Step1: Relative depth estimation 进行相对深度估计<br>Step2: Heterogeneous Feature Integration Transformer (HFIT) development 开发异构特征集成变换器 (HFIT)<br>Step3: Feature integration and evaluation 特征集成与评估<br>Output: Enhanced driving scene parsing model 改进的驾驶场景解析模型 |
8.5 | [[8.5] 2502.06337 Accelerating Outlier-robust Rotation Estimation by Stereographic Projection](https://arxiv.org/abs/2502.06337) <br> [{'name': 'Taosi Xu, Yinlong Liu, Xianbo Wang, Zhi-Xin Yang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Rotation Estimation<br>Outlier Robustness<br>Stereographic Projection<br>Point Cloud Registration | Input: 3D point sets from different views 3D 点集来自不同视角<br>Step1: Investigate geometric constraints 调查几何约束<br>Step2: Use stereographic projection for rotation axis estimation 使用立体投影进行旋转轴估计<br>Step3: Implement spatial voting for axis identification 实施空间投票以识别轴<br>Output: Optimal rotation estimations  optimal 旋转估计 |
8.5 | [[8.5] 2502.06392 TANGLED: Generating 3D Hair Strands from Images with Arbitrary Styles and Viewpoints](https://arxiv.org/abs/2502.06392) <br> [{'name': 'Pengyu Long, Zijun Zhao, Min Ouyang, Qingcheng Zhao, Qixuan Zhang, Wei Yang, Lan Xu, Jingyi Yu'}] | 3D Generation 三维生成 | v2<br>3D hair generation<br>diffusion models<br>multi-view input | Input: Multi-view linearts and images 多视角线稿和图像<br>Step 1: Collecting and annotating diverse hairstyle dataset 收集和标注多样的发型数据集<br>Step 2: Implementing a latent diffusion model with cross-attention 采用具有跨注意力的潜在扩散模型<br>Step 3: Applying parametric post-processing to enforce structural constraints 应用参数后处理以强制执行结构约束<br>Output: High-quality 3D hair strands 高质量三维发丝 |
8.5 | [[8.5] 2502.06543 Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos](https://arxiv.org/abs/2502.06543) <br> [{'name': 'Zhu Chen, Ina Laube, Johannes Stegmaier'}] | 3D Reconstruction  三维重建 | v2<br>3D+t point clouds<br>temporal alignment<br>unsupervised learning | Input: 3D+t point clouds of zebrafish embryos 3D+t 点云<br>Step1: Feature extraction using autoencoder 特征提取通过自编码器<br>Step2: Temporal alignment using regression network 时间对齐通过回归网络<br>Output: Aligned time frames of 3D+t point clouds 对齐的3D+t点云时间帧 |
8.5 | [[8.5] 2502.06782 Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT](https://arxiv.org/abs/2502.06782) <br> [{'name': 'Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, Peng Gao'}] | Image and Video Generation 图像生成与视频生成 | v2<br>video generation<br>Diffusion Transformers | Input: Video generation task 视频生成任务<br>Step1: Implement Multi-scale Next-DiT architecture 实现多尺度Next-DiT架构<br>Step2: Incorporate motion conditioning 引入运动条件<br>Step3: Progressive and multi-source training for efficiency 进行渐进和多源训练以提高效率<br>Output: High-quality generated videos 高质量生成视频 |
8.5 | [[8.5] 2502.06787 Visual Agentic AI for Spatial Reasoning with a Dynamic API](https://arxiv.org/abs/2502.06787) <br> [{'name': 'Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari'}] | Spatial Reasoning 空间推理 | v2<br>3D spatial reasoning<br>Visual reasoning<br>Dynamic API | Input: Queries for 3D understanding 3D理解的查询<br>Step1: Dynamic API generation 动态API生成<br>Step2: Program synthesis 程序合成<br>Step3: Evaluation with benchmarks 使用基准评估<br>Output: Enhanced 3D spatial reasoning capabilities 改进的3D空间推理能力 |
8.0 | [[8.0] 2502.06023 Dual Caption Preference Optimization for Diffusion Models](https://arxiv.org/abs/2502.06023) <br> [{'name': 'Amir Saeidi, Yiran Luo, Agneet Chatterjee, Shamanthak Hegde, Bimsara Pathiraja, Yezhou Yang, Chitta Baral'}] | Image Generation 图像生成 | v2<br>image generation<br>text-to-image<br>diffusion models | Input: Text-to-image diffusion model 文本到图像扩散模型<br>Step1: Mitigate irrelevant prompts 减少无关提示<br>Step2: Optimize dual caption preferences 优化双重标题偏好<br>Step3: Experiment with different caption strategies 采用不同的标题策略<br>Output: Improved image generation 改进的图像生成 |


## Arxiv 2025-02-10

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.04630 High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting](https://arxiv.org/abs/2502.04630) <br> [{'name': 'Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>sensor fusion<br>Gaussian splatting<br>high-speed imaging | Input: RGB, depth, and event camera data 输入: RGB、深度和事件相机数据<br>Step1: Data integration 数据集成<br>Step2: Scene representation using deformable 3D Gaussians 场景表示使用可变形3D高斯<br>Step3: Joint optimization of Gaussian parameters jointly 优化高斯参数<br>Output: High-quality 3D scene reconstruction 输出: 高质量3D场景重建 |
9.5 | [[9.5] 2502.04734 SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting](https://arxiv.org/abs/2502.04734) <br> [{'name': 'Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>omnidirctional images | Input: 360-degree images 360度图像<br>Step1: Direct pose calibration 直接姿态标定<br>Step2: 3D Gaussians optimization 3D高斯优化<br>Step3: Joint optimization of parameters 参数的联合优化<br>Output: Enhanced omnidirectional radiance fields 改进的全方位辐射场 |
9.5 | [[9.5] 2502.04804 DetVPCC: RoI-based Point Cloud Sequence Compression for 3D Object Detection](https://arxiv.org/abs/2502.04804) <br> [{'name': 'Mingxuan Yan, Ruijie Zhang, Xuedou Xiao, Wei Wang'}] | 3D Object Detection 3D 物体检测 | v2<br>3D reconstruction<br>point cloud compression<br>object detection | Input: 3D point cloud sequences 3D 点云序列<br>Step1: Identify regions of interest (RoIs) 识别兴趣区域 (RoIs)<br>Step2: Apply RoI-based encoding 应用 RoI 基于编码<br>Step3: Compress using VPCC and evaluate compressive performance 基于 VPCC 压缩并评估压缩性能<br>Output: Compressed point cloud data with improved detection accuracy 输出: 经过压缩的点云数据，具有改进的检测准确性 |
9.5 | [[9.5] 2502.04843 PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression](https://arxiv.org/abs/2502.04843) <br> [{'name': 'Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>scene coordinate regression<br>novel view synthesis | Input: Rendered images and sparse inputs 渲染图像和稀疏输入<br>Step1: Pixel filtering to retain well-rendered pixels 像素过滤以保留渲染良好的像素<br>Step2: Scene Coordinate Regression (SCR) model training based on filtered data 基于过滤数据的场景坐标回归模型训练<br>Step3: Evaluation of pose estimation performance 性能评估 |
9.5 | [[9.5] 2502.04981 OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and Geometric-Aware Gaussian Splatting](https://arxiv.org/abs/2502.04981) <br> [{'name': 'Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang'}] | 3D Reconstruction 三维重建 | v2<br>3D occupancy reconstruction<br>semantic reconstruction<br>Gaussian Splatting | Input: Raw sensor data 原始传感器数据<br>Step1: Extract semantic information from vision-language models 提取语言模型中的语义信息<br>Step2: Construct Semantic and Geometric-Aware Gaussians 构建语义和几何意识高斯<br>Step3: Implement cumulative Gaussian-to-3D voxel splatting 实现累积高斯到3D体素的溅射<br>Output: Semantic 3D occupancy reconstruction 语义3D占用重建 |
9.5 | [[9.5] 2502.05040 GaussRender: Learning 3D Occupancy with Gaussian Rendering](https://arxiv.org/abs/2502.05040) <br> [{'name': 'Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord'}] | 3D Reconstruction 三维重建 | v2<br>3D occupancy<br>Gaussian rendering<br>autonomous driving<br>semantic understanding<br>voxel-based supervision | Input: 3D voxel representations 3D体素表示<br>Step1: Projection to 2D perspectives 投影到2D视图<br>Step2: Introduction of Gaussian splatting 高斯点云引入<br>Step3: Loss integration for training 损失函数集成<br>Output: Enhanced 3D occupancy models 改进的3D占用模型 |
9.5 | [[9.5] 2502.05175 Fillerbuster: Multi-View Scene Completion for Casual Captures](https://arxiv.org/abs/2502.05175) <br> [{'name': 'Ethan Weber, Norman M\\"uller, Yash Kant, Vasu Agrawal, Michael Zollh\\"ofer, Angjoo Kanazawa, Christian Richardt'}] | 3D Reconstruction 三维重建 | v2<br>3D scene completion<br>multi-view synthesis<br>novel view generation | Input: Multi-view casual captures 多视角随意捕捉<br>Step1: Unobserved content recovery 未观察到的内容恢复<br>Step2: Generative model training 生成模型训练<br>Step3: Scene completion and pose prediction 场景补全与姿势预测<br>Output: Complete 3D scene with novel views 输出: 完整的三维场景与新视角 |
9.5 | [[9.5] 2502.05176 AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360{\deg} Unbounded Scene Inpainting](https://arxiv.org/abs/2502.05176) <br> [{'name': 'Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D scene inpainting<br>Gaussian Splatting<br>depth-aware methods<br>multi-view coherence<br>unbounded scenes | Input: Multi-view images, camera parameters, object masks, and reference images  输入: 多视角图像、相机参数、对象掩膜和参考图像<br>Step1: Generate depth-aware unseen masks for occlusion identification 步骤1: 生成深度感知的看不见掩膜以识别遮挡<br>Step2: Apply Adaptive Guided Depth Diffusion for point placement 步骤2: 应用自适应引导深度扩散进行点放置<br>Step3: Employ SDEdit for detail enhancement and coherence 步骤3: 使用SDEdit进行细节增强和一致性<br>Output: High-quality inpainted 3D scenes 输出: 高质量的3D场景修复 |
8.5 | [[8.5] 2502.04361 Predicting 3D Motion from 2D Video for Behavior-Based VR Biometrics](https://arxiv.org/abs/2502.04361) <br> [{'name': 'Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee'}] | 3D Motion Prediction 三维运动预测 | v2<br>3D motion prediction<br>biometric authentication<br>virtual reality<br>2D video | Input: 2D body joint data from video 输入: 来自视频的2D身体关节数据<br>Step1: External video tracking 外部视频追踪<br>Step2: 2D to 3D motion prediction 从2D到3D的运动预测<br>Step3: Authentication model evaluation 认证模型评估<br>Output: Enhanced biometric authentication system 输出: 增强的生物识别认证系统 |
8.5 | [[8.5] 2502.04377 MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction](https://arxiv.org/abs/2502.04377) <br> [{'name': 'Xiaoshuai Hao, Yunfeng Diao, Mengchuan Wei, Yifan Yang, Peng Hao, Rong Yin, Hui Zhang, Weiming Li, Shu Zhao, Yu Liu'}] | Map Construction 地图构建 | v2<br>BEV Feature Fusion<br>Autonomous Driving<br>Map Construction<br>Cross-modal Interaction | Input: Multi-modal data from camera and LiDAR sensors<br>Step1: Cross-modal Interaction Transform (CIT) for semantic alignment<br>Step2: Dual Dynamic Fusion (DDF) for selective information integration<br>Step3: Map construction tasks evaluation<br>Output: Enhanced HD and BEV maps |
8.5 | [[8.5] 2502.04378 DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation](https://arxiv.org/abs/2502.04378) <br> [{'name': "Luciano Baresi, Davide Yi Xian Hu, Muhammad Irfan Mas'udi, Giovanni Quattrocchi"}] | Multi-modal Testing and Image Generation 多模态测试与图像生成 | v2<br>autonomous driving<br>deep learning testing<br>diffusion models | Input: Existing images from datasets 现有数据集中的图像<br>Step1: Image captioning 进行图像描述<br>Step2: Keyword identification 关键词识别<br>Step3: Counterfactual caption generation 生成反事实描述<br>Step4: Image generation using diffusion model 利用扩散模型生成图像<br>Output: Augmented test images 增强的测试图像 |
8.5 | [[8.5] 2502.04478 OneTrack-M: A multitask approach to transformer-based MOT models](https://arxiv.org/abs/2502.04478) <br> [{'name': 'Luiz C. S. de Araujo, Carlos M. S. Figueiredo'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Multi-Object Tracking<br>transformers<br>autonomous vehicles | Input: Video sequences from cameras 视频序列<br>Step1: Data pre-processing 数据预处理<br>Step2: Model architecture design 模型架构设计<br>Step3: Multitask training techniques 多任务训练技术<br>Output: Enhanced tracking and detection performance 改进的跟踪与检测性能 |
8.5 | [[8.5] 2502.04483 Measuring Physical Plausibility of 3D Human Poses Using Physics Simulation](https://arxiv.org/abs/2502.04483) <br> [{'name': 'Nathan Louis, Mahzad Khoshlessan, Jason J. Corso'}] | 3D Reconstruction 三维重建 | v2<br>3D human pose estimation<br>physical plausibility<br>physics simulation<br>3D reconstruction | Input: 3D human poses from estimation models 3D 人类姿势估计模型<br>Step1: Physics simulation setup 物理仿真设置<br>Step2: Metric introduction (CoM distance, Pose Stability Duration) 指标引入（质心距离，姿态稳定时间）<br>Step3: Evaluation against state-of-the-art methods 评估与现有最佳方法的比较<br>Output: Metrics for physical plausibility and stability 普适性的物理合理性和稳定性的指标 |
8.5 | [[8.5] 2502.04566 An Optimized YOLOv5 Based Approach For Real-time Vehicle Detection At Road Intersections Using Fisheye Cameras](https://arxiv.org/abs/2502.04566) <br> [{'name': 'Md. Jahin Alam, Muhammad Zubair Hasan, Md Maisoon Rahman, Md Awsafur Rahman, Najibul Haque Sarker, Shariar Azad, Tasnim Nishat Islam, Bishmoy Paul, Tanvir Anjum, Barproda Halder, Shaikh Anowarul Fattah'}] | Autonomous Systems and Robotics 自主系统与机器人技术 | v2<br>vehicle detection<br>YOLOv5<br>fisheye camera<br>autonomous systems | Input: Fisheye camera images 鱼眼摄像头图像<br>Step1: Data acquisition 数据采集<br>Step2: Image preprocessing 图像预处理<br>Step3: Vehicle detection using modified YOLOv5 基于改进的YOLOv5进行车辆检测<br>Step4: Model training and ensemble 模型训练与集成<br>Output: Real-time vehicle detection results 实时车辆检测结果 |
8.5 | [[8.5] 2502.04615 Neural Clustering for Prefractured Mesh Generation in Real-time Object Destruction](https://arxiv.org/abs/2502.04615) <br> [{'name': 'Seunghwan Kim, Sunha Park, Seungkyu Lee'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>point cloud segmentation<br>real-time object destruction | Input: Point cloud data 点云数据<br>Step1: Clustering point cloud with a neural network 使用神经网络进行点云聚类<br>Step2: Predicting structural weaknesses 预测结构弱点<br>Step3: Generating prefractured meshes 生成预裂网格<br>Output: Ready-to-use prefractured meshes 准备使用的预裂网格 |
8.5 | [[8.5] 2502.05055 Differentiable Mobile Display Photometric Stereo](https://arxiv.org/abs/2502.05055) <br> [{'name': 'Gawoon Ban, Hyeongjun Kim, Seokjun Choi, Seungwoo Yoon, Seung-Hwan Baek'}] | 3D Reconstruction 三维重建 | v2<br>Photometric stereo<br>3D reconstruction<br>Mobile devices<br>Surface normals | Input: Mobile phone display and camera 移动电话显示器和相机<br>Step1: Developing a mobile app 开发移动应用<br>Step2: Capturing HDR images and display patterns 捕获HDR图像和显示模式<br>Step3: Learning display patterns 通过可微学习模式<br>Output: 3D surface normals and albedos 3D表面法线和反射率 |
8.5 | [[8.5] 2502.05091 DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions](https://arxiv.org/abs/2502.05091) <br> [{'name': 'Gorkem Can Ates, Kuang Gong, Wei Shao'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D vision-language models<br>medical imaging<br>zero-shot classification<br>efficient computation | Input: 3D medical images 3D医学图像<br>Step1: Decomposed convolution设计 设计分解卷积<br>Step2: Integration into CLIP framework 集成到 CLIP 框架中<br>Step3: Evaluation on CT-RATE dataset 在 CT-RATE 数据集上评估<br>Output: Efficient 3D vision-language model 高效的 3D 视觉-语言模型 |
8.5 | [[8.5] 2502.05153 Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment](https://arxiv.org/abs/2502.05153) <br> [{'name': 'Minh-Quan Le, Gaurav Mittal, Tianjian Meng, A S M Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen'}] | Image Generation 图像生成 | v2<br>Image Generation<br>Visual Question Answering<br>Multimodal learning | Input: Multimodal context (reference image + text guidance) 多模态上下文（参考图像 + 文本指导）<br>Step1: Context description generation 上下文描述生成<br>Step2: Fine-tuning of the diffusion model 调整扩散模型<br>Step3: Image generation 生成图像<br>Output: High-fidelity, diverse images 高保真、多样化图像 |
8.5 | [[8.5] 2502.05178 QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178) <br> [{'name': 'Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Kr\\"ahenb\\"uhl, De-An Huang'}] | Neural Rendering 神经渲染 | v2<br>visual tokenization<br>multimodal understanding<br>image generation<br>reconstruction | Input: Image data 影像数据<br>Step1: Train binary-spherical-quantization-based autoencoder 训练基于二元球面量化的自编码器<br>Step2: Dynamically balance reconstruction and alignment objectives 动态平衡重建与对齐目标<br>Step3: Validate performance on multimodal understanding and image generation 验证在多模态理解与图像生成中的表现<br>Output: Unified model for multimodal tasks 输出：多模态任务的统一模型 |
7.5 | [[7.5] 2502.04475 Augmented Conditioning Is Enough For Effective Training Image Generation](https://arxiv.org/abs/2502.04475) <br> [{'name': 'Jiahui Chen, Amy Zhang, Adriana Romero-Soriano'}] | Image Generation 图像生成 | v2<br>image generation<br>data augmentation<br>classification | Input: Real images and text prompts 真实图像和文本提示<br>Step1: Apply data augmentations 应用数据增强<br>Step2: Condition image generation on augmented data 基于增强数据进行图像生成<br>Step3: Generate synthetic training images 生成合成训练图像<br>Output: Enhanced training datasets 改进的训练数据集 |
7.5 | [[7.5] 2502.04896 Goku: Flow Based Video Generative Foundation Models](https://arxiv.org/abs/2502.04896) <br> [{'name': 'Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>image generation<br>video generation<br>text-to-video tasks | Input: Image and video datasets 图像和视频数据集<br>Step1: Data processing pipeline 数据处理管道<br>Step2: Model architecture optimization 模型架构优化<br>Step3: Training and evaluation 训练与评估<br>Output: High-quality image and video generation 高质量的图像和视频生成 |


## Arxiv 2025-02-07

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.03901 LeAP: Consistent multi-domain 3D labeling using Foundation Models](https://arxiv.org/abs/2502.03901) <br> [{'name': 'Simon Gebraad, Andras Palffy, Holger Caesar'}] | 3D Semantic Understanding 3D语义理解 | v2<br>3D semantic labeling<br>Bayesian update<br>Vision Foundation Models | Input: Unlabeled image-pointcloud pairs 输入: 未标记的图像-点云对<br>Step1: Generate soft 2D labels using Vision Foundation Models 步骤1: 使用视觉基础模型生成软2D标签<br>Step2: Apply Bayesian updating to obtain 3D pseudo-labels 步骤2: 应用贝叶斯更新以获得3D伪标签<br>Step3: Use 3D Consistency Network to improve label quality 步骤3: 使用3D一致性网络提高标签质量<br>Output: High-quality 3D semantic labels 输出: 高质量的3D语义标签 |
9.5 | [[9.5] 2502.04318 sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D Reconstruction from Sparse-Views](https://arxiv.org/abs/2502.04318) <br> [{'name': 'Eyvaz Najafli, Marius K\\"astingsch\\"afer, Sebastian Bernhard, Thomas Brox, Andreas Geiger'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>sparse views<br>latent features | Input: Sparse view images 稀疏视图图像<br>Step1: Generate intermediate virtual views 生成中间虚拟视图<br>Step2: Decode Gaussian primitives 解码高斯原语<br>Step3: Render novel views 渲染新视图<br>Output: 360-degree reconstructed scene 360度重建场景 |
9.0 | [[9.0] 2502.04139 Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation](https://arxiv.org/abs/2502.04139) <br> [{'name': 'Jiahao Lu, Jiacheng Deng, Tianzhu Zhang'}] | 3D Instance Segmentation 3D实例分割 | v2<br>3D instance segmentation<br>transformer-based methods | Input: Scene point cloud input 场景点云输入<br>Step1: Query initialization 查询初始化<br>Step2: Hierarchical query fusion 层次查询融合<br>Step3: Instance segmentation 实例分割<br>Output: Binary foreground masks with semantic labels 输出：带语义标签的二元前景掩码 |
8.5 | [[8.5] 2502.03510 Mapping and Localization Using LiDAR Fiducial Markers](https://arxiv.org/abs/2502.03510) <br> [{'name': 'Yibo Liu'}] | Mapping and Localization 映射与定位 | v2<br>LiDAR<br>fiducial markers<br>mapping<br>localization | Input: LiDAR sensors and fiducial markers<br>Step1: Development of Intensity Image-based LiDAR Fiducial Marker system<br>Step2: Detection of 3D fiducials from intensity images<br>Step3: Algorithm enhancement for 3D map merging and localization<br>Output: Optimized mapping and localization using LFMs |
8.5 | [[8.5] 2502.03628 The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628) <br> [{'name': 'Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, Dimitris N. Metaxas'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>hallucination<br>VISTA<br>multimodal learning | Input: Visual tokens from large Vision-Language Models (LVLMs) 视觉令牌来自大型视觉-语言模型<br>Step1: Analyze token logits ranking 分析令牌的对数排名<br>Step2: Identify visual information loss 识别视觉信息损失<br>Step3: Propose VISTA framework 提出VISTA框架<br>Output: Enhanced decoding with reduced hallucination 输出：减少幻觉的增强解码 |
8.5 | [[8.5] 2502.03639 Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach](https://arxiv.org/abs/2502.03639) <br> [{'name': 'Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, Jian Ren'}] | Image and Video Generation 图像生成与视频生成 | v2<br>Video Generation 视频生成<br>3D Point Regularization 3D点正则化<br>Diffusion Models 扩散模型 | Input: 2D videos with 3D point trajectories 2D视频与3D点轨迹<br>Step1: Data augmentation 数据增强<br>Step2: Model fine-tuning 模型微调<br>Step3: Regularization of shape and motion 形状与运动的正则化<br>Output: Enhanced video quality 改进的视频质量 |
8.5 | [[8.5] 2502.03836 Adapting Human Mesh Recovery with Vision-Language Feedback](https://arxiv.org/abs/2502.03836) <br> [{'name': 'Chongyang Xu, Buzhen Huang, Chengfang Zhang, Ziliang Feng, Yangang Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>human mesh recovery<br>vision-language models<br>3D reconstruction<br>diffusion-based framework | Input: Monocular images 单目图像<br>Step1: Initial pose prediction using a regression model 初始姿态预测<br>Step2: 2D keypoints extraction from images 从图像中提取2D关键点<br>Step3: Integration of vision-language descriptions 结合视觉语言描述<br>Step4: Refinement of 3D mesh using diffusion modeling 使用扩散模型优化3D网格<br>Output: Enhanced 3D human mesh 改进的3D人类网格 |
8.5 | [[8.5] 2502.03877 Advanced Object Detection and Pose Estimation with Hybrid Task Cascade and High-Resolution Networks](https://arxiv.org/abs/2502.03877) <br> [{'name': 'Yuhui Jin, Yaqiong Zhang, Zheyuan Xu, Wenqing Zhang, Jingyu Xu'}] | 6D Object Detection and Pose Estimation 6D对象检测与姿态估计 | v2<br>6D object detection<br>pose estimation<br>Hybrid Task Cascade<br>High-Resolution Network | Input: 6D object detection data 6D对象检测数据<br>Step1: Hybrid Task Cascade integration 集成混合任务级联<br>Step2: High-Resolution Network backbone usage 使用高分辨率网络骨干<br>Step3: Advanced post-processing techniques 先进的后处理技术<br>Output: Improved object detection and pose estimation models 改进的对象检测和姿态估计模型 |
8.5 | [[8.5] 2502.04111 Adaptive Margin Contrastive Learning for Ambiguity-aware 3D Semantic Segmentation](https://arxiv.org/abs/2502.04111) <br> [{'name': 'Yang Chen, Yueqi Duan, Runzhong Zhang, Yap-Peng Tan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Semantic Segmentation<br>Point Cloud Processing<br>Contrastive Learning | Input: 3D point cloud 数据集<br>Step1: Ambiguity estimation based on position embeddings 基于位置嵌入的模糊性估计<br>Step2: Development of adaptive margin contrastive learning algorithm 自适应边际对比学习算法开发<br>Step3: Evaluation on large-scale datasets 在大规模数据集上进行评估<br>Output: Improved semantic segmentation results 改进的语义分割结果 |
8.5 | [[8.5] 2502.04293 GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation](https://arxiv.org/abs/2502.04293) <br> [{'name': 'Weihang Li, Hongli Xu, Junwen Huang, Hyunjun Jung, Peter KT Yu, Nassir Navab, Benjamin Busam'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic shape<br>pose estimation | Input: Partial RGB-D observations 具有部分可见性的RGB-D观测<br>Step1: Semantic Shape Reconstruction (SSR) 语义形状重建<br>Step2: Global Context Enhanced (GCE) feature fusion module 全球上下文增强特征融合模块<br>Output: Enhanced object poses 改进的物体姿态 |
8.5 | [[8.5] 2502.04329 SMART: Advancing Scalable Map Priors for Driving Topology Reasoning](https://arxiv.org/abs/2502.04329) <br> [{'name': 'Junjie Ye, David Paz, Hengyuan Zhang, Yuliang Guo, Xinyu Huang, Henrik I. Christensen, Yue Wang, Liu Ren'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>autonomous driving<br>lane topology reasoning | Input: Standard-definition (SD) and satellite maps 标准清晰度和卫星地图<br>Step 1: Train map prior model to infer lane graphs 训练地图先验模型以推断车道图<br>Step 2: Integrate model with online topology reasoning models 将模型与在线拓扑推理模型集成<br>Output: Enhanced lane topology understanding 改进的车道拓扑理解 |
7.5 | [[7.5] 2502.03813 Optimized Unet with Attention Mechanism for Multi-Scale Semantic Segmentation](https://arxiv.org/abs/2502.03813) <br> [{'name': 'Xuan Li, Quanchao Lu, Yankaiqi Li, Muqing Li, Yijiashun Qi'}] | Image Generation 图像生成 | v2<br>semantic segmentation<br>attention mechanism<br>autonomous driving | Input: Multi-scale images 多尺度图像<br>Step1: Implement attention mechanism 实施注意力机制<br>Step2: Optimize Unet architecture 优化Unet架构<br>Step3: Evaluate on Cityscapes dataset 在Cityscapes数据集上评估<br>Output: Improved segmentation results 改进的分割结果 |
7.5 | [[7.5] 2502.04244 An object detection approach for lane change and overtake detection from motion profiles](https://arxiv.org/abs/2502.04244) <br> [{'name': 'Andrea Benericetti, Niccol\\`o Bellaccini, Henrique Pi\\~neiro Monteagudo, Matteo Simoncini, Francesco Sambo'}] | Autonomous Driving 自动驾驶 | v2<br>object detection<br>lane change<br>ADAS<br>motion profiles<br>autonomous driving | Input: Motion profile images 运动轮廓图像<br>Step1: Dataset creation 数据集创建<br>Step2: Object detection model development 目标检测模型开发<br>Step3: Performance evaluation 性能评估<br>Output: Detection of lane change and overtake maneuvers 车道变换和超车动作检测 |


## Arxiv 2025-02-06

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.02936 Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal Structures from Multi-view Joint Cloud](https://arxiv.org/abs/2502.02936) <br> [{'name': 'Junkun Jiang, Jie Chen, Ho Yin Au, Mingyuan Chen, Wei Xue, Yike Guo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Joint Cloud<br>multi-view motion capture | Input: Multi-view images 多视角图像<br>Step1: Triangulate 2D joints into Joint Cloud 将2D关节三角测量为联合云<br>Step2: Process using JCSAT to explore correlations 使用JCSAT处理以探索相关性<br>Step3: Utilize OTAP for feature selection 使用OTAP进行特征选择<br>Output: 3D motion estimation 3D运动估计 |
9.5 | [[9.5] 2502.03449 Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics](https://arxiv.org/abs/2502.03449) <br> [{'name': 'Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang'}] | 3D Reconstruction 三维重建 | v2<br>3D reconstruction<br>garment generation<br>multi-view images<br>simulation-ready | Input: In-the-wild image 单张图像<br>Step1: Pre-trained image-to-sewing pattern generation model 预训练的图像到缝制模式生成模型<br>Step2: Multi-view diffusion model for producing images 多视角扩散模型用于生成图像<br>Step3: Refinement using a differentiable garment simulator differentiable garment simulator 进行细化<br>Output: Simulation-ready 3D garment 适合模拟的三维服装 |
8.5 | [[8.5] 2502.02907 PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette Stacking](https://arxiv.org/abs/2502.02907) <br> [{'name': 'Jacopo Villa, Jay W. McMahon, Issa A. D. Nesnas'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D pole estimation<br>silhouette stacking | Input: Silhouette images from multiple camera poses 多个相机视角的轮廓图像<br>Step1: Create a silhouette-stack image 创建轮廓堆叠图像<br>Step2: Apply Discrete Fourier Transform to enhance robustness 应用离散傅里叶变换以增强鲁棒性<br>Step3: Estimate 3D pole orientation using projected-pole measurements 使用投影极坐标测量来估计3D极坐标方向<br>Output: Accurate pole orientation estimation 准确的极坐标方向估计 |
8.5 | [[8.5] 2502.02977 Disentangling CLIP Features for Enhanced Localized Understanding](https://arxiv.org/abs/2502.02977) <br> [{'name': 'Samyak Rawelekar, Yujun Cai, Yiwei Wang, Ming-Hsuan Yang, Narendra Ahuja'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>mutual feature information (MFI)<br>vision-language models (VLM)<br>multi-label recognition (MLR) | Input: CLIP features from vision-language models 视觉语言模型中的CLIP特征<br>Step1: Analyze feature correlation 分析特征相关性<br>Step2: Implement MFI loss 施加MFI损失<br>Step3: Align text and image features 对齐文本和图像特征<br>Output: Improved localized understanding 改进的局部理解 |
8.5 | [[8.5] 2502.03005 Driver Assistance System Based on Multimodal Data Hazard Detection](https://arxiv.org/abs/2502.03005) <br> [{'name': 'Long Zhouxiang, Ovanes Petrosian'}] | Autonomous Driving 自动驾驶 | v2<br>multimodal data<br>hazard detection<br>autonomous driving<br>incident recognition | Input: Multimodal data (video, audio) 输入：多模态数据（视频、音频）<br>Step1: Data integration 数据集成<br>Step2: Attention-based fusion strategy 基于注意力的融合策略<br>Step3: Incident recognition incidents 事件识别<br>Output: Enhanced detection accuracy 改进的检测精度 |
8.5 | [[8.5] 2502.03465 Seeing World Dynamics in a Nutshell](https://arxiv.org/abs/2502.03465) <br> [{'name': 'Qiuhong Shen, Xuanyu Yi, Mingbao Lin, Hanwang Zhang, Shuicheng Yan, Xinchao Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D representation<br>Monocular video<br>Dynamic Gaussian Splatting | Input: Monocular videos 单目视频<br>Step1: Transform videos to dynamic Gaussian representations 将视频转换为动态高斯表示<br>Step2: Introduce STAG representation 引入结构化时空对齐高斯表示<br>Step3: Optimizing for spatial and temporal coherence 进行空间和时间一致性的优化<br>Output: High-fidelity video reconstruction and spatial-temporal modeling 高保真视频重建和时空建模 |
7.5 | [[7.5] 2502.02951 VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA](https://arxiv.org/abs/2502.02951) <br> [{'name': 'Madhuri Latha Madaka, Chakravarthy Bhagvati'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>VQA dataset<br>Hierarchical questions | Input: Visual content and questions 视觉内容和问题<br>Step1: Dataset development 数据集开发<br>Step2: Classification of questions 问题分类<br>Step3: Initial testing on VQA systems 在VQA系统上的初步测试<br>Output: VQA-Levels dataset VQA-Levels数据集 |


## Arxiv 2025-02-05

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.01666 Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding](https://arxiv.org/abs/2502.01666) <br> [{'name': 'Jingming Xia, Guanqun Cao, Guang Ma, Yiben Luo, Qinzhao Li, John Oyekan'}] | Depth Estimation 深度估计 | v2<br>monocular depth estimation<br>3D reconstruction<br>generative models<br>autonomous driving | Input: RGB image<br>Step1: Extract latent features using Image Encoder<br>Step2: Extract semantic vector through Image Semantic Encoder<br>Step3: Integrate features within a denoising UNet<br>Step4: Generate final metric depth map<br>Output: Enhanced depth prediction |
9.5 | [[9.5] 2502.01846 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping](https://arxiv.org/abs/2502.01846) <br> [{'name': 'Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Arthur Chen, Srinath Sridhar, Aayush Prakash'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>diffusion models<br>3D generation<br>structured representation | Input: 3D Gaussian Splatting data 3D高斯点云数据<br>Step1: Spherical mapping to transform data into structured 2D representation 使用球面映射将数据转换为结构化2D表示<br>Step2: Multi-branch network for feature compression 使用多分支网络进行特征压缩<br>Step3: Integration with existing 2D models with zero-shot learning 将其与现有的2D模型进行无缝整合<br>Output: Structured 3D representation ready for generative tasks 输出：准备好用于生成任务的结构化3D表示 |
9.5 | [[9.5] 2502.01855 Learning Fine-to-Coarse Cuboid Shape Abstraction](https://arxiv.org/abs/2502.01855) <br> [{'name': 'Gregor Kobsik, Morten Henkel, Yanjiang He, Victor Czech, Tim Elsner, Isaak Lim, Leif Kobbelt'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>shape abstraction<br>cuboids<br>unsupervised learning<br>structural analysis | Input: Collections of 3D shapes 3D形状集<br>Step1: Initialize with fine reconstruction to capture details 细致重建以捕获细节<br>Step2: Gradually reduce primitives while optimizing loss 渐进减少原始体并优化损失<br>Step3: Evaluate performance on shape benchmarks 在形状基准上评估性能<br>Output: Compact cuboid-based representations 紧凑的立方体表示 |
9.5 | [[9.5] 2502.01856 Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2502.01856) <br> [{'name': 'Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, WonSook Lee'}] | 3D Object Detection 三维物体检测 | v2<br>LiDAR-camera fusion<br>3D object detection<br>autonomous driving | Input: LiDAR and camera data 数据<br>Step1: Spatio-Temporal Feature Aggregation (STFA) module processes input 提取时空特征<br>Step2: Reliability module assigns confidence scores 可靠性模块自信度评分<br>Step3: Confidence-Weighted Mutual Cross-Attention (CW-MCA) module balances information with confidence 用置信度动态平衡信息<br>Output: Enhanced 3D object detection 改进的三维物体检测 |
9.5 | [[9.5] 2502.01896 INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy](https://arxiv.org/abs/2502.01896) <br> [{'name': 'Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi'}] | 3D Perception and Modeling 3D 感知与建模 | v2<br>LiDAR<br>3D perception<br>object detection | Input: Noisy LiDAR data 噪声激光雷达数据<br>Step 1: Meta-learning phase 迁移学习阶段<br>Step 2: Generate robust saliency maps 生成健壮的显著性图<br>Step 3: Adversarial curriculum training 对抗性课程训练<br>Output: Enhanced noise resilience 提升噪声鲁棒性 |
9.5 | [[9.5] 2502.02163 Progressive Correspondence Regenerator for Robust 3D Registration](https://arxiv.org/abs/2502.02163) <br> [{'name': 'Guiyu Zhao, Sheng Ao, Ye Zhang, Kai Xu Yulan Guo'}] | 3D Registration 3D配准 | v2<br>3D registration<br>point cloud<br>outlier removal<br>reconstruction<br>robustness | Input: Point cloud data 点云数据<br>Step1: Prior-guided local grouping using generalized mutual matching 先验引导的局部分组与互匹配<br>Step2: Local correspondence correction using center-aware three-point consistency 局部对应关系修正<br>Step3: Global correspondence refinement using extensive iterations 全局对应关系的细化<br>Output: High-quality point correspondences 高质量的点对应关系 |
9.5 | [[9.5] 2502.02187 ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion](https://arxiv.org/abs/2502.02187) <br> [{'name': 'Nissim Maruani, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun'}] | 3D Generation 三维生成 | v2<br>3D Generation 3D生成<br>Shape Variations 形状变体 | Input: Reference 3D model 参考3D模型<br>Step1: Sparse voxel grid and point sampling 稀疏体素网格和点采样<br>Step2: Multiscale neural architecture training 多尺度神经架构训练<br>Step3: Generate shape variations 生成形状变体<br>Output: High-quality 3D shapes 高质量3D形状 |
9.5 | [[9.5] 2502.02247 Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning](https://arxiv.org/abs/2502.02247) <br> [{'name': 'Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud analysis 3D点云分析<br>domain generalization 域推广<br>rotation robustness 旋转鲁棒性 | Input: 3D point clouds 3D点云<br>Step 1: Identify challenging rotations 识别具有挑战性的旋转<br>Step 2: Construct intricate orientation set 构建复杂方向集<br>Step 3: Utilize contrastive learning against orientations 使用对比学习进行方向建模<br>Output: Generalizable features with rotation consistency 输出: 具有旋转一致性的可泛化特征 |
9.5 | [[9.5] 2502.02283 GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/abs/2502.02283) <br> [{'name': 'Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>Structure-from-Motion<br>point clouds<br>novel view synthesis | Input: Sparse SfM point clouds 稀疏结构光点云<br>Step1: Dynamic sampling dynamic sampling 动态采样<br>Step2: Gaussian Process modeling 高斯过程建模<br>Step3: Densification of point clouds 点云稠密化<br>Output: Enhanced 3D Gaussian representation 改进的3D高斯表示 |
9.5 | [[9.5] 2502.02334 Event-aided Semantic Scene Completion](https://arxiv.org/abs/2502.02334) <br> [{'name': 'Shangwei Guo, Hao Shi, Song Wang, Xiaoting Yin, Kailun Yang, Kaiwei Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>Semantic Scene Completion<br>3D Reconstruction | Input: Multi-view images 多视角图像<br>Step1: Data integration 数据集成<br>Step2: Algorithm development 算法开发<br>Step3: Model evaluation 模型评估<br>Output: Enhanced 3D models 改进的三维模型 |
9.5 | [[9.5] 2502.02338 Geometric Neural Process Fields](https://arxiv.org/abs/2502.02338) <br> [{'name': 'Wenzhe Yin, Zehao Xiao, Jiayi Shen, Yunlu Chen, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D scenes<br>probabilistic modeling | Input: Limited context images 限制的上下文图像<br>Step1: Probabilistic modeling 概率建模<br>Step2: Integrate geometric bases 集成几何基底<br>Step3: Hierarchical latent variable design 分层潜变量设计<br>Output: Improved generalization 改进的泛化能力 |
9.5 | [[9.5] 2502.02372 MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning](https://arxiv.org/abs/2502.02372) <br> [{'name': 'Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>avatar generation<br>continual learning | Input: Image data of avatars 头像图像数据<br>Step1: Implement continual learning strategy 进行持续学习策略<br>Step2: Develop Global-Local Joint Storage Module 开发全局-局部联合存储模块<br>Step3: Develop Pose Distillation Module 开发姿态提炼模块<br>Output: Maintainable virtual avatar 可维护虚拟头像 |
9.5 | [[9.5] 2502.02548 Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2502.02548) <br> [{'name': 'Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy'}] | 3D Segmentation 三维分割 | v2<br>3D segmentation<br>open-vocabulary<br>Vision-Language Models | Input: Multi-view images 多视角图像<br>Step1: Data generation 数据生成<br>Step2: Data annotation 数据注释<br>Step3: Training model 训练模型<br>Output: Open-vocabulary segmentation model 开放词汇分割模型 |
9.5 | [[9.5] 2502.02590 Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling](https://arxiv.org/abs/2502.02590) <br> [{'name': 'Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D articulated objects<br>Vision-Language Models<br>3D modeling | Input: 3D meshes 3D 网格<br>Step1: Movable Part Segmentation 可动部分分割<br>Step2: Articulation Estimation 关节估计<br>Step3: Refinement 精化<br>Output: Articulated 3D objects 装配式三维物体 |
9.2 | [[9.2] 2502.01940 Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach](https://arxiv.org/abs/2502.01940) <br> [{'name': 'Mohammed Alsakabi, Aidan Erickson, John M. Dolan, Ozan K. Tonguz'}] | Autonomous Driving 自动驾驶 | v2<br>3D reconstruction<br>autonomous driving<br>depth maps | Input: Images from 4D radar detectors and RGB cameras 4D 雷达探测器和 RGB 摄像头的图像<br>Step1: Integrate radar depth maps and RGB images 集成雷达深度图和 RGB 图像<br>Step2: Apply pixel positional encoding algorithm 应用像素位置信息编码算法<br>Step3: Develop spectrum estimation algorithms 研发光谱估计算法<br>Step4: Train depth map generative models 训练深度图生成模型<br>Output: Enhanced depth maps 改进的深度图 |
9.2 | [[9.2] 2502.02144 DOC-Depth: A novel approach for dense depth ground truth generation](https://arxiv.org/abs/2502.02144) <br> [{'name': 'Simon de Moreau, Mathias Corsia, Hassan Bouchiba, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Fabien Moutarde'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Reconstruction 三维重建<br>Dense Depth Generation 密集深度生成<br>LiDAR 激光雷达 | Input: LiDAR sensor data 利用激光雷达传感器数据<br>Step1: 3D environment reconstruction 3D环境重建<br>Step2: Dynamic object classification 动态对象分类<br>Step3: Dense depth generation 密集深度生成<br>Output: Dense depth annotation output 输出：密集深度标注 |
8.5 | [[8.5] 2502.01814 PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://arxiv.org/abs/2502.01814) <br> [{'name': 'Dazhou Yu, Genpei Zhang, Liang Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>polyhedral representation<br>surface-attributed graph | Input: Polyhedral data 多面体数据<br>Step1: Decompose into local rigid representations 将其分解为局部刚性表示<br>Step2: Hierarchical aggregation of representations 层次聚合表示<br>Output: Global representation of polyhedra 全球多面体表示 |
8.5 | [[8.5] 2502.01894 SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset](https://arxiv.org/abs/2502.01894) <br> [{'name': 'Goodarz Mehr, Azim Eskandarian'}] | Autonomous Systems and Robotics 自主系统与机器人 | v2<br>Synthetic Data Generation 合成数据生成<br>Autonomous Driving 自动驾驶<br>BEV Representation 鸟瞰视图表示 | Input: Multi-sensor data collection 多传感器数据收集<br>Step1: Configuration of synthetic data generation 生成合成数据的配置<br>Step2: Data generation for BEV representation 生成鸟瞰视图表示的数据<br>Step3: Annotation of perception data 性能数据的标注<br>Output: SimBEV dataset with annotated driving scenarios 输出: 包含标注的驾驶场景的SimBEV数据集 |
8.5 | [[8.5] 2502.01949 LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.01949) <br> [{'name': 'Yang Zhou, Zongjin He, Qixuan Li, Chao Wang'}] | 3D Generation 三维生成 | 3D scene generation<br>physically consistent layouts<br>text-guided generation | Input: Text prompt 文本提示<br>Step1: Convert text to scene graph 将文本转换为场景图<br>Step2: Adjust Gaussian densities and layouts 调整高斯密度和布局<br>Step3: Make dynamic camera adjustments 进行动态相机调整<br>Output: 3D compositional scene generation 3D 组合场景生成 |
8.5 | [[8.5] 2502.01961 Hierarchical Consensus Network for Multiview Feature Learning](https://arxiv.org/abs/2502.01961) <br> [{'name': 'Chengwei Xia, Chaoxi Niu, Kun Zhan'}] | Multi-view and Stereo Vision 多视角立体 | v2<br>multiview feature learning<br>hierarchical consensus<br>3D reconstruction | Input: Multi-view images 多视角图像<br>Step1: Learning view-consistency features 学习视图一致性特征<br>Step2: Hierarchical consensus derivation 层次共识推导<br>Step3: Comprehensive feature extraction 综合特征提取<br>Output: Discriminative features 具有区分性的特征 |
8.5 | [[8.5] 2502.02091 Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation](https://arxiv.org/abs/2502.02091) <br> [{'name': 'JooHyun Kwon, Hanbyel Cho, Junmo Kim'}] | Image and Video Generation 图像生成 | v2<br>4D Gaussian Splatting<br>dynamic scene editing<br>computer vision<br>motion artifacts | Input: 4D dynamic scene data 4D动态场景数据<br>Step1: Model static 3D Gaussians 模型静态三维高斯<br>Step2: Implement Hexplane-based deformation field 实现基于Hexplane的变形场<br>Step3: Perform editing on static 3D Gaussians 在静态三维高斯上执行编辑<br>Step4: Apply score distillation for refinement 应用得分蒸馏进行细化<br>Output: Enhanced edited dynamic scenes 改进的编辑动态场景 |
8.5 | [[8.5] 2502.02322 Improving Generalization Ability for 3D Object Detection by Learning Sparsity-invariant Features](https://arxiv.org/abs/2502.02322) <br> [{'name': 'Hsin-Cheng Lu, Chung-Yi Lin, Winston H. Hsu'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection 3D物体检测<br>autonomous driving 自动驾驶<br>generalization 泛化 | Input: Source domain 3D point clouds 源域3D点云<br>Step1: Downsample the point cloud based on confidence scores 根据置信度得分下采样点云<br>Step2: Teacher-student framework to align BEV features 使用师生框架对齐鸟瞰视图特征<br>Step3: Apply FCA and GERA to maintain consistency 使用FCA和GERA保持一致性<br>Output: Domain-agnostic 3D object detector 域无关的3D物体检测器 |
8.5 | [[8.5] 2502.02468 High-Fidelity Human Avatars from Laptop Webcams using Edge Compute](https://arxiv.org/abs/2502.02468) <br> [{'name': 'Akash Haridas, Imran N. Junejo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Morphable Models 3D可变形模型<br>Photo-realistic Rendering 照相真实渲染<br>Avatar Generation 头像生成 | Input: Images from consumer-grade laptop webcams 笔记本电脑网络摄像头拍摄的图像<br>Step1: Shape generation by fitting 3DMM shape parameters 通过拟合3D形状模型参数生成形状<br>Step2: Texture map generation 纹理图生成<br>Step3: Rendering using pre-defined parameters 使用预定义参数进行渲染<br>Output: High-fidelity animatable avatars 高保真可动画化头像 |
8.5 | [[8.5] 2502.02537 Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks](https://arxiv.org/abs/2502.02537) <br> [{'name': 'Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, Fei Miao'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Collaborative Object Detection<br>Uncertainty Quantification<br>Adversarial Attacks<br>Autonomous Driving | Input: Collaborative Object Detection (COD) models 协作目标检测模型<br>Step1: Apply adversarial training adversarially during collaboration 在协作中施加对抗性训练<br>Step2: Provide output uncertainty estimation through learning-based module 提供基于学习的模块输出的不确定性估计<br>Step3: Calibrate uncertainty using conformal prediction 对不确定性进行校准<br>Output: Enhanced object detection accuracy 提高的目标检测准确性 |
7.5 | [[7.5] 2502.01906 Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models](https://arxiv.org/abs/2502.01906) <br> [{'name': 'Chia-Wen Kuo, Sijie Zhu, Fan Chen, Xiaohui Shen, Longyin Wen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>Decomposed Attention<br>cross-modal learning | Input: Visual and textual embeddings 视觉和文本嵌入<br>Step1: Decompose the self-attention mechanism 解构自注意力机制<br>Step2: Optimize visual-to-visual self-attention 视觉-视觉自注意力优化<br>Step3: Merge visual and textual information 视觉与文本信息合并<br>Output: Improved efficiency and performance of LVLMs 提高LVLM效率与性能 |
7.5 | [[7.5] 2502.01969 Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration](https://arxiv.org/abs/2502.01969) <br> [{'name': 'Younan Zhu, Linwei Tao, Minjing Dong, Chang Xu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucination<br>attention calibration | Input: Large Vision-Language Models (LVLMs) 大型视觉语言模型<br>Step1: Bias estimation from input image 输入图像的偏差估计<br>Step2: Uniform Attention Calibration (UAC) application 应用统一注意力校准<br>Step3: Dynamic Attention Calibration (DAC) implementation 实现动态注意力校准<br>Output: Reduced object hallucination 减少物体幻觉 |


## Arxiv 2025-02-05

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.01814 PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://arxiv.org/abs/2502.01814) <br> [{'name': 'Dazhou Yu, Genpei Zhang, Liang Zhao'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>polyhedral representation | Input: 3D polyhedral objects 3D 多面体对象<br>Step1: Surface-attributed graph construction 表面属性图构建<br>Step2: Local rigid representation learning 局部刚性表示学习<br>Step3: Hierarchical aggregation of representations 表示的分层聚合<br>Output: Global representation of polyhedra 全球多面体表示 |
9.5 | [[9.5] 2502.01846 UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping](https://arxiv.org/abs/2502.01846) <br> [{'name': 'Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Arthur Chen, Srinath Sridhar, Aayush Prakash'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D Gaussian Splatting<br>UV Mapping<br>image-based generation<br>3D reconstruction 3D重建 | Input: 3D Gaussian Splatting (3DGS) data 3D高斯点云数据<br>Step1: Spherical mapping to create a structured 2D representation 使用球面映射创建结构化的2D表示<br>Step2: Compression of heterogeneous features into a shared feature space 将异构特征压缩到共享特征空间<br>Step3: Integration with pre-trained 2D generative models 与预训练的2D生成模型集成<br>Output: Structured 2D UV Gaussian Splatting representation 结构化的2D UV高斯点云表示 |
9.5 | [[9.5] 2502.01856 Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection](https://arxiv.org/abs/2502.01856) <br> [{'name': 'Reza Sadeghian, Niloofar Hooshyaripour, Chris Joslin, WonSook Lee'}] | 3D Object Detection 3D目标检测 | v2<br>3D object detection<br>LiDAR-camera fusion<br>autonomous driving | Input: Sensor data from LiDAR and camera LiDAR和摄像头的传感器数据<br>Step1: Integration of spatial and semantic information 空间和语义信息的集成<br>Step2: Implementation of Reliability module to assess confidence 实现可靠性模块以评估置信度<br>Step3: Use of CW-MCA for dynamic weighting of modalities 使用CW-MCA对模态进行动态加权<br>Output: Robust 3D object detection results 稳健的3D目标检测结果 |
9.5 | [[9.5] 2502.01940 Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach](https://arxiv.org/abs/2502.01940) <br> [{'name': 'Mohammed Alsakabi, Aidan Erickson, John M. Dolan, Ozan K. Tonguz'}] | Depth Estimation 深度估计 | v2<br>Depth Estimation 深度估计<br>Autonomous Vehicles 自动驾驶<br>Radar-RGB Integration 雷达- RGB集成 | Input: Radar depth maps and RGB images 雷达深度图和RGB图像<br>Step1: Pixel positional encoding 像素位置编码<br>Step2: Transformation to Spatial Spectrum 转换为空间谱<br>Step3: Generating denser depth maps 生成更密集的深度图<br>Output: Enhanced depth maps 改进的深度图 |
9.5 | [[9.5] 2502.02144 DOC-Depth: A novel approach for dense depth ground truth generation](https://arxiv.org/abs/2502.02144) <br> [{'name': 'Simon de Moreau, Mathias Corsia, Hassan Bouchiba, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Fabien Moutarde'}] | Depth Estimation 深度估计 | v2<br>depth estimation 深度估计<br>LiDAR<br>3D reconstruction 三维重建 | Input: LiDAR measurements LiDAR测量<br>Step1: Data aggregation 数据聚合<br>Step2: Dynamic object classification 动态物体分类<br>Step3: Dense depth generation 密集深度生成<br>Output: Fully-dense depth annotations 完全密集的深度注解 |
9.5 | [[9.5] 2502.02163 Progressive Correspondence Regenerator for Robust 3D Registration](https://arxiv.org/abs/2502.02163) <br> [{'name': 'Guiyu Zhao, Sheng Ao, Ye Zhang, Kai Xu Yulan Guo'}] | 3D Registration 3D 注册 | v2<br>3D registration<br>point cloud registration | Input: Point clouds from different perspectives 从不同视角获得点云<br>Step1: Prior-guided local grouping prior引导局部分组<br>Step2: Generalized mutual matching 广义互匹配<br>Step3: Center-aware three-point consistency center-aware三点一致性<br>Step4: Global correspondence refinement 全局对应关系精炼<br>Output: High-quality correspondences 高质量对应关系 |
9.5 | [[9.5] 2502.02187 ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion](https://arxiv.org/abs/2502.02187) <br> [{'name': 'Nissim Maruani, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun'}] | 3D Generation 三维生成 | v2<br>3D Generation<br>shape variations<br>multiscale neural architecture<br>interactive generation | Input: A single reference 3D model 单一参考3D模型<br>Step1: Shape variations generation 形状变体生成<br>Step2: Multiscale diffusion sampling 多尺度扩散采样<br>Step3: Interactive editing 交互式编辑<br>Output: High-quality 3D shape variants 高质量3D形状变体 |
9.5 | [[9.5] 2502.02247 Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning](https://arxiv.org/abs/2502.02247) <br> [{'name': 'Bangzhen Liu, Chenxi Zheng, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Shengfeng He'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D point cloud<br>domain generalization<br>rotation robustness | Input: Point clouds with variable orientations 变量方向的点云<br>Step1: Identify challenging rotations 识别具有挑战性的旋转<br>Step2: Construct intricate orientation set 构建复杂方向集<br>Step3: Apply contrastive learning using intricate samples 使用复杂样本进行对比学习<br>Output: Enhanced orientation-aware 3D representations 改进的方向感知3D表示 |
9.5 | [[9.5] 2502.02283 GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/abs/2502.02283) <br> [{'name': 'Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>Gaussian Processes<br>novel view synthesis | Input: Sparse SfM point clouds 稀疏的结构光点云<br>Step1: Develop MOGP model 开发多输出高斯过程模型<br>Step2: Adaptive sampling and filtering strategy 自适应采样和过滤策略<br>Step3: Densify the point clouds 使点云密集化<br>Output: High-quality 3D Gaussians 高质量的3D高斯 |
9.5 | [[9.5] 2502.02322 Improving Generalization Ability for 3D Object Detection by Learning Sparsity-invariant Features](https://arxiv.org/abs/2502.02322) <br> [{'name': 'Hsin-Cheng Lu, Chung-Yi Lin, Winston H. Hsu'}] | 3D Object Detection 3D物体检测 | v2<br>3D object detection<br>autonomous driving<br>domain generalization | Input: LiDAR point clouds from various domains 各种域的LiDAR点云<br>Step1: Data subsampling based on confidence scores 根据置信度评分进行数据子采样<br>Step2: Teacher-student framework implementation 教师-学生框架实施<br>Step3: Feature alignment between domains 域间特征对齐<br>Output: Generalized 3D object detector 具备良好泛化能力的3D物体检测器 |
9.5 | [[9.5] 2502.02334 Event-aided Semantic Scene Completion](https://arxiv.org/abs/2502.02334) <br> [{'name': 'Shangwei Guo, Hao Shi, Song Wang, Xiaoting Yin, Kailun Yang, Kaiwei Wang'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>semantic scene completion<br>autonomous driving<br>event cameras | Input: Event and RGB images 输入：事件图像和RGB图像<br>Step1: Data integration 数据集成<br>Step2: Event-aided Lifting Module (ELM) 事件辅助提升模块开发<br>Step3: 3D scene reconstruction 三维场景重建<br>Output: Enhanced 3D semantic occupancy models 输出：改进的3D语义占用模型 |
9.5 | [[9.5] 2502.02338 Geometric Neural Process Fields](https://arxiv.org/abs/2502.02338) <br> [{'name': 'Wenzhe Yin, Zehao Xiao, Jiayi Shen, Yunlu Chen, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>Geometric Neural Process Fields<br>3D reconstruction | Input: Limited context observations 有限上下文观察<br>Step 1: Formulate NeF generalization as a probabilistic problem 将NeF泛化表述为一个概率问题<br>Step 2: Design geometric bases to encode structural information 设计几何基以编码结构信息<br>Step 3: Develop a hierarchical latent variable model for parameterization 建立分层潜变量模型以进行参数化<br>Output: Improved generalization for novel scenes and signals 改进的新场景和信号的泛化能力 |
9.5 | [[9.5] 2502.02548 Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2502.02548) <br> [{'name': 'Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Chris Choy'}] | 3D Segmentation 三维分割 | v2<br>3D segmentation 3D分割<br>open-vocabulary 开放词汇 | Input: 3D scene datasets 3D场景数据集<br>Step1: Data generation data generation 数据生成<br>Step2: Model training 模型训练<br>Step3: Segmentation validation 分割验证<br>Output: Open-vocabulary 3D segmentation results 开放词汇3D分割结果 |
9.5 | [[9.5] 2502.02590 Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling](https://arxiv.org/abs/2502.02590) <br> [{'name': 'Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D modeling<br>articulated objects 3D建模<br>可动物体 | Input: 3D mesh 输入: 3D网格<br>Step1: Movable Part Segmentation 可移动部分分割<br>Step2: Articulation Estimation and Refinement 动作估计与精细化<br>Output: Articulated 3D object 输出: 可动的3D物体 |
9.0 | [[9.0] 2502.01666 Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding](https://arxiv.org/abs/2502.01666) <br> [{'name': 'Jingming Xia, Guanqun Cao, Guang Ma, Yiben Luo, Qinzhao Li, John Oyekan'}] | Depth Estimation 深度估计 | v2<br>Monocular Depth Estimation 单目深度估计<br>Autonomous Driving 自动驾驶<br>3D Reconstruction 三维重建 | Input: Single RGB image 单个RGB图像<br>Step1: Image-based semantic embedding image-based using SeeCoder 图像语义嵌入<br>Step2: Integration of features via denoising UNet 特征集成通过去噪UNet<br>Step3: Depth map generation 深度图生成<br>Output: Enhanced depth map 改进的深度图 |
9.0 | [[9.0] 2502.01855 Learning Fine-to-Coarse Cuboid Shape Abstraction](https://arxiv.org/abs/2502.01855) <br> [{'name': 'Gregor Kobsik, Morten Henkel, Yanjiang He, Victor Czech, Tim Elsner, Isaak Lim, Leif Kobbelt'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D shape abstraction<br>unsupervised learning<br>cuboids | Input: Collections of 3D shapes 三维形状集合<br>Step1: Initial fine reconstruction 初始化细致重建<br>Step2: Apply fine-to-coarse abstraction fine-to-coarse abstraction<br>Step3: Optimize reconstruction and volume preservation 优化重建与体积保持<br>Output: Cuboid-based structural abstraction cuboid 基于的结构抽象 |
8.5 | [[8.5] 2502.01894 SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset](https://arxiv.org/abs/2502.01894) <br> [{'name': 'Goodarz Mehr, Azim Eskandarian'}] | Autonomous Driving 自动驾驶 | v2<br>BEV perception<br>synthetic data generation<br>autonomous driving | Input: Multi-sensor data 多传感器数据<br>Step1: Data generation 生成数据<br>Step2: Ground truth capture 捕获真实数据<br>Step3: Dataset creation 创建数据集<br>Output: Comprehensive BEV dataset 完整的鸟瞩图数据集 |
8.5 | [[8.5] 2502.01896 INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy](https://arxiv.org/abs/2502.01896) <br> [{'name': 'Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi'}] | 3D Point Cloud Processing 点云处理 | v2<br>LiDAR<br>adversarial training<br>3D perception | Input: Noisy LiDAR data 噪声LiDAR数据<br>Step1: Prepare saliency maps 准备显著性图<br>Step2: Apply adversarial curriculum training 应用对抗课程训练<br>Step3: Train student network 训练学生网络<br>Output: Robust deep learning model 稳健的深度学习模型 |
8.5 | [[8.5] 2502.01949 LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.01949) <br> [{'name': 'Yang Zhou, Zongjin He, Qixuan Li, Chao Wang'}] | 3D Generation 三维生成 | 3D scene generation<br>3D Gaussian Splatting<br>physics-guided generation | Input: Text prompt 文本提示<br>Step1: Convert text to scene graph 将文本转换为场景图<br>Step2: Adjust density and layout 调整密度和布局<br>Step3: Dynamic camera adjustments 动态相机调整<br>Output: Compositional 3D scenes 组合三维场景 |
8.5 | [[8.5] 2502.01961 Hierarchical Consensus Network for Multiview Feature Learning](https://arxiv.org/abs/2502.01961) <br> [{'name': 'Chengwei Xia, Chaoxi Niu, Kun Zhan'}] | Multi-view and Stereo Vision 多视角与立体视觉 | v2<br>Multiview Learning 多视角学习<br>Consensus Learning 共识学习<br>Feature Integration 特征整合 | Input: Multi-view data 多视角数据<br>Step1: Learn distinct and common information 学习独特和共同信息<br>Step2: Derive consensus indices 生成共识指标<br>Step3: Perform hierarchical consensus learning 进行分层共识学习<br>Output: Comprehensive and discriminative features 详尽和有辨识度的特征 |
8.5 | [[8.5] 2502.01969 Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration](https://arxiv.org/abs/2502.01969) <br> [{'name': 'Younan Zhu, Linwei Tao, Minjing Dong, Chang Xu'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>object hallucination | Input: LVLMs with visual tokens 视觉语言模型与视觉标记<br>Step1: Analyze attention biases 分析注意力偏差<br>Step2: Implement UAC for calibration 实施均匀注意力校准<br>Step3: Develop DAC for dynamic adjustment 开发动态注意力校准模块<br>Output: Improved alignment and reduced hallucination 输出: 改进的对齐和减少的幻觉 |
8.5 | [[8.5] 2502.02171 DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging](https://arxiv.org/abs/2502.02171) <br> [{'name': 'Mohamed Youssef, Jian Peng, Oliver Bimber'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>remote sensing<br>vegetation analysis | Input: Aerial images from drones 通过无人机获取航空图像<br>Step1: Synthetic-aperture imaging 合成孔径成像<br>Step2: Use 3D convolutional neural networks to reduce out-of-focus signals 使用3D卷积神经网络减少模糊信号<br>Step3: Combine multiple reflectance stacks from various spectral channels 结合来自不同光谱通道的多重反射堆栈<br>Output: Volumetric representations of vegetation 体积植被表示 |
8.5 | [[8.5] 2502.02372 MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by Continual Learning](https://arxiv.org/abs/2502.02372) <br> [{'name': 'Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng'}] | Neural Rendering 神经渲染 | v2<br>Neural Radiance Fields<br>3D rendering<br>continual learning | Input: Limited training data 对应的有限训练数据<br>Step1: Employ NeRF for 3D rendering 使用NeRF进行3D渲染<br>Step2: Implement a Global-Local Joint Storage Module 实现全局-局部联合存储模块<br>Step3: Utilize a Pose Distillation Module 使用姿态蒸馏模块<br>Output: Maintainable virtual avatars 可维护的虚拟 avatar |
8.5 | [[8.5] 2502.02468 High-Fidelity Human Avatars from Laptop Webcams using Edge Compute](https://arxiv.org/abs/2502.02468) <br> [{'name': 'Akash Haridas Imran N. Junejo'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>3D reconstruction<br>avatar generation<br>differentiable rendering | Input: Consumer-grade laptop webcam images 使用普通笔记本电脑网络摄像头的图像<br>Step1: Shape generation using 3D morphable models 使用3D可变形模型生成形状<br>Step2: Landmark detection using optimization 标记检测使用优化<br>Step3: Texture generation with GANs 使用GAN生成纹理<br>Step4: Differentiable rendering to create avatars 使用可微渲染创建虚拟形象<br>Output: High-fidelity human avatars 高保真度人类虚拟形象 |
8.5 | [[8.5] 2502.02525 Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation](https://arxiv.org/abs/2502.02525) <br> [{'name': 'Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, Ajmal Mian'}] | Object Pose Estimation 物体姿态估计 | v2<br>9-DoF object pose estimation<br>domain generalization<br>robotic grasping | Input: Rendered synthetic data 渲染合成数据<br>Step1: Model training 模型训练<br>Step2: Pose estimation 估计姿态<br>Step3: Real-time performance optimization 实时性能优化<br>Output: Estimated 9-DoF object poses 估计的9自由度物体姿态 |
8.5 | [[8.5] 2502.02537 Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks](https://arxiv.org/abs/2502.02537) <br> [{'name': 'Huiqun Huang, Cong Chen, Jean-Philippe Monteuuis, Jonathan Petit, Fei Miao'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>Collaborative Object Detection<br>Uncertainty Quantification<br>Adversarial Robustness<br>Autonomous Vehicles | Input: Collaborative object detection models 协作目标检测模型<br>Step1: Adversarial training for robustness 对抗训练以增强鲁棒性<br>Step2: Uncertainty quantification estimation 不确定性量化估计<br>Step3: Calibration of uncertainty using conformal prediction 使用保形预测进行不确定性校准<br>Output: Enhanced object detection accuracy 改进的目标检测准确性 |
8.0 | [[8.0] 2502.01890 Geometric Framework for 3D Cell Segmentation Correction](https://arxiv.org/abs/2502.01890) <br> [{'name': 'Peter Chen, Bryan Chang, Olivia Annette Creasey, Julie Beth Sneddon, Yining Liu'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D Segmentation 3D分割<br>Geometric Framework 几何框架 | Input: 2D cell segmentation results 2D细胞分割结果<br>Step1: Extract geometric features 提取几何特征<br>Step2: Train binary classifier 训练二元分类器<br>Step3: Correct segmentation errors 修正分割错误<br>Output: Accurate 3D cell body reconstruction 精确的3D细胞体重建 |
8.0 | [[8.0] 2502.01906 Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models](https://arxiv.org/abs/2502.01906) <br> [{'name': 'Chia-Wen Kuo, Sijie Zhu, Fan Chen, Xiaohui Shen, Longyin Wen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Vision-Language Models<br>Decomposed Attention<br>Computational Efficiency | Input: Visual and textual embeddings 视觉和文本嵌入<br>Step1: Decompose the attention mechanism 分解注意力机制<br>Step2: Optimize visual-to-visual self-attention 优化视觉间自注意力<br>Step3: Debias positional encodings 去偏差位置编码<br>Output: Enhanced processing of visual and textual embeddings 改进的视觉和文本嵌入处理 |
7.5 | [[7.5] 2502.02225 Exploring the latent space of diffusion models directly through singular value decomposition](https://arxiv.org/abs/2502.02225) <br> [{'name': 'Li Wang, Boyan Gao, Yanran Li, Zhao Wang, Xiaosong Yang, David A. Clifton, Jun Xiao'}] | Image Generation 图像生成 | v2<br>diffusion models<br>image editing<br>latent space<br>Singular Value Decomposition<br>image generation | Input: Latent space of diffusion models 扩散模型的潜在空间<br>Step1: Investigate latent space using Singular Value Decomposition (SVD) 通过奇异值分解（SVD）研究潜在空间<br>Step2: Discover properties of latent space 发现潜在空间的属性<br>Step3: Propose image editing framework based on properties 提出基于属性的图像编辑框架<br>Output: Enhanced image editing capabilities 改进的图像编辑能力 |


## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯点云辐射场<br>novel view synthesis 新视图合成 | Input: Posed 2D image data 2D图像数据<br>Step1: Extract per-image 2D segmentation masks 提取每帧的2D分割掩码<br>Step2: 2D-to-3D lifting to assign unique object IDs 在3D中分配唯一对象ID的2D到3D提升流程<br>Step3: Incremental merging of object fragments into coherent objects 将对象片段合并成一致的对象<br>Output: High-quality 3D object segments 高质量的3D对象片段 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Shape Generation 3D形状生成 | v2<br>3D reconstruction<br>shape generation<br>semantic input | Input: Semantic descriptions 语义描述<br>Step1: Distill 3D geometry from 2D diffusion models 从2D扩散模型提取3D几何<br>Step2: Refine textures using image and video generation models 使用图像和视频生成模型细化纹理<br>Step3: Represent the refined 3D model with neural implicit representations 使用神经隐式表示来表示细化的3D模型<br>Output: Fabricable high-quality meshes 可制造的高质量网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR-camera calibration<br>3D reconstruction<br>autonomous driving | Input: LiDAR and camera data 激光雷达和相机数据<br>Step1: Environment interpretation 环境解读<br>Step2: Data fusion 数据融合<br>Step3: Dual-path correspondence matching 双色通道对应匹配<br>Step4: Spatial-temporal optimization 空间-时间优化<br>Output: Accurate extrinsic calibration 精准的外部标定 |
9.5 | [[9.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>generative models<br>dynamic avatars | Input: Monocular video 单目视频<br>Step1: Generative prior usage 生成优先级使用<br>Step2: Dual-Space Optimization 双空间优化<br>Step3: View selection strategy 视图选择策略<br>Step4: Pose feature injection 姿势特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人形象 |
9.5 | [[9.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Few-Shot NeRF<br>3D Reconstruction<br>Neural Rendering | Input: Limited input views 有限的输入视角<br>Step1: Frequency control frequency control<br>Step2: Curriculum training curriculum training<br>Step3: Scene reconstruction scene reconstruction<br>Output: Accurate 3D representations 准确的三维表示 |
9.2 | [[9.2] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>hazard detection<br>vision-language model<br>autonomous driving | Input: Multimodal data fusion 多模态数据融合<br>Step1: Semantic and visual inputs integration 语义和视觉输入集成<br>Step2: Supervised fine-tuning of vision-language models 有监督微调视觉语言模型<br>Step3: Hazard detection and edge case evaluation 危险检测和边缘案例评估<br>Output: Enhanced situational awareness 改进的情境意识 |
9.2 | [[9.2] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Object Detection 3D对象检测 | v2<br>3D object detection 3D对象检测<br>monocular vision 单目视觉<br>depth estimation 深度估计 | Input: Monocular images 单目图像<br>Step1: Depth estimation using Vision Transformer 步骤1：使用视觉Transformer进行深度估计<br>Step2: Feature extraction with Hierarchical Feature Fusion 步骤2：利用层次特征融合提取特征<br>Step3: Object detection using DETR architecture 步骤3：使用DETR架构进行对象检测<br>Output: 3D bounding boxes for detected objects 输出：检测到对象的3D边界框 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 目标检测 | v2<br>4D Radar<br>3D object detection<br>energy efficiency<br>autonomous driving | Input: 4D Radar point clouds 4D雷达点云<br>Step1: Convert RTNH to SNN architecture 将RTNH转换为SNN架构<br>Step2: Implement biological top-down inference (BTI) 实现生物学自上而下推理(BTI)<br>Step3: Model evaluation and comparison 模型评估与比较<br>Output: Energy-efficient 3D object detection model 能源高效的3D目标检测模型 |
8.5 | [[8.5] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D Scene Question Answering<br>multimodal models | Input: 3D scene representation and query 3D场景表示和查询<br>Step1: Systematic literature review 系统文献综述<br>Step2: Dataset analysis 数据集分析<br>Step3: Methodology evaluation 方法评估<br>Output: Comprehensive insights and challenges on 3D SQA 对3D SQA的综合见解和挑战 |
8.5 | [[8.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Video Generation 视频生成 | v2<br>video generation<br>interpolation<br>extrapolation<br>latent flow matching | Input: Video frames 视频帧<br>Step1: Model latent flow 模型潜在流<br>Step2: Polynomial projection 多项式投影<br>Step3: Generate time-dependent frames 生成时间相关帧<br>Output: Video with interpolation and extrapolation 带插值和外推的视频 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | v2<br>3D generation<br>compositional scenes<br>large language models | Input: Complex scene descriptions 复杂场景描述<br>Step1: Semantic parsing and relationship extraction 语义解析和关系提取<br>Step2: Scene graph generation 场景图生成<br>Step3: 2D and 3D asset generation 2D和3D资产生成<br>Step4: Layout prediction and planning 布局预测与规划<br>Output: High-quality 3D compositional scenes 高质量三维组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>Vision-Language Models<br>Autonomous Driving | Input: Visual Question Answering task in autonomous driving 视觉问答任务之于自动驾驶<br>Step1: Integrate Vision-Language Models with continual learning 结合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放与知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用特定任务的投影层正则化<br>Output: Enhanced VQA performance in autonomous driving environments 改进的自动驾驶环境中的视觉问答性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning in Scenes 三维场景推理 | v2<br>3D reasoning<br>visual question answering<br>hypothetical reasoning | Input: Context change descriptions 上下文变化描述<br>Step1: Dataset construction 数据集构建<br>Step2: Model evaluation 模型评估<br>Output: Performance analysis 性能分析 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Semantic Segmentation 三维语义分割 | v2<br>3D semantic segmentation<br>domain adaptation<br>pseudo labels<br>autonomous driving | Input: 3D point cloud and SAM masks 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 类别标签确定（使用投票法）<br>Step2: Application of filtering constraints to unreliable labels 对不可靠标签应用过滤约束<br>Step3: Geometry-Aware Progressive Propagation (GAPP) for label propagation 到所有3D点进行标签传播（GAPP方法）<br>Output: Enhanced pseudo-labels and improved segmentation performance 输出：改进的伪标签和增强的分割性能 |
8.5 | [[8.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>image generation<br>video generation<br>state-space models<br>transformer models | Input: Images and video sequences 图像和视频序列<br>Step1: Develop SSM-Transformer hybrid model 开发SSM-Transformer混合模型<br>Step2: Efficient processing of visual sequences 高效处理视觉序列<br>Step3: Generate images and videos 生成图像和视频<br>Output: High-quality images and dynamic videos 高质量图像和动态视频 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>6D pose estimation<br>bin-picking<br>zero-shot learning<br>robotic manipulation | Input: RGB-D image and CAD model 输入: RGB-D图像和CAD模型<br>Step1: Object detection 物体检测<br>Step2: Point cloud extraction 点云提取<br>Step3: Position-Aware Correspondence learning 位置感知对应学习<br>Step4: Pose estimation 位置估计<br>Output: 6D pose predictions 输出: 6D姿态预测 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | v2<br>differentiable rendering<br>volumetric meshes<br>real-time rendering | Input: Volumetric mesh representations 体积网格表示<br>Step1: Mesh parameterization 网格参数化<br>Step2: Differentiable ray tracing 可微光线追踪<br>Step3: Rendering and evaluation 渲染与评估<br>Output: Real-time rendering results 实时渲染结果 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Systems and Robotics 自动驾驶机器人系统 | v2<br>road segmentation<br>autonomous vehicles<br>image registration<br>deep learning | Input: Roadside camera images 道路监控摄像头图像<br>Step1: Automatic data collection 自动数据收集<br>Step2: Semi-automatic annotation method 开发半自动注释方法<br>Step3: Image registration to correct labels 图像配准以修正标签<br>Output: Enhanced road segmentation models 改进的道路分割模型 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>Visual Inertial Odometry<br>Initialization<br>Feature Matching<br>AR<br>VR | Input: Visual Inertial Odometry (VIO) data 视觉惯性里程计数据<br>Step1: Initialization using gyroscope and visual measurements 初始化算法<br>Step2: Hybrid feature matching using optical flow and descriptor methods 特征匹配<br>Step3: Evaluation on benchmarks and practical applications 验证和实际应用<br>Output: Enhanced VIO performance 改进的VIO性能 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Autonomous Driving 自动驾驶 | v2<br>3D multi-object tracking<br>4D Radar | Input: 4D Radar data 4D雷达数据<br>Step1: Object detection using Bayesian approximation 基于贝叶斯近似进行目标检测<br>Step2: Motion prediction with transformer network 使用变换器网络进行运动预测<br>Step3: Two-stage data association integrating Doppler measurements 两阶段数据关联，整合多普勒测量<br>Output: Accurate 3D MOT results 准确的3D多目标跟踪结果 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉基础 | v2<br>3D visual grounding<br>Large Language Model<br>3D reconstruction<br>vision-language model | Input: Referring utterances and 3D scene scans 参考话语和三维场景扫描<br>Step1: Parse utterance into symbolic expression 将话语解析为符号表达式<br>Step2: Generate spatial relation features 生成空间关系特征<br>Step3: Use VLM to process visual information 使用视觉语言模型处理视觉信息<br>Output: Identified target object 确定目标对象 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | v2<br>Generative Adversarial Networks<br>Data Augmentation<br>Image Generation | Input: Limited training data 有限训练数据<br>Step 1: Estimate covariance matrices 估计协方差矩阵<br>Step 2: Identify semantic transformation directions 确定语义转换方向<br>Step 3: Apply adversarial semantic augmentation 应用对抗性语义增强<br>Output: Improved generation quality 改进的生成质量 |
7.5 | [[7.5] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>knowledge forgetting<br>general attributes | Input: Pretrained Vision-Language Models (VLMs) 预训练视觉语言模型<br>Step1: Generating General Attribute Descriptions 生成通用属性描述<br>Step2: Establishing Vision-GA-Class Associations 建立视觉-通用属性-类关联<br>Step3: Tuning Visual Encoder 调整视觉编码器<br>Output: Enhanced Adaptation with Reduced Knowledge Forgetting 改进的适应性，减少知识遗忘 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | v2<br>Diffusion Model<br>Image Generation<br>Video Generation | Input: Diffusion Model (DM) diffusion模型<br>Step1: Analyze variance and bias variance和偏差分析<br>Step2: Develop Recursive Likelihood Ratio optimizer 开发递归似然比优化器<br>Step3: Validate on image and video tasks 在图像和视频任务上验证<br>Output: Fine-tuned model 改进的模型 |
7.0 | [[7.0] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>inductive biases<br>generalization | Input: Visual and textual stimuli 视觉和文本刺激<br>Step1: Inductive bias analysis 偏置分析<br>Step2: Experimental paradigm application 实验范式应用<br>Step3: Data collection and evaluation 数据收集与评估<br>Output: Insights on model generalization 关于模型泛化的见解 |
6.5 | [[6.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal learning<br>Large Language Models<br>parameter-efficient learning<br>Vision-Language Models | Input: Vision-language models 视觉-语言模型<br>Step1: Categorize and review VLLMs 对VLLMs进行分类和审查<br>Step2: Discuss training paradigms 讨论训练范式<br>Step3: Summarize benchmarks 总结基准测试<br>Output: Comprehensive survey report 综合调查报告 |


## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建 | 3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯喷溅辐射场 | Input: 2D segmentation masks 2D分割掩码<br>Step1: Feature integration 特征集成<br>Step2: 3D Gaussian lifting 3D高斯提升<br>Step3: Segmentation application 分割应用<br>Output: 3D segmented assets 3D分割资产 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Generation 三维生成 | 3D reconstruction<br>shape generation<br>semantics | Input: Multi-view semantics 多视角语义<br>Step1: Semantic input analysis 语义输入分析<br>Step2: Geometry and appearance distillation from 2D models 从2D模型提取几何与外观<br>Step3: Image restoration and detail enhancement 图像修复与细节增强<br>Step4: Shape reconstruction using neural SDF representation 使用神经签名距离场重建形状<br>Output: Complex detailed 3D meshes 复杂细节的三维网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | LiDAR-camera calibration<br>3D reconstruction<br>data fusion | Input: LiDAR and camera data LiDAR和相机数据<br>Step1: Environmental interpretation 环境解释<br>Step2: Dual-path correspondence matching 双路径对应匹配<br>Step3: Spatial-temporal optimization 空间时间优化<br>Output: Precise extrinsic calibration 精确的外部标定 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 三维物体检测 | 3D object detection<br>neural networks<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Process high-density point clouds 处理高密度点云<br>Step2: Implement spiking neural network architecture 实现脉冲神经网络架构<br>Step3: Apply biological top-down inference (BTI) 应用生物学的自上而下推理法<br>Output: Efficient 3D object detection results 高效的三维物体检测结果 |
8.5 | [[8.5] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Driving 自动驾驶 | hazard detection<br>autonomous driving<br>multimodal data fusion | Input: Multimodal data 输入: 多模态数据<br>Step1: Data integration 数据集成<br>Step2: Hazard detection 危险检测<br>Step3: Spatial localization 空间定位<br>Output: Enhanced hazard prediction 改进的危险预测 |
8.5 | [[8.5] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Reconstruction 三维重建 | 3D object detection<br>depth estimation | Input: Monocular images 单目图像<br>Step1: Feature extraction using Vision Transformer 基于视觉变换器的特征提取<br>Step2: Depth estimation using a relative depth model 使用相对深度模型进行深度估计<br>Step3: Object detection using DETR architecture 使用DETR架构进行物体检测<br>Output: Enhanced 3D object detection capabilities 改进的3D物体检测能力 |
8.5 | [[8.5] 2502.00528 Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings](https://arxiv.org/abs/2502.00528) <br> [{'name': 'Zachary Huemann, Samuel Church, Joshua D. Warner, Daniel Tran, Xin Tie, Alan B McMillan, Junjie Hu, Steve Y. Cho, Meghan Lubner, Tyler J. Bradshaw'}] | VLM & VLA 视觉语言模型 | 3D vision-language model<br>PET/CT<br>visual grounding | Input: PET/CT reports and images PET/CT 报告和图像<br>Step1: Automation of weak labeling pipeline 弱标记生成管道自动化<br>Step2: Data extraction from reports 报告中数据提取<br>Step3: Training of ConTEXTual Net 3D 训练 ConTEXTual Net 3D<br>Output: 3D visual grounding model 3D 视觉定位模型 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | text-to-3D generation<br>compositional scenes<br>physics-guided generation | Input: Complex scene descriptions 复杂场景描述<br>Step1: Scene graph generation 场景图生成<br>Step2: Asset creation using multimodal agents 使用多模态代理进行资产创建<br>Step3: Layout prediction with physical model 使用物理模型进行布局预测<br>Output: Compositional scenes with physical rationality 具有物理合理性的组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | Vision-Language Models<br>Visual Question Answering<br>autonomous driving<br>continual learning | Input: Visual Question Answering tasks in autonomous driving 在自动驾驶中的视觉问答任务<br>Step1: Integrate Vision-Language Models with continual learning 整合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放和知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用任务特定投影层正则化<br>Output: Improved VQA system performance 改进的视觉问答系统性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning 3D推理 | 3D reasoning<br>Visual Question Answering<br>scene understanding | Input: Context changes and indoor scene descriptions 上下文变化和室内场景描述<br>Step1: Benchmark formulation 基准测试制定<br>Step2: Model evaluation models performance evaluation 模型性能评估<br>Output: Hypothetical reasoning capabilities 设想推理能力 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Reconstruction and Modeling 三维重建 | 3D semantic segmentation<br>domain adaptation<br>pseudo-labels<br>autonomous driving | Input: 3D point cloud and SAM masks 输入: 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 步骤1: 使用投票法确定类别标签<br>Step2: Unreliable mask label filtering using constraints 步骤2: 使用约束过滤不可靠的掩码标签<br>Step3: Geometry-Aware Progressive Propagation (GAPP) to propagate mask labels 步骤3: 使用几何感知逐步传播来传递掩码标签<br>Output: Enhanced pseudo-labels with improved quality 输出: 质量提升的增强伪标签 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶 | 6D pose estimation<br>bin-picking<br>robotic manipulation<br>zero-shot learning | Input: Scene instances and CAD models 场景实例与CAD模型<br>Step1: Feature extraction 特征提取<br>Step2: Position-aware correspondence learning 基于位置的对应学习<br>Step3: Pose estimation 位置估计<br>Output: Accurate 6D poses 准确的6D姿势 |
8.5 | [[8.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction 三维重建 | 3D human reconstruction<br>photorealistic rendering | Input: Monocular video 单目视频<br>Step1: Dual-Space Optimization 双空间优化<br>Step2: Score Distillation Sampling (SDS) 评分蒸馏采样<br>Step3: View Selection_strategy 视图选择策略<br>Step4: Pose Feature Injection 姿态特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人类虚拟形象 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | differentiable rendering<br>ray tracing<br>computer vision | Input: Scene representations 场景表示<br>Step1: Implement volumetric mesh ray tracing 实现体积网格光线追踪<br>Step2: Develop a novel scene representation 发展新场景表示<br>Step3: Evaluate rendering speed and quality 评估渲染速度和质量<br>Output: Real-time rendering model 实时渲染模型 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Driving 自动驾驶 | road segmentation<br>deep learning<br>autonomous vehicles<br>data annotation | Input: Roadside camera feeds 路边摄像头视频<br>Step1: Manual labeling of one frame 手动标注一帧<br>Step2: Transfer labels to other frames 转移标签到其他帧<br>Step3: Compensate for camera movements 使用频域图像配准补偿相机位移<br>Output: Semi-automatically labeled road data 半自动标注的道路数据 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Visual Odometry 视觉里程计 | Visual Inertial Odometry<br>Structure from Motion<br>Augmented Reality<br>Virtual Reality | Input: Visual inertial measurements 视觉惯性测量<br>Step1: Robust initialization initialization 稳健初始化<br>Step2: Feature matching 特征匹配<br>Step3: State estimation 状态估计<br>Output: Accurate visual inertial odometry result 精确的视觉惯性里程计结果 |
8.5 | [[8.5] 2502.01356 Quasi-Conformal Convolution : A Learnable Convolution for Deep Learning on Riemann Surfaces](https://arxiv.org/abs/2502.01356) <br> [{'name': 'Han Zhang, Tsz Lok Ip, Lok Ming Lui'}] | 3D Reconstruction and Modeling 3D重建 | 3D facial analysis<br>Riemann surfaces | Input: Geometric data and Riemann surfaces 几何数据和黎曼曲面<br>Step1: Define quasi-conformal mappings 定义准保形映射<br>Step2: Develop Quasi-Conformal Convolution operators 开发准保形卷积算子<br>Step3: Implement Quasi-Conformal Convolutional Neural Network (QCCNN) 实现准保形卷积神经网络<br>Output: Adaptive convolution for geometric data 自适应卷积用于几何数据 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Robotic Perception 机器人感知 | 3D multi-object tracking<br>Bayesian approximation<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Motion prediction using transformer-based network 使用基于变换器的网络进行运动预测<br>Step2: Bayesian approximation for detection and prediction 步骤 2: 检测和预测中的贝叶斯近似<br>Step3: Two-stage data association leveraging Doppler measurements 基于多普勒测量的两阶段数据关联<br>Output: Enhanced multi-object tracking performance 提升的多目标跟踪性能 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉定位 | 3D visual grounding<br>weakly supervised learning | Input: 3D visual information and language 3D视觉信息与语言<br>Step1: Code generation using LLM 通过LLM生成代码<br>Step2: Spatial relationship computation 空间关系计算<br>Step3: Quality evaluation and optimization 质量评估和优化<br>Output: Efficient grounding results 高效的定位结果 |
8.5 | [[8.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction 三维重建 | Few-Shot NeRFs 少样本神经辐射场<br>3D Reconstruction 三维重建 | Input: Scene images 场景图像<br>Step1: Curriculum training curriculum training 课程训练<br>Step2: Feature parameterization 特征参数化<br>Step3: Scene complexity increment 增加场景复杂性<br>Output: High-quality reconstruction 高质量重建 |
8.0 | [[8.0] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | 3D Reconstruction and Modeling 3D重建与建模 | 3D scene question answering<br>multimodal modelling<br>datasets | Input: 3D scene data 3D场景数据<br>Step1: Systematic review of datasets 数据集的系统评审<br>Step2: Analysis of methodologies 方法论分析<br>Step3: Evaluation of metrics 评估指标<br>Output: Comprehensive understanding of 3D SQA 3D场景问答的综合理解 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | Generative Adversarial Networks<br>data augmentation<br>image synthesis<br>semantic features | Input: Limited image datasets 有限图像数据集<br>Step1: Estimate covariance matrices 估计协方差矩阵<br>Step2: Identify meaningful transformation directions 识别有意义的转化方向<br>Step3: Apply transformations to semantic features 对语义特征应用转化<br>Output: Enhanced synthetic images 增强合成图像 |
7.5 | [[7.5] 2502.00333 BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution](https://arxiv.org/abs/2502.00333) <br> [{'name': 'Kai Liu, Kaicheng Yang, Zheng Chen, Zhiteng Li, Yong Guo, Wenbo Li, Linghe Kong, Yulun Zhang'}] | Image Generation 图像生成 | super-resolution<br>diffusion model<br>binarization<br>model compression | Input: Diffusion model for super-resolution 超分辨率扩散模型<br>Step1: Binarization of model models 模型的二值化<br>Step2: One-step distillation into extreme compression 一步蒸馏以实现极端压缩<br>Step3: Integration of sparse and low rank matrix branches 结合稀疏和低秩矩阵分支<br>Output: Compressed and accelerated super-resolution model 压缩和加速的超分辨率模型 |
7.5 | [[7.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Image and Video Generation 图像生成 | video generation<br>interpolation<br>extrapolation | Input: Video frames 视频帧<br>Step1: Hypothesis generation 假设生成<br>Step2: Optimal projection approximation 最优投影近似<br>Step3: Interpolation and extrapolation 插值和外推<br>Output: Time-dependent video frames 时间依赖视频帧 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | Diffusion Model<br>image generation<br>video generation | Input: Probabilistic diffusion model 概率扩散模型<br>Step1: Pre-training on unlabeled data 在无标签数据上进行预训练<br>Step2: Recursive Likelihood Ratio optimizer proposal 提出递归似然比优化器<br>Step3: Implementation of zero-order gradient estimation 零阶梯度估计的实施<br>Output: Aligned diffusion models 对齐的扩散模型 |
7.5 | [[7.5] 2502.00662 Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation](https://arxiv.org/abs/2502.00662) <br> [{'name': 'Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki'}] | VLM & VLA 视觉语言模型与对齐 | vision-language models<br>out-of-distribution detection<br>few-shot learning | Input: ID image and text prototypes 输入: ID图像和文本原型<br>Step1: Theoretical analysis 理论分析<br>Step2: Incorporation of image prototypes 图像原型的整合<br>Step3: Development of biased prompts generation (BPG) module 偏差提示生成(BPG)模块的开发<br>Step4: Implementation of image-text consistency (ITC) module 图像文本一致性(ITC)模块的实施<br>Output: Enhanced VLM-based OOD detection performance 输出: 改进的基于VLM的OOD检测性能 |
7.5 | [[7.5] 2502.00711 VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework](https://arxiv.org/abs/2502.00711) <br> [{'name': 'Chunbai Zhang, Chao Wang, Yang Zhou, Yan Peng'}] | Vision-Language Models (VLMs) 视觉语言模型 | visual reasoning<br>evidence-based reasoning<br>VLM | Input: Visual information (images/videos) 输入: 视觉信息（图像/视频）<br>Step1: Extract fine-grained visual knowledge from visual relationships 第一步: 从视觉关系中提取细粒度视觉知识<br>Step2: Paraphrase questions with underspecification using extracted knowledge 第二步: 利用提取的知识对欠规范的问题进行改写<br>Step3: Employ Chain-of-Evidence prompting for interpretable reasoning 第三步: 使用证据链提示进行可解释推理<br>Output: Enhanced visual reasoning capabilities 输出: 改进的视觉推理能力 |
7.5 | [[7.5] 2502.00719 Vision and Language Reference Prompt into SAM for Few-shot Segmentation](https://arxiv.org/abs/2502.00719) <br> [{'name': 'Kosuke Sakurai, Ryotaro Shimizu, Masayuki Goto'}] | VLM & VLA 视觉语言模型与对齐 | few-shot segmentation<br>vision-language model | Input: Annotated reference images and text labels 参考图像和文本标签<br>Step1: Input visual and semantic reference信息输入视觉和语义参考<br>Step2: Integrate prompt embeddings into SAM 将提示嵌入集成到SAM<br>Step3: Few-shot segmentation via VLP-SAM 通过VLP-SAM进行少样本分割<br>Output: High-performance segmentation results 高性能的分割结果 |
7.5 | [[7.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image Generation 图像生成 | image generation<br>video generation | Input: Visual sequences 视觉序列<br>Step1: Model development 模型开发<br>Step2: Integration of SSM and Transformers SSM与变换器的整合<br>Step3: Evaluation of generated outputs 生成结果的评估<br>Output: Generated images and videos 生成的图像和视频 |
7.5 | [[7.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | VLM & VLA 视觉语言模型与对齐 | Vision-Language<br>Large Language Models<br>parameter efficiency | Step1: Introduce architecture of LLMs 介绍LLM架构<br>Step2: Discuss parameter-efficient learning methods 讨论参数效率学习方法<br>Step3: Present taxonomy of modality integrators 提出模态集成器分类<br>Step4: Review training paradigms and efficiency considerations 回顾训练范式及效率考虑<br>Step5: Compare experimental results of representative models 比较代表模型的实验结果 |
7.5 | [[7.5] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>inductive biases<br>generalization | Input: Stimuli presented in vision and text 视觉和文本中呈现的刺激<br>Step1: Conduct experiments 进行实验<br>Step2: Analyze generalization across models 分析模型间的概括性<br>Output: Insights on inductive biases regarding shape and color 对形状和颜色的归纳偏见的见解 |
5.0 | [[5.0] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>continual adaptation<br>attribute descriptions | Input: Visual features and class text visuals 视觉特征和类别文本<br>Step1: Generate general attribute descriptions 生成一般属性描述<br>Step2: Design anchor-based embedding filter 设计基于锚点的嵌入过滤器<br>Step3: Tune visual encoder 调整视觉编码器<br>Output: Robust vision-GA-class associations 稳健的视觉-一般属性-类别关联 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent representation<br>3D高斯渲染<br>视角依赖表示 | input: images 图片<br>extend the 3D Gaussian Splatting model 扩展3D高斯渲染模型<br>introduce an additional symmetric matrix 引入额外的对称矩阵<br>achieve view-dependent opacity representation 实现视角依赖的透明度表示<br>output: improved 3D scene reconstruction 输出：改进的3D场景重建 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction 三维重建 | 3D reconstruction<br>3D Gaussian Splatting<br>endoscopic SLAM<br>depth reconstruction<br>三维重建<br>3D高斯斑点<br>内窥镜SLAM<br>深度重建 | input: endoscopic image sequences 内窥镜图像序列<br>Step 1: tracking using Gaussian Splatting 使用高斯斑点的跟踪<br>Step 2: mapping and bundle adjustment 映射与束调整<br>Step 3: surface normal-aware reconstruction 结合表面法向量进行重构<br>output: accurate 3D reconstruction and real-time tracking 输出: 精确的3D重建与实时跟踪 |
8.5 | [[8.5] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction  三维重建 | Point Cloud Completion<br>3D Shape Completion<br>Knowledge Distillation<br>Points Completion<br>点云补全<br>3D形状补全<br>知识蒸馏<br>点补全 | input: incomplete point cloud  有缺失的点云<br>step1: apply autoencoder to encode the point cloud  应用自编码器对点云进行编码<br>step2: use knowledge distillation for completion  使用知识蒸馏进行补全<br>step3: output: completed 3D shape  输出：完整的3D形状 |
8.5 | [[8.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>Gaussian Splatting<br>3D高斯喷溅<br>高斯喷溅 | Input: 2D images 2D图像<br>Ray-tracing mechanism 射线追踪机制<br>Intersection computation 交点计算<br>Ray-tracing algorithms construction 射线追踪算法构建<br>Final 3D object with lighting and shadows 最终带有光影效果的三维物体 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>3D reconstruction<br>实时渲染<br>3D高斯分喷<br>三维重建 | input: 3D key points (输入：3D关键点)<br>Step 1: Create a joint-driven 3D Gaussian representation (步骤1：创建联合驱动的3D高斯表示)<br>Step 2: Implement differentiable spatial transformations (步骤2：实现可微分的空间变换)<br>Step 3: Apply real-time shadow simulation method (步骤3：应用实时阴影模拟方法)<br>output: High-fidelity hand images (输出：高保真的手部图像) |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>物体生成<br>3D高斯 | input: 3D assets 3D资产<br>extract: physical properties 提取物理属性<br>generate: physics-based dynamics 生成基于物理的动态<br>output: dynamic scene 输出动态场景 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自动驾驶 | LiDAR<br>loop closure detection<br>graph attention networks<br>place recognition<br>semanitic registration<br>激光雷达<br>回环闭合检测<br>图注意力网络<br>地点识别<br>语义注册 | input: semantic graphs 语义图<br>step1: encode semantic graphs using graph attention networks 使用图注意力网络编码语义图<br>step2: compare graph vectors to identify loop closure 比较图向量以识别回环闭合<br>step3: estimate 6 DoF pose constraint using semantic registration 使用语义注册估计6自由度位姿约束<br>output: loop closure detection results 回环闭合检测结果 |
7.5 | [[7.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | Autonomous Driving<br>Neuromorphic Vision<br>Real-time Navigation<br>Autonomous Systems<br>自驾驶<br>神经形态视觉<br>实时导航<br>自主系统 | Input: Human speech commands 人类语音指令<br>Step 1: Translate speech into planning commands 将语音翻译成规划指令<br>Step 2: Execute commands using neuromorphic vision 执行命令使用神经形态视觉<br>Step 3: Navigate and avoid obstacles in real-time 实时导航和避免障碍<br>Output: Autonomous drone navigation output 自主无人机导航输出 |
7.5 | [[7.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | video generation<br>text-to-video models<br>视频生成<br>文本到视频模型 | input: diffusion model inputs 输入：扩散模型输入<br>step1: align video frames with text prompts 步骤1：将视频帧与文本提示对齐<br>step2: utilize a beam search strategy to optimize output 使用束搜索策略优化输出<br>step3: compute metrics for perceptual quality evaluation 计算感知质量评估的指标<br>output: high-quality, aligned video generation 输出：高质量、对齐的视频生成 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自动驾驶 | Semantic Segmentation<br>LiDAR Imaging<br>Autonomous Driving<br>合成分割<br>LiDAR成像<br>自动驾驶 | input: LiDAR data 输入: LiDAR 数据<br>step1: generate synthetic dataset 生成合成数据集<br>step2: utilize CARLA simulator 使用 CARLA 模拟器<br>step3: train segmentation algorithms 训练分割算法<br>output: improved segmentation performance 输出: 改进的分割性能 |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | personalized portrait generation<br>identity preservation<br>view-consistent reconstruction<br>个性化肖像生成<br>身份保留<br>视角一致重建 | input: reference images 参考图像<br>step1: Lighting-Aware Stitching 光照感知拼接<br>step2: View-Consistent Adaptation 视角一致自适应<br>step3: ControlNet-like supervision 控制网络样监督<br>output: personalized portraits 个性化肖像 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving (自动驾驶) | Monocular Positioning<br>Extended Kalman Filter<br>Deep Learning<br>Single-shot<br>单目定位<br>扩展卡尔曼滤波<br>深度学习<br>单次 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integrate APR and RPR using EKF 通过扩展卡尔曼滤波整合APR和RPR<br>output: accurate positioning results 精确定位结果 |
6.0 | [[6.0] 2501.19331v1 Consistent Video Colorization via Palette Guidance](http://arxiv.org/abs/2501.19331v1) | Video Generation 视频生成 | Video Colorization<br>Stable Video Diffusion<br>Palette Guidance<br>视频上色<br>稳定视频扩散<br>调色板引导 | input: video sequences 视频序列<br>step 1: design palette-based color guider 设计调色板引导器<br>step 2: utilize Stable Video Diffusion as base model 利用稳定视频扩散作为基础模型<br>step 3: generate vivid colors using color context 根据颜色上下文生成生动的颜色<br>output: colorized video sequences 上色的视频序列 |
5.5 | [[5.5] 2501.18865v1 REG: Rectified Gradient Guidance for Conditional Diffusion Models](http://arxiv.org/abs/2501.18865v1) | Image Generation 图像生成 | conditional generation<br>diffusion models<br>conditional generation 条件生成<br>扩散模型 | input: guidance techniques 指导技术<br>step1: replace the scaled marginal distribution target 替换缩放的边际分布目标<br>step2: implement rectified gradient guidance 实施矩形梯度指导<br>step3: conduct experiments on image generation tasks 进行图像生成任务的实验<br>output: improved image generation results 改进的图像生成结果 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 三维生成 | 3D Gaussian Splatting<br>Ray Tracing<br>3D高斯点云<br>光线追踪 | input: 2D images 2D图像<br>process: Gaussian Splatting 高斯点云渲染<br>process: ray tracing based on Gaussian primitives 基于高斯原始体的光线追踪<br>output: 3D objects with light and shadow effects 输出具有光影效果的3D物体 |
9.0 | [[9.0] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent rendering<br>3D高斯点云<br>视角依赖的渲染 | input: 3D scene reconstruction from images 3D场景重建从图像中提取<br>step 1: extend 3D Gaussian Splatting model 扩展3D高斯点云模型<br>step 2: introduce symmetric matrix to enhance opacity representation 引入对称矩阵以增强不透明性表示<br>step 3: optimize suppression of Gaussians based on viewer perspective 根据观察者视角优化高斯的抑制<br>output: improved representation of view-dependent reflections and specular highlights 输出：改进视角依赖的反射和镜面高光的表示 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction  三维重建 | 3D Gaussian Splatting<br>SLAM<br>endoscopic reconstruction<br>depth reconstruction<br>3D 高斯点<br>SLAM<br>内窥镜重建<br>深度重建 | input: endoscopic images 内窥镜图像<br>step1: surface normal-aware tracking 表面法线感知跟踪<br>step2: accurate mapping 精确地图构建<br>step3: bundle adjustment 捆绑调整<br>output: geometrically accurate 3D reconstruction 准确的三维重建 |
8.5 | [[8.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | Text-to-video<br>Diffusion models<br>Video generation<br>评分调整<br>文本转视频<br>扩散模型<br>视频生成<br>奖励校准 | input: video generation prompts 视频生成提示<br>step1: employ diffusion latent beam search 使用扩散潜在光束搜索<br>step2: maximize alignment reward 最大化对齐奖励<br>step3: improve perceptual quality 提升感知质量<br>output: high-quality video optimized for natural movement 输出：高质量视频，优化自然运动 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>animatable hand avatar<br>3D高斯喷涂<br>可动画手部化身 | input: 3D key points 3D关键点<br>Jointly 3D Gaussian Splatting (3DGS) joint-driven representation 联合3D高斯喷涂（3DGS）驱动表示<br>apply spatial transformations based on 3D key points 基于3D关键点应用空间变换<br>real-time rendering and shadow simulation 实时渲染和阴影模拟<br>output: animatable high-fidelity hand images 输出：可动画的高保真手部图像 |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>3D生成<br>3D高斯 | input: user-specified prompts 用户指定的提示<br>step1: define a scene according to user prompts 根据用户提示定义场景<br>step2: estimate material weighting factors using a pretrained video diffusion model 使用预训练的视频扩散模型估计材料权重因子<br>step3: represent each 3D asset as a collection of constitutive 3D Gaussians 将每个3D资产表示为一组组成的3D高斯分布<br>output: a physics-based 3D dynamic scene 输出：基于物理的3D动态场景 |
8.0 | [[8.0] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction三维重建 | Point Cloud Completion<br>Multi-view Distillation<br>3D Shape Recovery<br>点云补全<br>多视图蒸馏<br>3D形状恢复 | input: incomplete point cloud 输入: 不完整的点云<br>step1: apply autoencoder architecture 应用自编码器架构<br>step2: use knowledge distillation strategy to enhance completion 使用知识蒸馏策略以增强完成度<br>step3: output: completed point cloud 输出: 完整的点云 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自主驾驶 | Loop Closure Detection<br>Semantic Graphs<br>Graph Attention Networks<br>闭环检测<br>语义图<br>图注意力网络 | input: point cloud 输入: 点云<br>step1: encode semantic graphs using graph attention networks 步骤1: 使用图注意力网络编码语义图<br>step2: generate graph vectors through self-attention mechanisms 步骤2: 通过自注意力机制生成图向量<br>step3: compare graph vectors to detect loop closure 步骤3: 比较图向量以检测闭环<br>output: loop closure candidates 输出: 闭环候选 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自主驾驶 | Semantic segmentation<br>LiDAR imaging<br>autonomous driving<br>合成分割<br>LiDAR成像<br>自主驾驶 | input: LiDAR images (输入: LiDAR图像)<br>modify CARLA simulator (修改CARLA模拟器)<br>generate SynthmanticLiDAR dataset (生成SynthmanticLiDAR数据集)<br>evaluate with transfer learning (使用迁移学习进行评估)<br>output: improved semantic segmentation performance (输出: 改进的语义分割性能) |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | Personalized Portrait Generation<br>3D-aware relighting<br>个性化肖像生成<br>具3D感知的重光照 | Input: reference portrait images 参考肖像图像<br>Step 1: Lighting-Aware Stitching 具光照感知的拼接<br>Step 2: View-Consistent Adaptation 具视图一致的适配<br>Output: personalized portraits with identity preservation 具有身份保留的个性化肖像 |
7.0 | [[7.0] 2501.19243v1 Accelerating Diffusion Transformer via Error-Optimized Cache](http://arxiv.org/abs/2501.19243v1) | Image Generation 图像生成 | Image Generation<br>Diffusion Transformer<br>ImageNet Dataset<br>图像生成<br>扩散变换器<br>ImageNet数据集 | input: Diffusion Transformer features (扩散变换器特征)<br>extract caching differences (提取缓存差异)<br>optimize cache based on errors (基于错误优化缓存)<br>output: improved generated images (输出: 改进的生成图像) |
6.5 | [[6.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | autonomous driving<br>natural language processing<br>neuroscience<br>autonomous navigation<br>自主驾驶<br>自然语言处理<br>神经科学<br>自主导航 | input: human speech and dynamic environment 输入：人类语言和动态环境<br>step1: translate human speech into planning commands 步骤1：将人类语言翻译为规划命令<br>step2: navigate and avoid obstacles using neuromorphic vision 步骤2：利用神经形态视觉导航并避免障碍物<br>output: real-time autonomous navigation output 实时自主导航结果 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving 自主驾驶 | monocular positioning<br>extended kalman filter<br>variational bayesian inference<br>单目定位<br>扩展卡尔曼滤波<br>变分贝叶斯推理 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integration with Extended Kalman Filter (EKF) 通过扩展卡尔曼滤波整合<br>output: accurate positional predictions 准确的位置信息预测 |


## Arxiv 2025-01-30

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
8.5 | [[8.5] 2501.18594v1 Foundational Models for 3D Point Clouds: A Survey and Outlook](http://arxiv.org/abs/2501.18594v1) | 3D reconstruction 3D重建 | 3D point clouds<br>foundational models<br>3D视觉理解<br>基础模型<br>3D点云 | input: 3D point clouds 3D点云<br>step1: review of foundational models FMs 基础模型的回顾<br>step2: categorize use of FMs in 3D tasks 分类基础模型在3D任务中的应用<br>step3: summarize state-of-the-art methods 总结最新的方法<br>output: comprehensive overview of FMs for 3D understanding 输出：基础模型在3D理解中的综合概述 |
8.5 | [[8.5] 2501.18162v1 IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain](http://arxiv.org/abs/2501.18162v1) | Autonomous Driving 自动驾驶 | 3D object detection<br>autonomous driving<br>3D对象检测<br>自动驾驶 | input: roadside data and vehicle-side data<br>In-Domain Query Interaction module learns content and depth information<br>Cross-Domain Query Enhancement decouples queries into semantic and geometry parts<br>outputs enhanced object queries |
8.5 | [[8.5] 2501.18110v1 Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR Mapping Systems](http://arxiv.org/abs/2501.18110v1) | 3D reconstruction 三维重建 | 3D Mapping<br>3D Reconstruction<br>Lifelong Mapping<br>激光雷达<br>三维映射<br>三维重建<br>终身映射 | Input: Hand-held and robot-mounted LiDAR maps 输入：手持和机器人安装的激光雷达地图<br>Dynamic point removal algorithm 动态点去除算法<br>Multi-session map alignment using feature descriptor matching and fine registration 多会话地图对齐，使用特征描述符匹配和精细配准<br>Map change detection to identify changes between aligned maps 地图变化检测以识别对齐地图之间的变化<br>Map version control for maintaining current environmental state and querying changes 地图版本控制，用于维护当前环境状态和查询变化 |
8.0 | [[8.0] 2501.18595v1 ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer](http://arxiv.org/abs/2501.18595v1) | Mesh Reconstruction 网格重建 | Mesh Reconstruction<br>3D reconstruction<br>网格重建<br>三维重建 | input: limited set of images 限制的图像集<br>step1: optimize mesh geometry 优化网格几何形状<br>step2: refine mesh with spatially adaptive resolution 使用空间自适应分辨率细化网格<br>step3: reconstruct high-resolution textures 重新构建高分辨率纹理<br>output: textured mesh with detailed appearance 带有详细外观的纹理网格 |
7.5 | [[7.5] 2501.18590v1 DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models](http://arxiv.org/abs/2501.18590v1) | Rendering Techniques 渲染技术 | Inverse Rendering<br>Forward Rendering<br>Video Diffusion Models<br>Inverse渲染<br>正向渲染<br>视频扩散模型 | input: real-world videos, 真实世界视频<br>step1: estimate G-buffers using inverse rendering model, 使用逆向渲染模型估计G-buffer<br>step2: generate photorealistic images from G-buffers, 从G-buffer生成照片级真实图像<br>output: relit images, material edited images, realistic object insertions, 重新照明图像，材料编辑图像，逼真的物体插入 |
7.5 | [[7.5] 2501.18315v1 Surface Defect Identification using Bayesian Filtering on a 3D Mesh](http://arxiv.org/abs/2501.18315v1) | Mesh Reconstruction 网格重建 | 3D Mesh<br>Mesh Reconstruction<br>3D网格<br>网格重建 | input: CAD model and point cloud data 输入：CAD模型和点云数据<br>transform CAD model into polygonal mesh 将CAD模型转换为多边形网格<br>apply weighted least squares algorithm 应用加权最小二乘算法<br>estimate state based on point cloud measurements 根据点云测量估计状态<br>output: high-precision defect identification 输出：高精度缺陷识别 |
7.5 | [[7.5] 2501.17636v2 Efficient Interactive 3D Multi-Object Removal](http://arxiv.org/abs/2501.17636v2) | 3D reconstruction 三维重建 | 3D scene understanding<br>multi-object removal<br>3D场景理解<br>多对象移除 | input: selected areas and objects for removal 选定的移除区域和对象<br>step1: mask matching and refinement mask 匹配和细化掩码步骤<br>step2: homography-based warping 同伦变换基础的扭曲<br>step3: inpainting process 修复过程<br>output: modified 3D scene 修改后的3D场景 |
7.0 | [[7.0] 2501.18246v1 Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation](http://arxiv.org/abs/2501.18246v1) | 3D reconstruction  三维重建 | point cloud segmentation<br>outdoor point clouds<br>semantic segmentation<br>point cloud<br>关键点云分割<br>户外点云<br>语义分割<br>点云 | input: outdoor point clouds 户外点云<br>compute Digital Terrain Models (DTMs) 计算数字地形模型<br>employ RandLA-Net for segmentation 使用 RandLA-Net 进行分割<br>evaluate performance on datasets 评估在数据集上的表现<br>integrate relative elevation features 集成相对高程特征 |
6.5 | [[6.5] 2501.18494v1 Runway vs. Taxiway: Challenges in Automated Line Identification and Notation Approaches](http://arxiv.org/abs/2501.18494v1) | Autonomous Driving 自动驾驶 | Automated line identification 自动化线识别<br>Convolutional Neural Network 卷积神经网络<br>runway markings 跑道标记<br>autonomous systems 自动化系统<br>labeling algorithms 标记算法 | input: runway and taxiway images 跑道和滑行道图像<br>Step 1: color threshold adjustment 颜色阈值调整<br>Step 2: refine region of interest selection 精细化感兴趣区域选择<br>Step 3: integrate CNN classification 集成CNN分类<br>output: improved marking identification 改进的标记识别 |


## Newly Found Papers on ...
(Older entries get replaced automatically when the script runs again.)